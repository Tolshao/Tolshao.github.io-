<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>mac开启HiDPI</title>
      <link href="/posts/mac-kai-qi-hidpi/"/>
      <url>/posts/mac-kai-qi-hidpi/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是HiDPI"><a href="#什么是HiDPI" class="headerlink" title="什么是HiDPI"></a>什么是HiDPI</h1><ul><li>全名：<code>High Dots Per Inch</code></li></ul><blockquote><p>它使用横纵 2 个物理像素也就是 4 个物理像素来显示 1 个像素区域，结果就是图像的细节得到翻倍、更清晰、边缘更平滑。</p><p>拿 13 寸的 MacBook Pro 举例，它的屏幕物理分辨率是 2560 x 1600，所以原生的 HiDPI 分辨率就是 1280x800。更高的一档 1440x900 HiDPI 缩放分辨率，则是虚拟出一个 2880x1800 的分辨率，再进行软缩放输入。所以你能发现苹果的电脑总是有很高的分辨率。</p><p>好了，既然你的屏幕是 1080p，本身就没有那么多像素去合成 HiDPI，如果以原生的显示计算，你这屏幕的 1080p 分辨率应该是 960x540，这么低的分辨率你是没法用的。</p><p>这个脚本的功能就是虚拟出比你的屏幕物理分辨率更高的假分辨率……如果你要开启 1080p 的 HiDPI 分辨率，就虚拟一个 3840 x 2160 的假分辨率，然后 macOS 会使用 4 个像素来显示 1 个像素区域，也就是和你物理分辨率一样的 1080p 分辨率。</p></blockquote><ul><li>on-off 对比图<br><img src="/img/hidpi-on.png" alt="hidpi-on"></li></ul><p><img src="/img/hidpi-off.png" alt="hidpi-off"><br>原来一直觉得字体辣眼睛，不是显示器的锅</p><h1 id="为什么要手动开启hidpi"><a href="#为什么要手动开启hidpi" class="headerlink" title="为什么要手动开启hidpi"></a>为什么要<strong>手动</strong>开启hidpi</h1><p>不只是黑苹果需要开启hidpi，白苹果外接非4k显示屏的时候默认也是不开启hidpi的，显示效果不佳，颗粒感严重</p><h1 id="操作方法"><a href="#操作方法" class="headerlink" title="操作方法"></a>操作方法</h1><ul><li>靠第三方软件<ul><li>SwitchResX（收费）<br><img src="/img/16099873336577.png" alt="SwitchResX"></li><li>RDM（Retina Display Manager），免费<br><img src="/img/16099896834987.png" alt="-w337"><br><a href="http://avi.alkalay.net/software/RDM/">下载链接</a></li></ul></li></ul><h2 id="原生：修改系统配置文件"><a href="#原生：修改系统配置文件" class="headerlink" title="原生：修改系统配置文件"></a>原生：修改系统配置文件</h2><ul><li><p>见<a href="https://sspai.com/post/57549">少数派教程</a></p></li><li><p>并附有懒人版，一键bash脚本操作搞定</p><pre><code class="lang-bash">sh -c "$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi.sh)"</code></pre></li><li>脚本运行过程如下：<br><img src="/img/%E8%84%9A%E6%9C%AC.png" alt="操作过程"></li></ul><p>如果成功的话，在系统显示器配置页面可以见到分辨率调整选项<br><img src="/img/16099901559419.jpg" alt="显示器分辨率调整页面"></p><p>如果需要更多选项，可以按<code>option</code>单击<code>缩放</code>选项<br><img src="/img/16099902334267.jpg" alt="Advanced"><br>从中选择一个支持hidpi的分辨率，让你的眼睛舒服一些吧。</p><h1 id="针对不同系统版本的说明"><a href="#针对不同系统版本的说明" class="headerlink" title="针对不同系统版本的说明"></a>针对不同系统版本的说明</h1><ul><li><p>系统在10.15之前的，配置文件放在<code>/System</code>下，按照教程开启<code>SIP(System Integrity Protection)</code>可以完成系统配置文件的修改</p></li><li><p>系统从big sur开始，将原生系统配置文件锁死在<br><code>/System/Library/Displays/Contents/Resources/Overrides/</code><br>开启<code>SIP</code>后仍无法修改。<br>可以通过在<code>/Library/Displays/Contents/Resources/Overrides/</code>下放置对应配置文件，系统自动完成优先调用</p></li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 设置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> settings </tag>
            
            <tag> HiDPI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Getting Started with gym</title>
      <link href="/posts/rl-gym-start/"/>
      <url>/posts/rl-gym-start/</url>
      
        <content type="html"><![CDATA[<h1 id="gym入门"><a href="#gym入门" class="headerlink" title="gym入门"></a>gym入门</h1><p>gym是用于开发和比较强化学习算法的工具包。它不对代理的结构做任何假设，并且与任何数字计算库(例如TensorFlow或Theano)兼容。</p><p><code>gym</code>库是测试问题(环境)的集合，您可以用来制定强化学习算法。这些环境具有共享的接口，使您可以编写常规算法。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>首先，您需要安装Python 3.5+。只需使用pip安装gym：<br>如果你的电脑中存在系统版本Python2，那你可能要用<code>pip3</code>来安装</p><pre><code class="lang-python">pip install gym</code></pre><ul><li>从源头建造</li></ul><p>如果愿意，还可以直接克隆gym Git存储库。当您要修改gym本身或添加环境时，此功能特别有用。使用以下方法下载并安装：</p><pre><code class="lang-python">git clone https://github.com/openai/gymcd gympip install -e。</code></pre><p>您以后可以运行<code>pip install -e.[all]</code>执行包含所有环境的完整安装。这需要安装更多涉及的依赖项，包括<code>cmake</code>和最新的<code>pip</code>版本。</p><h2 id="环境环境"><a href="#环境环境" class="headerlink" title="环境环境"></a>环境环境</h2><p>这是运行某件事的最低限度示例。这将在1000个时间步中运行CartPole-v0环境的实例，并在每个步骤中渲染该环境。您应该会看到一个弹出窗口，呈现经典的购物车问题：</p><pre><code class="lang-python">import gymenv = gym.make('CartPole-v0')env.reset()for _ in range(1000)：    env.render()    env.step(env.action_space.sample())＃采取随机行动env.close()</code></pre><p>它看起来应该像这样：</p><p><img src="/img/cartpole.gif" alt="cartpole"></p><p>通常，我们会在允许球杆离开屏幕之前结束模拟。以后再说。现在，即使此环境已经返回<code>done = True</code>，也请忽略有关调用<code>step()</code>的警告。</p><p>如果您希望看到其他运行环境，请尝试将上面的<code>CartPole-v0</code>替换为<code>MountainCar-v0</code>，<code>MsPacman-v0</code>(需要Atari依赖项)或<code>Hopper-v1</code>(需要<code>MuJoCo</code>依赖项)。所有环境均来自<code>Env</code>基类。</p><p>请注意，如果您缺少任何依赖项，则应该收到一条有用的错误消息，告诉您所缺少的内容。 (让我们知道依赖项是否给您带来麻烦，而没有明确的修复说明。)安装缺少的依赖项通常非常简单。您还需要<code>Hopper-v1</code>的<code>MuJoCo</code>许可证。</p><h2 id="观察结果"><a href="#观察结果" class="headerlink" title="观察结果"></a>观察结果</h2><p>如果我们想做的比每步都采取随机行动要好，那么最好是真正了解我们的行动对环境有何影响。</p><p>环境的<code>step</code>函数恰好返回了我们所需要的。实际上，<code>step</code>返回四个值。这些是：</p><ul><li><code>observation</code>(object)：特定于环境的对象，代表您对环境的观察。例如，来自摄像机的像素数据，机器人的关节角度和关节速度或棋盘游戏中的棋盘状态。</li><li><code>reward</code>(float)：上一操作获得的奖励金额。规模因环境而异，但目标始终是增加总奖励。</li><li><code>done</code>(布尔值)：是否应该再次重置环境。大多数(但不是全部)任务被划分为定义明确的情节，如果为True，则表示情节已终止。 (例如，也许杆子太尖了，或者您失去了上一生。)</li><li><code>info</code>(dict)：诊断信息，可用于调试。它有时对学习很有用(例如，它可能包含环境上次状态更改背后的原始概率)。但是，您的代理人的官方评估不允许将其用于学习。<br>这只是经典“代理程序-环境循环”的实现。每个时间步长，代理都会选择一个动作，环境会返回观察结果和奖励。</li></ul><p><img src="/img/2.png" alt="agent and env"></p><p>该过程通过调用<code>reset()</code>开始，此返回初始<code>observation</code>。因此，编写前面的代码的更合适的方法是检查<code>done</code>flag：</p><pre><code class="lang-python">import gymenv = gym.make('CartPole-v0')for i_episode in range(20):    observation = env.reset()    for t in range(100):        env.render()        print(observation)        action = env.action_space.sample()        observation, reward, done, info = env.step(action)        if done:            print("Episode finished after &amp;#123;&amp;#125; timesteps".format(t+1))            breakenv.close()</code></pre><p>这应该提供视频和类似以下的输出。您应该能够看到重置发生的位置。<br><img src="/img/cartpole1.gif" alt="cartpole"></p><pre><code>[-0.061586   -0.75893141  0.05793238  1.15547541][-0.07676463 -0.95475889  0.08104189  1.46574644][-0.0958598  -1.15077434  0.11035682  1.78260485][-0.11887529 -0.95705275  0.14600892  1.5261692 ][-0.13801635 -0.7639636   0.1765323   1.28239155][-0.15329562 -0.57147373  0.20218013  1.04977545]Episode finished after 14 timesteps[-0.02786724  0.00361763 -0.03938967 -0.01611184][-0.02779488 -0.19091794 -0.03971191  0.26388759][-0.03161324  0.00474768 -0.03443415 -0.04105167]</code></pre><h2 id="空间"><a href="#空间" class="headerlink" title="空间"></a>空间</h2><p>在上面的示例中，我们从环境的操作空间中采样了随机操作。但是这些动作实际上是什么？每个环境都有一个action_space和一个observation_space。这些属性的类型为Space，它们描述了有效操作和观察的格式：</p><pre><code>import gymenv = gym.make('CartPole-v0')print(env.action_space)#&gt; Discrete(2)print(env.observation_space)#&gt; Box(4,)</code></pre><p><code>Discrete</code>空间允许固定范围的非负数，因此在这种情况下，有效操作为0或1。<code>Box</code>空间表示n维盒子，因此有效观察值将是4个数字组成的数组。我们还可以检查<code>Box</code>的边界：</p><pre><code class="lang-python">print(env.observation_space.high)#&gt; array([ 2.4       ,         inf,  0.20943951,         inf])print(env.observation_space.low)#&gt; array([-2.4       ,        -inf, -0.20943951,        -inf])</code></pre><p>这种<code>assert</code>对于编写适用于许多不同环境的通用代码很有帮助。 Box和Discrete是最常见的空间。您可以从某个空间采样或检查某个空间是否属于该空间：</p><pre><code class="lang-python">from gym import spacesspace = spaces.Discrete(8) # Set with 8 elements &amp;#123;0, 1, 2, ..., 7&amp;#125;x = space.sample()assert space.contains(x)assert space.n == 8</code></pre><p>对于<code>CartPole-v0</code>，其中一个动作向左施加力，而其中一个动作向右施加力。 (您能找出哪个吗？)</p><p>幸运的是，您的学习算法越好，您自己尝试解释这些数字的次数就越少。</p><h1 id="可用环境"><a href="#可用环境" class="headerlink" title="可用环境"></a>可用环境</h1><p>gym拥有各种环境，从容易到困难，涉及许多不同种类的数据。查看环境的完整列表以鸟瞰。</p><ul><li><code>经典控制</code>和<code>玩具文字</code>：完成小规模任务，大部分来自RL文献。他们是来帮助您入门的。<br>算法：执行计算，例如添加多位数和反转顺序。有人可能会反对说这些任务对于计算机来说很容易。挑战在于仅从示例中学习这些算法。这些任务具有很好的特性，即可以通过改变序列长度来轻松地改变难度。</li><li><code>Atari</code>：玩经典的Atari游戏。我们以易于安装的形式集成了Arcade学习环境(这对强化学习研究产生了重大影响)。</li><li><code>2D和3D机器人</code>：在仿真中控制机器人。这些任务使用了MuJoCo物理引擎，该引擎设计用于快速而准确的机器人仿真。其中包括加州大学伯克利分校研究人员最新基准的一些环境(偶然会在今年夏天加入我们)。 <code>MuJoCo</code>是专有软件，但提供免费试用许可证。</li></ul><h2 id="注册表Registry"><a href="#注册表Registry" class="headerlink" title="注册表Registry"></a>注册表Registry</h2><p><code>gym</code>的主要目的是提供大量环境，这些环境暴露出一个通用的界面，并进行版本控制以进行比较。要列出安装中可用的环境，只需询问<code>gym.envs.registry</code>：</p><pre><code class="lang-python">from gym import envsprint(envs.registry.all())#&gt; [EnvSpec(DoubleDunk-v0), EnvSpec(InvertedDoublePendulum-v0), EnvSpec(BeamRider-v0), EnvSpec(Phoenix-ram-v0), EnvSpec(Asterix-v0), EnvSpec(TimePilot-v0), EnvSpec(Alien-v0), EnvSpec(Robotank-ram-v0), EnvSpec(CartPole-v0), EnvSpec(Berzerk-v0), EnvSpec(Berzerk-ram-v0), EnvSpec(Gopher-ram-v0), ...</code></pre><p>这将为您提供<code>EnvSpec</code>对象的列表。这些定义了特定任务的参数，包括要运行的试验次数和最大步骤数。例如，<code>EnvSpec(Hopper-v1)</code>定义了一个环境，目标是让2D模拟机器人跳跃； <code>EnvSpec(Go9x9-v0)</code>在9x9板上定义Go游戏。</p><p>这些环境ID被视为不透明字符串。为了确保将来进行有效的比较，绝不会以影响性能的方式更改环境，而只能用较新的版本来替换。目前，我们为每个环境都添加了v0后缀，以便将来可以自然地将其替换为v1，v2等。</p><p>将您自己的环境添加到注册表非常容易，从而使它们可用于<code>gym.make()</code>：只需在加载时<code>register()</code>即可。</p><h1 id="背景：为什么要选择gym？"><a href="#背景：为什么要选择gym？" class="headerlink" title="背景：为什么要选择gym？"></a>背景：为什么要选择<code>gym</code>？</h1><p>强化学习(RL)是机器学习的子领域，涉及决策和运动控制。它研究代理商如何在复杂，不确定的环境中学习如何实现目标。令人兴奋的原因有两个：</p><ul><li><strong>RL是一个大的范式(框架)，涵盖了涉及一系列决策的所有问题</strong>：例如，控制机器人的电动机以使其能够运行和跳跃，制定价格，库存管理等商业决策，或者玩视频游戏和棋盘游戏。 RL甚至可以应用于具有顺序或结构化输出的监督学习问题。</li><li><p><strong>RL算法已开始在许多困难的环境中取得良好的效果</strong>。 RL历史悠久，但在深度学习方面取得新进展之前，它需要大量针对特定问题的工程。 DeepMind的Atari结果，Pieter Abbeel小组的BRETT和AlphaGo都使用了深度RL算法，该算法并未对其环境做太多假设，因此可以在其他环境中应用。<br>但是，RL研究也因两个因素而减慢了速度：</p></li><li><p><strong>需要更好的基准</strong>。在监督学习中，像ImageNet这样的大型标签数据集推动了进步。在RL中，最接近的等效项是各种各样的环境。但是，现有的RL环境的开源集合种类繁多，并且通常甚至很难设置和使用。</p></li><li><strong>出版物中使用的环境缺乏标准化</strong>。问题定义上的细微差异(例如奖励功能或一组动作)会大大改变任务的难度。这个问题使得很难复制已发表的研究成果并比较不同论文的结果。<br><code>gym</code>是试图解决这两个问题的尝试。</li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> gym </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习：控制工程师帮你醍醐灌顶</title>
      <link href="/posts/rl-matlab-youtube/"/>
      <url>/posts/rl-matlab-youtube/</url>
      
        <content type="html"><![CDATA[<p>这里有很多形象的图，方便理解强化学习的公式<a href="https://zhuanlan.zhihu.com/p/111869532">知乎白话强化学习</a>，有空了可以来看</p><h1 id="强化学习-11：Matlab-RL"><a href="#强化学习-11：Matlab-RL" class="headerlink" title="强化学习-11：Matlab RL"></a>强化学习-11：Matlab RL</h1><p><img src="/img/15953228371087.jpg" alt="-w698"></p><ul><li><p>Agent：<br>由Policy 和 RL_Algorithm构成</p><ul><li>policy负责将observation映射为action</li><li>RL_Algorithm负责优化policy</li></ul></li><li><p>Enviroment：</p><ul><li>输入action</li><li>输出reward、state</li><li>内部执行状态转移、判断是否任务终止等</li></ul></li></ul><h2 id="关键定义"><a href="#关键定义" class="headerlink" title="关键定义"></a>关键定义</h2><ul><li>Reward：根据当前状态得到的即刻奖励</li><li>Value：根据当前状态预测的整个周期的reward（包括未来）</li></ul><p><img src="/img/15953233239703.jpg" alt="-w389"></p><p>未来奖励折扣：未来Value不最优</p><ol><li>reward now &gt; reward later</li><li>未来的不确定性</li></ol><ul><li>Balance: exploration探索 vs exploitation利用</li></ul><p>one step update<br><img src="/img/15953235640183.jpg" alt="-w668"><br><img src="/img/15953236270668.jpg" alt="-w651"></p><h2 id="RL-workflow"><a href="#RL-workflow" class="headerlink" title="RL workflow"></a>RL workflow</h2><ul><li>Env：Real or simulated？</li><li>Reward signal：指导agent按预期action</li><li>Policy：observation 映射到 action的结构</li><li>Training：选择算法寻优</li><li>Deploy/verify：部署agent</li></ul><h2 id="一些策略"><a href="#一些策略" class="headerlink" title="一些策略"></a>一些策略</h2><h3 id="Q-function"><a href="#Q-function" class="headerlink" title="Q-function"></a>Q-function</h3><p>更新states-action表格，根据s，选a<br><img src="/img/15953248613942.jpg" alt="-w395"></p><p>缺点：带来维度灾难<br>对于连续空间，构建Value = w1 <em> state + w2 </em> action<br>手段：函数近似器<br><img src="/img/15953249979358.jpg" alt="-w1317"></p><h3 id="策略梯度法"><a href="#策略梯度法" class="headerlink" title="策略梯度法"></a>策略梯度法</h3><p><img src="/img/15953279655333.jpg" alt="-w701"></p><p>缺点：</p><ol><li>对于稀疏奖励问题，梯度小，训练慢</li><li>容易陷入区间极值</li></ol><h3 id="Value-function-based"><a href="#Value-function-based" class="headerlink" title="Value-function based"></a>Value-function based</h3><p>crictic评价网络<br><img src="/img/15953280498462.jpg" alt="-w604"></p><ul><li>贝尔曼方程：<br>R:reward<br>Q:当前Q<br>maxQ’：未来最大的Q<br>γ：折扣率discount factor[0,1]<br>α：学习率learning rate<br><img src="/img/15953283341441.jpg" alt="-w586"></li></ul><h3 id="AC算法"><a href="#AC算法" class="headerlink" title="AC算法"></a>AC算法</h3><p>图中有两个网络：actor、critic<br>actor：根据policy给出最大概率下的action<br><img src="/img/15953289716908.jpg" alt="-w744"></p><p>完成离线仿真和学习之后，将policy部署到硬件<br>RL algorithm学习能力对于适应不确定干扰和缓变环境尤为重要<br><img src="/img/15953297916839.jpg" alt="-w641"></p><h4 id="AC网络的执行逻辑"><a href="#AC网络的执行逻辑" class="headerlink" title="AC网络的执行逻辑"></a>AC网络的执行逻辑</h4><pre><code>while True:  a = actor.choose_action(s)s_,r,done,info = env.step(a)td_error = critic.learn(s,r,s_)actor.learn(s,a,td_error)s = s_</code></pre><h3 id="DDPG——deep-deterministic-policy-gradient"><a href="#DDPG——deep-deterministic-policy-gradient" class="headerlink" title="DDPG——deep deterministic policy gradient"></a>DDPG——deep deterministic policy gradient</h3><h4 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h4><ul><li>连续空间与环境学习</li><li>确定策略比随机快</li></ul><p>加入视觉、雷达等传感器后，观测量维数暴增，全连接层不管用<br><img src="/img/15953840126963.jpg" alt="-w408"></p><h2 id="改进RL系统需要注意的点"><a href="#改进RL系统需要注意的点" class="headerlink" title="改进RL系统需要注意的点"></a>改进RL系统需要注意的点</h2><p>鲁棒性、安全性、可变性、可验证性<br><img src="/img/15953856661541.jpg" alt="-w765"></p><ol><li>Robust：对于实际系统具有不确定性的值：制造装配公差引起的几何参数、力（力矩）、传感器采回的信号，送给agent前作随机处理。</li><li>Safety：建立monitor，在系统出问题时接管到安全模式</li></ol><h2 id="与传统控制方式的结合"><a href="#与传统控制方式的结合" class="headerlink" title="与传统控制方式的结合"></a>与传统控制方式的结合</h2><h3 id="将RL-agent用于高级任务，低级任务交给传统"><a href="#将RL-agent用于高级任务，低级任务交给传统" class="headerlink" title="将RL-agent用于高级任务，低级任务交给传统"></a>将RL-agent用于高级任务，低级任务交给传统</h3><p><img src="/img/15953838314214.jpg" alt="-w772"></p><h3 id="以传统架构实现，RL网络负责调参"><a href="#以传统架构实现，RL网络负责调参" class="headerlink" title="以传统架构实现，RL网络负责调参"></a>以传统架构实现，RL网络负责调参</h3><p>优点：结构可解释， 验证性强<br>缺点：结构人为设计，对于复杂输入，性能非最优<br><img src="/img/15953858326711.jpg" alt="-w764"></p><h2 id="几种算法类型"><a href="#几种算法类型" class="headerlink" title="几种算法类型"></a>几种算法类型</h2><p><img src="/img/15936754840195.jpg" alt="-w855"></p><p>Model-free：不尝试去理解环境, 环境给什么就是什么，一步一步等待真实世界的反馈, 再根据反馈采取下一步行动。</p><p>Model-based：先理解真实世界是怎样的, 并建立一个模型来模拟现实世界的反馈，通过想象来预判断接下来将要发生的所有情况，然后选择这些想象情况中最好的那种，并依据这种情况来采取下一步的策略。它比 Model-free 多出了一个虚拟环境，还有想象力。</p><p>Policy based：通过感官分析所处的环境, 直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动。</p><p>Value based：输出的是所有动作的价值, 根据最高价值来选动作，这类方法不能选取连续的动作。</p><p>Monte-carlo update：游戏开始后, 要等待游戏结束, 然后再总结这一回合中的所有转折点, 再更新行为准则。</p><p>Temporal-difference update：在游戏进行中每一步都在更新, 不用等待游戏的结束, 这样就能边玩边学习了。</p><p>On-policy：必须本人在场, 并且一定是本人边玩边学习。</p><p>Off-policy：可以选择自己玩, 也可以选择看着别人玩, 通过看别人玩来学习别人的行为准则。</p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> Matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL实践3——为Agent添加Policy、记忆功能</title>
      <link href="/posts/rl-pr3/"/>
      <url>/posts/rl-pr3/</url>
      
        <content type="html"><![CDATA[<p>参考自知乎（叶强）</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>在实践2中，介绍了<code>gym</code>环境的定义和使用方法。<br>在实践1中，介绍了 动态规划DP 求解 价值函数<br>并没有形成一个策略Policy$\pi$来指导agent的动作选取，本节将利用SARSA（0）的学习方法，帮助agent学习到价值函数(表），指导$\epsilon$-greedy策略选取动作。</p><h1 id="Agent的写法"><a href="#Agent的写法" class="headerlink" title="Agent的写法"></a>Agent的写法</h1><p>Agent的三要素是：价值函数、策略、模型</p><p>本节以Sarsa（0）为例，介绍为agent添加policy的方法<br>Sarsa（0）是不基于模型的控制，其动作选择策略是$\epsilon$-greedy，根据价值函数选择动作。</p><p>对于一般问题，Agent包括如下功能</p><ul><li>对环境的引用</li><li>自身变量：Q值，状态值的记忆</li><li>策略方法</li><li>动作执行方法</li><li>学习方法：改进策略，这部分是关键</li></ul><pre><code class="lang-python">class Agent():    def __init__(self, env: Env):        self.env = env      # 个体持有环境的引用        self.Q = &amp;#123;&amp;#125;         # 个体维护一张行为价值表Q        self.state = None   # 个体当前的观测，最好写成obs.    def performPolicy(self, state): pass # 执行一个策略    def act(self, a):       # 执行一个行为        return self.env.step(a)    def learning(self): pass   # 学习过程</code></pre><h2 id="Agent-class"><a href="#Agent-class" class="headerlink" title="Agent class"></a>Agent class</h2><p>SARSA（0）的伪算法流程如下：<br><img src="/img/15991172298667.png" alt=""></p><h2 id="核心方法：learning"><a href="#核心方法：learning" class="headerlink" title="核心方法：learning"></a>核心方法：learning</h2><pre><code class="lang-python">def learning(self, gamma, alpha, max_episode_num):    # self.Position_t_name, self.reward_t1 = self.observe(env)    total_time, time_in_episode, num_episode = 0, 0, 0    while num_episode &lt; max_episode_num: # 设置终止条件        self.state = self.env.reset()    # 环境初始化        s0 = self._get_state_name(self.state) # 获取个体对于观测的命名        self.env.render()                # 显示UI界面        a0 = self.performPolicy(s0, num_episode, use_epsilon = True)        time_in_episode = 0        is_done = False        while not is_done:               # 针对一个Episode内部            # a0 = self.performPolicy(s0, num_episode)            s1, r1, is_done, info = self.act(a0) # 执行行为            self.env.render()            # 更新UI界面            s1 = self._get_state_name(s1)# 获取个体对于新状态的命名            self._assert_state_in_Q(s1, randomized = True)            # 获得A'            a1 = self.performPolicy(s1, num_episode, use_epsilon=True)            old_q = self._get_Q(s0, a0)              q_prime = self._get_Q(s1, a1)            td_target = r1 + gamma * q_prime              #alpha = alpha / num_episode            new_q = old_q + alpha * (td_target - old_q)            self._set_Q(s0, a0, new_q)            if num_episode == max_episode_num: # 终端显示最后Episode的信息                print("t:&amp;#123;0:&gt;2&amp;#125;: s:&amp;#123;1&amp;#125;, a:&amp;#123;2:2&amp;#125;, s1:&amp;#123;3&amp;#125;".\                    format(time_in_episode, s0, a0, s1))            s0, a0 = s1, a1            time_in_episode += 1        print("Episode &amp;#123;0&amp;#125; takes &amp;#123;1&amp;#125; steps.".format(            num_episode, time_in_episode)) # 显示每一个Episode花费了多少步        total_time += time_in_episode        num_episode += 1    return</code></pre><h2 id="策略方法：performPolicy"><a href="#策略方法：performPolicy" class="headerlink" title="策略方法：performPolicy"></a>策略方法：performPolicy</h2><p>通过改变<code>use_epsilon</code>参数，可以切换SARSA 和 Q-learning<br>相同在于：</p><ul><li>都用$\epsilon$-greedy策略进行了探索</li></ul><p>区别在于：</p><ul><li><p>Q-learning：更激进，最优更新</p><ul><li>update： $greedy$策略，评估过程的A’没有实际执行</li><li>control：$\epsilon-greedy$策略</li></ul></li><li><p>SARSA：更新和执行都用$\epsilon-greedy$策略</p></li></ul><pre><code class="lang-python">def performPolicy(self, s, episode_num, use_epsilon):    epsilon = 1.00 / (episode_num+1)    Q_s = self.Q[s]    str_act = "unknown"    rand_value = random()    action = None    if use_epsilon and rand_value &lt; epsilon:          action = self.env.action_space.sample()    else:        str_act = max(Q_s, key=Q_s.get)        action = int(str_act)    return action</code></pre><h1 id="将Sarsa（0）升级为-Sarsa（-lambda-）"><a href="#将Sarsa（0）升级为-Sarsa（-lambda-）" class="headerlink" title="将Sarsa（0）升级为 Sarsa（$\lambda$）"></a>将Sarsa（0）升级为 Sarsa（$\lambda$）</h1><p>Sarsa（$\lambda$）相较于Sarsa（0）来说，引入了视野权重的概念，不是值考虑单步的价值更新，更新方式分为前向视角和后向视角</p><p>对于离散问题的Sarsa（$\lambda$）来说，agent不仅需要维护一张Q值表，还需要维护一张E值（迹）表</p><ul><li>伪算法如下</li></ul><p><img src="/img/15992869544578.png" alt=""></p><p>与上节的Sarsa（0）的主要不同，体现在2个方面</p><ul><li>Agent元素， 增加了E值，在时间尺度内记录（s，a）元组的迹值</li><li>Learning方法中，用后向视角更新，这样可以TD更新，不用MC更新，效率更高些</li></ul><p>E值是依附于episode存在的，每个episode初始化时，一并清零。</p><pre><code class="lang-python">def learning(self, lambda_, gamma, alpha, max_episode_num):    total_time = 0    time_in_episode = 0    num_episode = 1    while num_episode &lt;= max_episode_num:        self._resetEValue()        s0 = self._name_state(self.env.reset())        a0 = self.performPolicy(s0, num_episode)        # self.env.render()        time_in_episode = 0        is_done = False        # episode循环        while not is_done:            s1, r1, is_done, info = self.act(a0)            # self.env.render()            s1 = self._name_state(s1)            self._assert_state_in_QE(s1, randomized = True)            a1= self.performPolicy(s1, num_episode)            q = self._get_(self.Q, s0, a0)            q_prime = self._get_(self.Q, s1, a1)            delta = r1 + gamma * q_prime - q            e = self._get_(self.E, s0,a0)            e = e + 1            self._set_(self.E, s0, a0, e) # set E before update E            state_action_list = list(zip(self.E.keys(),self.E.values()))            for s, a_es in state_action_list:                for a in range(self.env.action_space.n):                    e_value = a_es[a]                    old_q = self._get_(self.Q, s, a)                    new_q = old_q + alpha * delta * e_value                    new_e = gamma * lambda_ * e_value                    self._set_(self.Q, s, a, new_q)                    self._set_(self.E, s, a, new_e)            if num_episode == max_episode_num:                print("t:&amp;#123;0:&gt;2&amp;#125;: s:&amp;#123;1&amp;#125;, a:&amp;#123;2:10&amp;#125;, s1:&amp;#123;3&amp;#125;".                      format(time_in_episode, s0, a0, s1))            s0, a0 = s1, a1            time_in_episode += 1        print("Episode &amp;#123;0&amp;#125; takes &amp;#123;1&amp;#125; steps.".format(            num_episode, time_in_episode))        total_time += time_in_episode        num_episode += 1    return</code></pre><ul><li>程序解读<ul><li>在上述程序中，以<code>dict</code>的形式存储E、Q值<br><code>E:dict = {key(s:string),value(a:dict)}</code><br>这种数据结构解释了，通过遍历<code>E.keys()</code>和<code>a[i], for in range self.env.action_space.n</code>，<code>for s,a_es in state_action_list</code>即可遍历状态、动作空间组成的元组<code>(s,a)</code>，即<code>E[s_name][a_name]</code></li><li>在值的更新中，没有的创建更新到E、Q值的dict中，进而创建的list<br>赋值的时候，先检测存不存在<code>`_assert()</code>不存在执行<code>init</code></li></ul></li></ul><pre><code class="lang-python">def _is_state_in_Q(self, s):        return self.Q.get(s) is not None    def _init_state_value(self, s_name, randomized = True):        if not self._is_state_in_Q(s_name):            self.Q[s_name], self.E[s_name] = &amp;#123;&amp;#125;,&amp;#123;&amp;#125;            for action in range(self.env.action_space.n):                default_v = random() / 10 if randomized is True else 0.0                self.Q[s_name][action] = default_v                self.E[s_name][action] = 0.0    def _assert_state_in_QE(self, s, randomized=True):        if not self._is_state_in_Q(s):            self._init_state_value(s, randomized)    def _name_state(self, state):         '''给个体的一个观测(状态）生成一个不重复的字符串作为Q、E字典里的键        '''        return str(state)                   def _get_(self, QorE, s, a):        self._assert_state_in_QE(s, randomized=True)        return QorE[s][a]    def _set_(self, QorE, s, a, value):        self._assert_state_in_QE(s, randomized=True)        QorE[s][a] = value    def _resetEValue(self):        for value_dic in self.E.values():            for action in range(self.env.action_space.n):                value_dic[action] = 0.00</code></pre><h1 id="给Agent添加记忆功能"><a href="#给Agent添加记忆功能" class="headerlink" title="给Agent添加记忆功能"></a>给Agent添加记忆功能</h1><p>以上章节的实现，是基于agent的E、Q值表，对于<strong>离散的、有限个</strong>的状态空间和动作空间，实现没问题。同时没有记忆功能的Agent只能进行单一episode的学习，无法对其他的episode学习，无法进行batch学习，上限较低，对于复杂问题，为了增强学习的鲁棒性，往往需要输入数据的规模扩充，也就是对Agent有了记忆能力的要求。</p><h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><h3 id="抽象基类Agent"><a href="#抽象基类Agent" class="headerlink" title="抽象基类Agent"></a>抽象基类Agent</h3><p>为了让代码具有较高的复用性和可读性，提现python的集成和多态特性，将Agent抽象为一个基类，在子类中实现记忆功能。</p><pre><code class="lang-python">class Agent(object):    '''Base Class of Agent    '''    def __init__(self, env: Env = None,                        trans_capacity = 0):        # 保存一些Agent可以观测到的环境信息以及已经学到的经验        self.env = env        self.obs_space = env.observation_space if env is not None else None        self.action_space = env.action_space if env is not None else None        self.experience = Experience(capacity = trans_capacity)        # 有一个变量记录agent当前的state相对来说还是比较方便的。要注意对该变量的维护、更新        self.state = None   # current observation of an agent    def performPolicy(self,policy_fun, s):        if policy_fun is None:            return self.action_space.sample()        return policy_fun(s)    def act(self, a0):        s0 = self.state        s1, r1, is_done, info = self.env.step(a0)        # TODO add extra code here        trans = Transition(s0, a0, r1, is_done, s1)        total_reward = self.experience.push(trans)        self.state = s1        return s1, r1, is_done, info, total_reward    def learning(self):        '''need to be implemented by all subclasses        '''        raise NotImplementedError    def sample(self, batch_size = 64):        '''随机取样        '''        return self.experience.sample(batch_size)    @property    def total_trans(self):        '''得到Experience里记录的总的状态转换数量        '''        return self.experience.total_trans</code></pre><p>可以发现，Agent不仅维护了<code>env</code>和<code>state</code>，同时增加了一个<code>experience</code>，这是一个缓存，将个体其他周期记录的状态转换、奖励等信息记录下来，其作用时，利用这些时间尺度上互相无关联的信息，让Agent学习到更好的价值函数的近似估计。<br>包含关系如下所示：</p><ul><li>Experience<ul><li>Episode<ul><li>Transition</li></ul></li></ul></li></ul><h3 id="Transition类"><a href="#Transition类" class="headerlink" title="Transition类"></a>Transition类</h3><p>一个完整的状态转换（Transition）类，包括了当前的状态$s_0$和动作$a_0$，以及个体执行了动作之后的奖励$r$和新状态$s_1$，另外用一个bool变量记录了$s_1$是否是终止状态。<br>代码来源于：知乎（叶强）</p><pre><code class="lang-pyton">class Transition(object):    def __init__(self, s0, a0, reward:float, is_done:bool, s1):        self.data = [s0,a0,reward,is_done,s1]    def __iter__(self):        return iter(self.data)    def __str__(self):        return "s:&amp;#123;0:&lt;3&amp;#125; a:&amp;#123;1:&lt;3&amp;#125; r:&amp;#123;2:&lt;4&amp;#125; is_end:&amp;#123;3:&lt;5&amp;#125; s1:&amp;#123;4:&lt;3&amp;#125;".\            format(self.data[0],                    self.data[1],                    self.data[2],                     self.data[3],                    self.data[4])    @property    def s0(self):   return self.data[0]    @property    def a0(self):   return self.data[1]    @property    def reward(self):   return self.data[2]    @property    def is_done(self):   return self.data[3]    @property    def s1(self):   return self.data[4]</code></pre><p>用<code>@property</code>函数装饰器的方法，可以快捷的调用类中的元素，将<strong>方法</strong>调用变为<strong>属性</strong>调用<br>其主要用于类的定义，元素属性（只读、可写）等的定义<br>例如用<code>@property</code>装饰了<code>myfunc</code>，则继而可以用<code>@myfunc.setter</code>装饰<code>myfunc</code>的其他设置方法，方法中，可以写规则判断等，增加<code>class</code>中变量的<strong>可控性</strong>。<br>示例</p><pre><code class="lang-python">t1 = Transition(s0,a0,r,False,s1)t1.s0   #调用t1.s0(self)输出 t1.data[0]</code></pre><h3 id="Episode"><a href="#Episode" class="headerlink" title="Episode"></a>Episode</h3><p><code>Episode</code>记录了时间序列的<code>Transition</code>对象，可以构建为由<code>Transition</code>构成的<code>list</code>，</p><ul><li>对于离线学习来说，可以从中抓取随机个数、无序的<code>Transition</code></li></ul><p>例子：</p><pre><code class="lang-python">class Episode(object):    def __init__(self, e_id:int = 0) -&gt; None:        self.total_reward = 0   # 总的获得的奖励        self.trans_list = []    # 状态转移列表        self.name = str(e_id)   # 可以给Episode起个名字："成功闯关,黯然失败？"    def push(self, trans:Transition) -&gt; float:        self.trans_list.append(trans)        self.total_reward += trans.reward        return self.total_reward    @property    def len(self):        return len(self.trans_list)    def __str__(self):        return "episode &amp;#123;0:&lt;4&amp;#125; &amp;#123;1:&gt;4&amp;#125; steps,total reward:&amp;#123;2:&lt;8.2f&amp;#125;".\            format(self.name, self.len,self.total_reward)    def print_detail(self):        print("detail of (&amp;#123;0&amp;#125;):".format(self))        for i,trans in enumerate(self.trans_list):            print("step&amp;#123;0:&lt;4&amp;#125; ".format(i),end=" ")            print(trans)    def pop(self) -&gt; Transition:        '''normally this method shouldn't be invoked.        '''        if self.len &gt; 1:            trans = self.trans_list.pop()            self.total_reward -= trans.reward            return trans        else:            return None    def is_complete(self) -&gt; bool:        '''check if an episode is an complete episode        '''        if self.len == 0:             return False         return self.trans_list[self.len-1].is_done    def sample(self,batch_size = 1):           '''随即产生一个trans        '''        return random.sample(self.trans_list, k = batch_size)    def __len__(self) -&gt; int:        return self.len</code></pre><h3 id="Experience（Memory）"><a href="#Experience（Memory）" class="headerlink" title="Experience（Memory）"></a>Experience（Memory）</h3><ul><li>有些模型框架用<code>Memory</code>，是乱序的<code>Episode</code>，这一步可以跨<code>episode</code>进行<code>Transition</code>记录和采样</li><li><code>Memory</code>需要设置容量上限<code>capacity</code>，超过上限，从早期去除<code>Transition</code></li></ul><pre><code class="lang-python">class Experience(object):    '''this class is used to record the whole experience of an agent organized    by an episode list. agent can randomly sample transitions or episodes from    its experience.'''    def __init__(self, capacity:int = 20000):        self.capacity = capacity    # 容量：指的是trans总数量        self.episodes = []          # episode列表        self.next_id = 0            # 下一个episode的Id        self.total_trans = 0        # 总的状态转换数量    def __str__(self):        return "exp info:&amp;#123;0:5&amp;#125; episodes, memory usage &amp;#123;1&amp;#125;/&amp;#123;2&amp;#125;".\                format(self.len, self.total_trans, self.capacity)    def __len__(self):        return self.len    @property    def len(self):        return len(self.episodes)    def _remove(self, index = 0):              '''扔掉一个Episode，默认第一个。           remove an episode, defautly the first one.           args:                the index of the episode to remove           return:               if exists return the episode else return None        '''        if index &gt; self.len - 1:            raise(Exception("invalid index"))        if self.len &gt; 0:            episode = self.episodes[index]            self.episodes.remove(episode)            self.total_trans -= episode.len            return episode        else:            return None    def _remove_first(self):        self._remove(index = 0)    def push(self, trans):         '''压入一个状态转换        '''        if self.capacity &lt;= 0:            return        while self.total_trans &gt;= self.capacity: # 可能会有空episode吗？            episode = self._remove_first()        cur_episode = None        if self.len == 0 or self.episodes[self.len-1].is_complete():            cur_episode = Episode(self.next_id)            self.next_id += 1            self.episodes.append(cur_episode)        else:            cur_episode = self.episodes[self.len-1]        self.total_trans += 1        return cur_episode.push(trans)      #return  total reward of an episode    def sample(self, batch_size=1): # sample transition        '''randomly sample some transitions from agent's experience.abs        随机获取一定数量的状态转化对象Transition        args:            number of transitions need to be sampled        return:            list of Transition.        '''        sample_trans = []        for _ in range(batch_size):            index = int(random.random() * self.len)            sample_trans += self.episodes[index].sample()        return sample_trans    def sample_episode(self, episode_num = 1):  # sample episode        '''随机获取一定数量完整的Episode        '''        return random.sample(self.episodes, k = episode_num)    @property    def last(self):        if self.len &gt; 0:            return self.episodes[self.len-1]        return None</code></pre><p>在这个例子中<code>Experience</code>维护了<code>episode</code>列表、<code>Capacity</code></p><h1 id="函数估计器-Approximator"><a href="#函数估计器-Approximator" class="headerlink" title="函数估计器 Approximator"></a>函数估计器 Approximator</h1><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>但是对于连续的空间，无法构建这个表，替代的解决方案是选取一个分辨率，将连续空间离散化，但是随着状态、动作的维数增加，表的规模爆炸，带来运算量的激增，这是我们不希望看到的</p><p>这里引入函数估计器的概念，利用估计器，设计（学习）映射函数，将输入（状态、动作 等）映射到输出（E、Q值 等），对于不同复杂度的问题，选取不同规模的函数估计器，一般要求函数估计器具有非线性的映射能力。</p><p>深度强化学习，是深度学习和强化学习的结合，以深度学习的网络作为工具，嵌入到强化学习的框架之下，形成对复杂问题强有力的解决工具。</p><h2 id="Approximator-类"><a href="#Approximator-类" class="headerlink" title="Approximator 类"></a>Approximator 类</h2><p>原理很简单：</p><ul><li>输入：状态、动作（s,a）</li><li>输出：价值Q(s,a,w)</li></ul><p>对于Actor网络，输入为状态s，输出为动作a<br>这里的例子是 用 <code>pytorch</code>搭建一个简单的BP神经网络 作为 估计器 Approximator</p><pre><code class="lang-python">import numpy as npimport torchfrom torch.autograd import Variableimport copyclass Approximator(torch.nn.Module):    '''base class of different function approximator subclasses    '''    def __init__(self, dim_input = 1, dim_output = 1, dim_hidden = 16):        super(Approximator, self).__init__()        self.dim_input = dim_input        self.dim_output = dim_output        self.dim_hidden = dim_hidden        self.linear1 = torch.nn.Linear(self.dim_input, self.dim_hidden)        self.linear2 = torch.nn.Linear(self.dim_hidden, self.dim_output)</code></pre><ul><li>实现正向传播方法</li></ul><pre><code class="lang-python"> def _forward(self, x):        h_relu = self.linear1(x).clamp(min=0) # 实现了ReLU        y_pred = self.linear2(h_relu)        return y_pred</code></pre><ul><li>实现训练方法<code>fit</code></li></ul><pre><code class="lang-python">def fit(self, x,               y,               criterion=None,               optimizer=None,               epochs=1,              learning_rate=1e-4):        if criterion is None:            criterion = torch.nn.MSELoss(size_average = False)        if optimizer is None:            optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate)        if epochs &lt; 1:            epochs = 1        x = self._prepare_data(x)        y = self._prepare_data(y, False)        for t in range(epochs):            y_pred = self._forward(x)            loss = criterion(y_pred, y)            optimizer.zero_grad()            loss.backward()            optimizer.step()        return loss</code></pre><ul><li>python数据类型复杂，需要对输入数据进行一定的处理</li></ul><pre><code class="lang-python"> def _prepare_data(self, x, requires_grad = True):        '''将numpy格式的数据转化为Torch的Variable        '''        if isinstance(x, np.ndarray):            x = Variable(torch.from_numpy(x), requires_grad = requires_grad)        if isinstance(x, int):            x = Variable(torch.Tensor([[x]]), requires_grad = requires_grad)        x = x.float()   # 从from_numpy()转换过来的数据是DoubleTensor形式        if x.data.dim() == 1:            x = x.unsqueeze(0)        return x</code></pre><ul><li><code>__call__</code>函数可以让类，像函数一样被调用</li></ul><pre><code class="lang-python">def __call__(self, x):        '''return an output given input.        similar to predict function        '''        x=self._prepare_data(x)        pred = self._forward(x)        return pred.data.numpy()</code></pre><h2 id="ApproxQagent的实现"><a href="#ApproxQagent的实现" class="headerlink" title="ApproxQagent的实现"></a>ApproxQagent的实现</h2><p>在本节，需要实现Agent基类的初始化<code>class ApproxQagent(Agent)</code>继承<br>同时，学习的时候从ExperienceReplay中学习，避免了单个Episode内Transition相关性带来的干扰，有利于提升Agent性能。</p><ul><li>ApproxQAgent初始化<br>ApproxQAgent继承自Agent父类，第二段的初始化过程中，执行了<code>__init__</code>方法，<code>super</code>调用了父类<code>Agent</code>的方法<code>__init__</code>进行初始化</li></ul><pre><code class="lang-python">class ApproxQAgent(Agent):    '''使用近似的价值函数实现的Q学习的个体    '''    def __init__(self, env: Env = None,                       trans_capacity = 20000,                       hidden_dim: int = 16):        if env is None:            raise "agent should have an environment"        super(ApproxQAgent, self).__init__(env, trans_capacity)        self.input_dim, self.output_dim = 1, 1        # 适应不同的状态和行为空间类型        if isinstance(env.observation_space, spaces.Discrete):            self.input_dim = 1        elif isinstance(env.observation_space, spaces.Box):            self.input_dim = env.observation_space.shape[0]        if isinstance(env.action_space, spaces.Discrete):            self.output_dim = env.action_space.n        elif isinstance(env.action_space, spaces.Box):            self.output_dim = env.action_space.shape[0]        # print("&amp;#123;&amp;#125;,&amp;#123;&amp;#125;".format(self.input_dim, self.output_dim))        # 隐藏层神经元数目        self.hidden_dim = hidden_dim        # 关键在下面两句，声明了两个近似价值函数        # 变量Q是一个计算价值，产生loss的近似函数（网络），        # 该网络参数在一定时间段内不更新参数        self.Q = Approximator(dim_input = self.input_dim,                              dim_output = self.output_dim,                              dim_hidden = self.hidden_dim)        # 变量PQ是一个生成策略的近似函数，该函数（网络）的参数频繁更新        self.PQ = self.Q.clone() # 更新参数的网络</code></pre><ul><li>从memory中学习<br>基本原理为<ul><li>从经验中采样</li><li>构建<code>s0,a0,r,is_done,s1</code>的array</li><li>创建网络训练集，<code>x_batch, y_batch</code></li><li>利用训练集训练网络，并更新网络参数</li></ul></li></ul><pre><code class="lang-python">def _learn_from_memory(self, gamma, batch_size, learning_rate, epochs):    trans_pieces = self.sample(batch_size)  # 随机获取记忆里的Transmition    states_0 = np.vstack([x.s0 for x in trans_pieces])    actions_0 = np.array([x.a0 for x in trans_pieces])    reward_1 = np.array([x.reward for x in trans_pieces])    is_done = np.array([x.is_done for x in trans_pieces])    states_1 = np.vstack([x.s1 for x in trans_pieces])    X_batch = states_0    y_batch = self.Q(states_0)  # 得到numpy格式的结果    # 使用了Batch，代码是矩阵运算，有点难理解，多通过观察输出来理解    Q_target = reward_1 + gamma * np.max(self.Q(states_1), axis=1)*\        (~ is_done) # is_done则Q_target==reward_1    y_batch[np.arange(len(X_batch)), actions_0] = Q_target    # loss is a torch Variable with size of 1    loss = self.PQ.fit(x = X_batch,                        y = y_batch,                        learning_rate = learning_rate,                       epochs = epochs)    mean_loss = loss.sum().data[0] / batch_size    self._update_Q_net()    return mean_loss</code></pre><ul><li><code>learning</code>方法，包含了<code>learn from experience</code>方法</li></ul><p>基本原理：</p><pre><code>- 输入网络训练超参，训练episode数，奖励衰减率等参数- 在每个episode的生命周期，调用环境env交互，并进行学习（当Transitions数大于网络学习所需batch超参）</code></pre><pre><code class="lang-python">def learning(self, gamma = 0.99,                       learning_rate=1e-5,                        max_episodes=1000,                        batch_size = 64,                       min_epsilon = 0.2,                       epsilon_factor = 0.1,                       epochs = 1):        """learning的主要工作是构建经历，当构建的经历足够时，同时启动基于经历的学习        """        total_steps, step_in_episode, num_episode = 0, 0, 0        target_episode = max_episodes * epsilon_factor        while num_episode &lt; max_episodes:            epsilon = self._decayed_epsilon(cur_episode = num_episode,                                            min_epsilon = min_epsilon,                                             max_epsilon = 1,                                            target_episode = target_episode)            self.state = self.env.reset()            # self.env.render()            step_in_episode = 0            loss, mean_loss = 0.00, 0.00            is_done = False            while not is_done:                s0 = self.state                a0  = self.performPolicy(s0, epsilon)                # act方法封装了将Transition记录至Experience中的过程，还记得吗？                s1, r1, is_done, info, total_reward = self.act(a0)                # self.env.render()                step_in_episode += 1                # 当经历里有足够大小的Transition时，开始启用基于经历的学习                if self.total_trans &gt; batch_size:                    loss += self._learn_from_memory(gamma,                                                     batch_size,                                                     learning_rate,                                                    epochs)            mean_loss = loss / step_in_episode            print("&amp;#123;0&amp;#125; epsilon:&amp;#123;1:3.2f&amp;#125;, loss:&amp;#123;2:.3f&amp;#125;".                format(self.experience.last, epsilon, mean_loss))            # print(self.experience)            total_steps += step_in_episode            num_episode += 1        return</code></pre><ul><li><code>learning</code>辅助代码</li></ul><p>由于使用了Actor、Q值网络，无法使用表搜索的方法，根据Q值获取最佳的action，所以对<code>curpolicy</code>等方法进行了重写</p><pre><code>- 衰减的$\epsilon$-greedy方法</code></pre><pre><code class="lang-python">def _decayed_epsilon(self,cur_episode: int,                           min_epsilon: float,                           max_epsilon: float,                           target_episode: int) -&gt; float:    '''获得一个在一定范围内的epsilon    '''    slope = (min_epsilon - max_epsilon) / (target_episode)    intercept = max_epsilon    return max(min_epsilon, slope * cur_episode + intercept)</code></pre><pre><code>- 选择动作方法</code></pre><pre><code class="lang-python">def _curPolicy(self, s, epsilon = None):    '''依据更新策略的价值函数(网络)产生一个行为    '''    Q_s = self.PQ(s)    rand_value = random()    if epsilon is not None and rand_value &lt; epsilon:        return self.env.action_space.sample()    else:        return int(np.argmax(Q_s))</code></pre><pre><code>- 执行策略方法</code></pre><pre><code class="lang-python">def performPolicy(self, s, epsilon = None):    return self._curPolicy(s, epsilon)</code></pre><pre><code>- 将训练的权重参数赋值给正在执行价值估计的网络</code></pre><pre><code class="lang-python">def _update_Q_net(self):    '''将更新策略的Q网络(连带其参数)复制给输出目标Q值的网络    '''    self.Q = self.PQ.clone()</code></pre><h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><ul><li>调用了<code>gym</code>调用了环境</li><li>调用<code>wrappers</code>监控变量</li><li>创建了基于Q值估计的Agent</li><li>初始化环境、开始学习</li></ul><pre><code class="lang-python">from random import random, choicefrom gym import Envimport gymfrom gridworld import *from core import Transition, Experience, Agentfrom approximator import Approximatorfrom agents import ApproxQAgentimport torchdef testApproxQAgent():    env = gym.make("MountainCar-v0")    #env = SimpleGridWorld()    directory = "/home/qiang/workspace/reinforce/monitor"    env = gym.wrappers.Monitor(env, directory, force=True)    agent = ApproxQAgent(env,                         trans_capacity = 10000,    # 记忆容量（按状态转换数计）                         hidden_dim = 16)           # 隐藏神经元数量    env.reset()    print("Learning...")      agent.learning(gamma=0.99,          # 衰减引子                   learning_rate = 1e-3,# 学习率                   batch_size = 64,     # 集中学习的规模                   max_episodes=2000,   # 最大训练Episode数量                   min_epsilon = 0.01,   # 最小Epsilon                   epsilon_factor = 0.3,# 开始使用最小Epsilon时Episode的序号占最大                                        # Episodes序号之比，该比值越小，表示使用                                        # min_epsilon的episode越多                   epochs = 2           # 每个batch_size训练的次数                   )if __name__ == "__main__":    testApproxQAgent()</code></pre><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> Sarsa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL实践2——RL环境gym搭建</title>
      <link href="/posts/rl-pr2-gym/"/>
      <url>/posts/rl-pr2-gym/</url>
      
        <content type="html"><![CDATA[<h1 id="RL回顾"><a href="#RL回顾" class="headerlink" title="RL回顾"></a>RL回顾</h1><p>首先先来回顾一下强化学习问题中，环境Env 和 代理Agent 分别承担的角色和作用。</p><p>RL组成要素是Agent、Env<br><img src="/img/15972068404024.png" alt=""></p><p><strong>代理</strong>和<strong>环境</strong> 分别承担的作用</p><ul><li>Agent：<br>由Policy 和 RL_Algorithm构成，这种对RL_algorithm的算法理解比较宽泛<ul><li>policy负责将observation映射为action</li><li>RL_Algorithm负责优化policy，具有学习和搜索（规划）的能力</li></ul></li><li>Enviroment：<ul><li>输入action</li><li>输出reward、state</li><li>内部还需要完成执行状态转移、判断是否终止等任务</li></ul></li><li>Agent的构成的另一种理解<br>组成要素：Policy、Value function、Model其中至少一个<br><img src="/img/15958481757170.jpg" alt="-w410"></li></ul><h1 id="gym"><a href="#gym" class="headerlink" title="gym"></a>gym</h1><h2 id="gym介绍"><a href="#gym介绍" class="headerlink" title="gym介绍"></a>gym介绍</h2><p>gym是一个热门的学习库，搭建了简单的示例，其主要完成的功能，是完成了RL问题中Env的搭建。</p><ul><li>对于强化学习算法的研究者，可以快速利用多种不同的环境验证迭代自己的算法有效性。</li><li>对于强化学习应用的研究者，我们可以效仿gym中的接口，搭建自己的环境。</li></ul><h2 id="gym定义"><a href="#gym定义" class="headerlink" title="gym定义"></a>gym定义</h2><p>gym是一个class 的形式，完成了接口定义和调用</p><p>gym的核心代码写在<code>core.py</code>里，定义两个最基本的基类<code>Env</code>和<code>Space</code></p><ul><li><code>Space</code><ul><li><code>Discrete</code>类，定义离散状态、动作，初始化需要1个参数，维度n</li><li><code>Box</code>类，定义连续状态、动作，初始化需要2个array（size=维数n）</li></ul></li></ul><p>伪代码如下：</p><ul><li>环境Env</li></ul><pre><code class="lang-python">class Environment():    self.states  # 所有可能的状态集合    self.agent_cur_state    # 记录个体当前的状态    self.observation_space  # 个体的观测空间    self.action_space  # 个体的行为空间    def reward(self) -&gt; reward # 根据状态确定个体的即时奖励    def dynamics(self, action) -&gt; None # 根据当前状态和个体的行为确定个体的新状态    def is_episode_end(self) -&gt; Bool # 判断是否一个Episode结束def obs_for_agent() -&gt; obs  # 环境把个体当前状态做一定变换，作为个体的观测</code></pre><ul><li>代理Agent</li></ul><pre><code class="lang-python">class Agent(env: Environment):    self.env = env  # 个体依附于一个环境存在    self.obs # 个体的观测    self.reward # 个体获得的即时奖励    def performPolicy(self, obs) -&gt; action # 个体执行一个策略产生一个行为    def performAction(self, action) -&gt; None  # 个体与环境交互，执行行为        action = self.performPolicy(self.obs)        self.env.dynamics(action)    def observe(self) -&gt; next_obs, reward # 个体得到从环境反馈来的观测和奖励        self.obs = self.env.obs_for_agent()        self.reward = self.env.reward()</code></pre><h2 id="gym调用"><a href="#gym调用" class="headerlink" title="gym调用"></a>gym调用</h2><p>gym的调用框架</p><pre><code class="lang-python">env = gym.make('x')observation = env.reset()for i in range(time_steps):    env.render() # 调用第三方库刷动画写这里    action = policy(observation)    observation, reward, done, info = env.step(action)    if done:        ……        breakenv.close()</code></pre><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p><img src="/img/15991043562505.jpg" alt=""></p><p>例程是一个简单的策略，杆左斜车左移，右斜则右移。</p><pre><code class="lang-python">import gymimport numpy as npenv = gym.make('CartPole-v0')t_all = [ ]action_bef = 0for i_episode in range(5):    observation = env.reset()    for t in range(100):        env.render()        cp, cv, pa, pv = observation        if abs(pa)&lt;= 0.1:            action = 1 -action_bef        elif pa &gt;= 0:            action = 1        elif pa &lt;= 0:            action = 0        observation, reward, done, info = env.step(action)        action_bef = action        if done:            # print("Episode finished after &amp;#123;&amp;#125; timesteps".format(t+1))            t_all.append(t)            break        if t ==99:            t_all.append(0)env.close()print(t_all)print(np.mean(t_all))</code></pre><h2 id="gym的搭建"><a href="#gym的搭建" class="headerlink" title="gym的搭建"></a>gym的搭建</h2><h3 id="函数接口"><a href="#函数接口" class="headerlink" title="函数接口"></a>函数接口</h3><p>一个完整的gym环境包括以下函数：</p><ul><li>class Cartpoleenv(gym.env)<ul><li><code>def __ init __(self)</code>：类构建</li><li><code>def reset(self)</code>：初始化</li><li><code>def seed(self, seed = None)</code>：随机初始条件种子<code>return [seed]</code></li><li><code>def step(self, action)</code>: 单步仿真<code>observation, reward, done, info</code></li><li><code>def render(self, mode='human')</code>：图像引擎调用绘制窗口<code>return self.viewer.render()</code></li><li><code>def close()</code>：关闭窗口</li></ul></li></ul><h2 id="功能函数"><a href="#功能函数" class="headerlink" title="功能函数"></a>功能函数</h2><ul><li><p>参数限位<br><code>vel = np.clip(vel, vel_min, vel_max)</code></p></li><li><p>action输入校验<br><code>self.action_space.contains(action)</code></p></li><li><p>action和observation空间定义</p></li></ul><p>例子：<br><code>Discrete</code>: 0,1,2三个离散值</p><pre><code class="lang-python">low = np.array([min_0,min_1],dtype=np.float32)high = np.array([max_0,max_1],dtype=np.float32)</code></pre><pre><code>self.action_space = spaces.Discrete(3)self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)</code></pre><h2 id="agent-的构建"><a href="#agent-的构建" class="headerlink" title="agent 的构建"></a>agent 的构建</h2><p>agent与环境进行交互，输入是env的输出（observation），输出是env的输入（action）</p><pre><code>class Agent():    def __ init__(self,action_space):        self.action_space = action_space    def act(self, observation, reward, done):        return action</code></pre><h2 id="agent和env交互逻辑如下："><a href="#agent和env交互逻辑如下：" class="headerlink" title="agent和env交互逻辑如下："></a>agent和env交互逻辑如下：</h2><pre><code>nb_episodes = xxnb_steps = xxreward = 0done = False        for i in range(nb_episodes):    ob = env.reset()    sum_reward = 0    for j in range(nb_steps):        action = agent.act(ob, reward, done)        ob, reward, done, _ = env.step(action)        sum_reward += reward        if done:            break</code></pre><h1 id="添加自己写的环境到gym，方便调用"><a href="#添加自己写的环境到gym，方便调用" class="headerlink" title="添加自己写的环境到gym，方便调用"></a>添加自己写的环境到gym，方便调用</h1><h2 id="设置过程"><a href="#设置过程" class="headerlink" title="设置过程"></a>设置过程</h2><ol><li>打开gym.envs目录：<code>/usr/local/lib/python3.7/site-packages/gym/envs</code></li><li>将自己编写的myenv.py拷贝至一个<code>custom</code>目录</li><li><code>envs/custom</code>下<code>__init__.py</code>添加<br><code>from gym.envs.custom.myenv import MyEnv</code><br>，将子文件夹的<code>.py</code>import到上层目录</li><li>env下<code>__init__.py</code>添加</li></ol><pre><code class="lang-python">register(id='myenv-v0',entry_point='gym.envs.custom:MyEnv,max_episode_steps=999,      #限制了最大终止仿真步数)</code></pre><p>授权<code>gym</code>的方法可以调用<code>myenv.py</code>中的<code>MyEnv</code>class</p><ul><li>注意：<ul><li><code>__init__.py</code>里的<code>register</code>方法中<code>env_name</code>版本号<code>-v0</code>不能省略</li><li>调用的时候，也要带上环境相应的版本号</li></ul></li></ul><h2 id="调用方法"><a href="#调用方法" class="headerlink" title="调用方法"></a>调用方法</h2><pre><code class="lang-python">env_name = 'myenv-v0'env = gym.make('env_name')env.reset()     # 初始化环境env.render()    # 绘制环境，if necessaryenv.step()      # 单步仿真env.close()     # 关闭环境，一般涉及图像绘制的任务，此步为必须</code></pre><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> gym </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL实践1——动态规划值迭代</title>
      <link href="/posts/rl-pr-1-dp/"/>
      <url>/posts/rl-pr-1-dp/</url>
      
        <content type="html"><![CDATA[<h1 id="RL实践1——值迭代求解随机策略"><a href="#RL实践1——值迭代求解随机策略" class="headerlink" title="RL实践1——值迭代求解随机策略"></a>RL实践1——值迭代求解随机策略</h1><p>参考自叶强《强化学习》第三讲，方格世界—— 使用 动态规划 求解随机策略</p><p>动态规划的使用条件时MDP已知，在简单游戏中，这个条件时显然成立的<br>使用Value iteration的方法求解每个状态的价值函数，迭代收敛之后，对应最优策略生成。</p><p>注意：动态规划和强化学习都用的价值函数，区别在于</p><ul><li><strong>动态规划</strong>需要基于模型获取采取动作后下一时刻的状态，已进行评估，需要MDP模型已知；</li><li><strong>强化学习</strong>无模型的学习方法，可以基于采样，对episode的状态（动作）价值函数进行学习。</li></ul><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p><img src="https://pic1.zhimg.com/80/v2-b4049aad5b8fb2d43138b8bfccc3d6cb_720w.png" alt=""><br>从方格状态走到终止状态（灰色标记）</p><h2 id="Python代码及注释"><a href="#Python代码及注释" class="headerlink" title="Python代码及注释"></a>Python代码及注释</h2><p>值得注意的是，知乎原版的注释是错误的，采用的是同步更新</p><p>有三个trick可以加快运算速度（对于大规模问题）</p><ul><li>in-place DP：新值直接替换旧值，只存储一个v(s)，<ul><li>异步更新，提高效率</li><li>缺点：更新顺序影响收敛性</li></ul></li><li>Prioritised sweeping：state的影响力排序<ul><li>比较贝尔曼误差绝对值，大的更新，小的忽略</li></ul></li><li>Real-time DP：遍历过的才更新<ul><li>省去了agent 未遍历的状态s，对于稀疏任务效率提升极大</li></ul></li></ul><pre><code class="lang-python"># 状态集合states = [i for i in range(16)]# 价值集合values = [0 for _ in range(16)]# 动作集：actions = ["n", "e", "s", "w"]# 动作字典：ds_actions = &amp;#123;"n": -4, "e": 1, "s": 4, "w": -1&amp;#125;# 衰减率gamma = 1.00# 定义MDPdef nextState(s, a):    next_state = s    if (s%4 == 0 and a == "w") or (s&lt;4 and a == "n") or \      ((s+1)%4 == 0 and a == "e") or (s &gt; 11 and a == "s"):        pass    else:        ds = ds_actions[a]        next_state = s + ds    return next_state# 定义奖励def rewardOf(s):    return 0 if s in [0, 15] else -1# 判断是否结束def isTerminateState(s):    return s in [0, 15]# 获取所有可能的next state 集合def getSuccessors(s):    successors = []    if isTerminateState(s):        return successors    for a in actions:        next_state = nextState(s, a)        # if s != next_state:        successors.append(next_state)    return successors# 更新当前位置的价值函数def updateValue(s):    sucessors = getSuccessors(s)    newValue = 0  # values[s]    num = 4  # len(successors)    reward = rewardOf(s)    for next_state in sucessors:        newValue += 1.00 / num * (reward + gamma * values[next_state])    return newValue# 打印所有状态对应价值函数def printValue(v):    for i in range(16):        print('&amp;#123;0:&gt;6.2f&amp;#125;'.format(v[i]), end=" ")        if (i + 1) % 4 == 0:            print("")    print()# 一次迭代# 这里采用的是同步更新，不是异步更新。创建了newvalues数组，遍历过states后，统一更新global valuesdef performOneIteration():    newValues = [0 for _ in range(16)]    for s in states:        newValues[s] = updateValue(s)    global values    values = newValues    printValue(values)</code></pre><pre><code class="lang-python"># 主函数def main():    max_iterate_times = 160    cur_iterate_times = 0    while cur_iterate_times &lt;= max_iterate_times:        print("Iterate No.&amp;#123;0&amp;#125;".format(cur_iterate_times))        performOneIteration()        cur_iterate_times += 1    printValue(values)</code></pre><pre><code class="lang-python">if __name__ == '__main__':    main()</code></pre><h2 id="运算结果如下"><a href="#运算结果如下" class="headerlink" title="运算结果如下"></a>运算结果如下</h2><pre><code>Iterate No.0  0.00  -1.00  -1.00  -1.00  -1.00  -1.00  -1.00  -1.00  -1.00  -1.00  -1.00  -1.00  -1.00  -1.00  -1.00   0.00 Iterate No.1  0.00  -1.75  -2.00  -2.00  -1.75  -2.00  -2.00  -2.00  -2.00  -2.00  -2.00  -1.75  -2.00  -2.00  -1.75   0.00 ...Iterate No.158  0.00 -14.00 -20.00 -22.00 -14.00 -18.00 -20.00 -20.00 -20.00 -20.00 -18.00 -14.00 -22.00 -20.00 -14.00   0.00 Iterate No.159  0.00 -14.00 -20.00 -22.00 -14.00 -18.00 -20.00 -20.00 -20.00 -20.00 -18.00 -14.00 -22.00 -20.00 -14.00   0.00 Iterate No.160  0.00 -14.00 -20.00 -22.00 -14.00 -18.00 -20.00 -20.00 -20.00 -20.00 -18.00 -14.00 -22.00 -20.00 -14.00   0.00   0.00 -14.00 -20.00 -22.00 -14.00 -18.00 -20.00 -20.00 -20.00 -20.00 -18.00 -14.00 -22.00 -20.00 -14.00   0.00</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 dynamic programming </tag>
            
            <tag> 强化学习实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记10：经典游戏示例 classic games</title>
      <link href="/posts/rl-10/"/>
      <url>/posts/rl-10/</url>
      
        <content type="html"><![CDATA[<h1 id="1、前沿-state-of-art"><a href="#1、前沿-state-of-art" class="headerlink" title="1、前沿 state of art"></a>1、前沿 state of art</h1><h2 id="学习经典游戏的原因"><a href="#学习经典游戏的原因" class="headerlink" title="学习经典游戏的原因"></a>学习经典游戏的原因</h2><ul><li>规则简单，细思又很深入</li><li>历史悠久，已经被研究了几百年</li><li>对IQ测试有意义</li><li>是现实世界的问题的缩影</li></ul><p>已经有很多RL案例，战胜了人类，例如<br><img src="/img/15988436216244.jpg" alt=""></p><h1 id="2、游戏理论-game-theory"><a href="#2、游戏理论-game-theory" class="headerlink" title="2、游戏理论 game theory"></a>2、游戏理论 game theory</h1><h2 id="游戏的最优性"><a href="#游戏的最优性" class="headerlink" title="游戏的最优性"></a>游戏的最优性</h2><p>对于石头剪刀布来说，最优策略，显然和对手agent策略相关，我们期望找到一种一致的策略策略，对所有对手都有效<br>什么是第i个玩家的最优策略$\pi$</p><ul><li><strong>最佳响应</strong> best response $\pi^i_*(\pi^{-i})$ 是针对其他agent的最优策略</li><li><strong>纳什平衡点</strong> Nash equilibrium是针对所有对手的联合策略<script type="math/tex; mode=display">\pi^i = \pi^i_*(\pi^{-i})</script></li></ul><p>对于agent来说的最优策略，是一种general 的 策略，对大多数情况，都适用一致的策略去action.</p><h2 id="单agent-自驱动-强化学习"><a href="#单agent-自驱动-强化学习" class="headerlink" title="单agent 自驱动 强化学习"></a>单agent 自驱动 强化学习</h2><ul><li>最佳响应 是 单代理RL问题的解决方案<ul><li>其他玩家 变成环境的一部分</li><li>将游戏 抽象为MDP</li><li>最佳策略是 最佳响应</li></ul></li><li>纳什平衡点 在 自学习RL问题中是 不动点<ul><li>学习的经验是 代理玩游戏产生的<script type="math/tex; mode=display">a_{1} \sim \pi^{1}, a_{2} \sim \pi^{2}, \ldots</script></li><li>每个代理学习针对其他玩家的最佳响应</li><li>代理的策略决定了其他代理的环境</li><li>所有的代理适应其他代理</li></ul></li></ul><h2 id="二人零和博弈游戏"><a href="#二人零和博弈游戏" class="headerlink" title="二人零和博弈游戏"></a>二人零和博弈游戏</h2><p>收益来自其他agent，一方受益，意味着其他亏损</p><script type="math/tex; mode=display">R^1 + R^2 = 0</script><p>methods for finding 纳什平衡点</p><ul><li>Game tree search （i.e. planning）</li><li>自驱动RL</li></ul><h2 id="perfect-and-imperfect-information-games"><a href="#perfect-and-imperfect-information-games" class="headerlink" title="perfect and imperfect information games"></a>perfect and imperfect information games</h2><ul><li>完美信息或者 马尔科夫游戏是  <strong>完全可观的</strong><ul><li>象棋</li><li>围棋</li><li>跳棋</li><li>五子棋</li></ul></li><li>不完全信息游戏是<strong>部分</strong>可观的<ul><li>扑克</li><li>拼图</li></ul></li></ul><h1 id="3、最小、最大搜索"><a href="#3、最小、最大搜索" class="headerlink" title="3、最小、最大搜索"></a>3、最小、最大搜索</h1><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><ul><li><p>价值函数定义了策略$\pi$下的价值</p><script type="math/tex; mode=display">v\pi(s) = \mathcal E_\pi [G_t|S_t= s]</script></li><li><p>最小、最大化价值函数，是在降低其他代理表现的同时，最大化自己的价值</p></li></ul><script type="math/tex; mode=display">v_{*}(s)=\max _{\pi^{1}} \min _{\pi^{2}} v_{\pi}(s)</script><ul><li><p>最小、最大搜索存在纳什平衡点</p></li><li><p>通过深度优先树搜索，找到极值</p></li></ul><p><img src="/img/15988568758900.jpg" alt=""><br>从下往上找：<br>一步找max，一步找min<br>缺点是，运算量指数增长，不能求解整个树的分支<br>Solution：</p><ul><li>用值函数估计器，估计叶节点</li><li>根据节点值，限制搜索深度</li></ul><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><h3 id="二进制-线性组合-值函数"><a href="#二进制-线性组合-值函数" class="headerlink" title="二进制 线性组合 值函数"></a>二进制 线性组合 值函数</h3><ul><li>每 个状态特征，只有0、1</li><li>每个特征对应权重 w</li><li>线性组合</li></ul><p><img src="/img/15988595984971.jpg" alt=""></p><h3 id="深蓝-Deep-blue，并不是真正的学习，手动权重"><a href="#深蓝-Deep-blue，并不是真正的学习，手动权重" class="headerlink" title="深蓝 Deep blue，并不是真正的学习，手动权重"></a>深蓝 Deep blue，并不是真正的学习，手动权重</h3><ul><li>知识 Knowledge<ul><li>8k个手动特征</li><li>二进制线性组合价值函数</li><li>人工个调参 权重</li></ul></li><li>搜索 Search<ul><li>高性能平行字母搜索</li><li>40步预测</li><li>每秒 </li></ul></li><li>结果 Results<ul><li>击败了世界冠军</li></ul></li></ul><h3 id="Chinook"><a href="#Chinook" class="headerlink" title="Chinook"></a>Chinook</h3><ul><li>知识 Knowledge<ul><li>二进制线性组合价值函数</li><li>21个经验权重（位置、流动性）</li><li>四象限</li></ul></li><li>搜索 Search<ul><li>高性能平行字母搜索</li><li>逆向搜索<ul><li>从赢的位置从后向前搜索</li><li>存储所有决胜点位置在 lookup 表中</li><li>在最后n步，表现完美</li></ul></li></ul></li><li>结果 Results<ul><li>击败了世界冠军</li></ul></li></ul><h1 id="4、自驱动强化学习-self-play-reinforcement-learning"><a href="#4、自驱动强化学习-self-play-reinforcement-learning" class="headerlink" title="4、自驱动强化学习 self-play reinforcement learning"></a>4、自驱动强化学习 self-play reinforcement learning</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>应用 Value-based RL，完成游戏自学</p><ul><li><p>MC 向$G_t$更新</p><script type="math/tex; mode=display">\Delta \mathbf{w}=\alpha\left(G_{t}-v\left(S_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} v\left(S_{t}, \mathbf{w}\right)</script></li><li><p>TD（0）向$v(s_t +1)$更新</p><script type="math/tex; mode=display">\Delta \mathbf{w}=\alpha\left(v\left(S_{t+1}, \mathbf{w}\right)-v\left(S_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} v\left(S_{t}, \mathbf{w}\right)</script></li><li><p>TD（$\lambda$）向$\lambda$-return $G_t^\lambda$更新</p><script type="math/tex; mode=display">\Delta \mathbf{w}=\alpha\left(G_{t}^{\lambda}-v\left(S_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} v\left(S_{t}, \mathbf{w}\right)</script></li></ul><h2 id="策略提升-Policy-improvement"><a href="#策略提升-Policy-improvement" class="headerlink" title="策略提升 Policy improvement"></a>策略提升 Policy improvement</h2><p>规则的定义决定了<strong>后继者</strong>的状态 $succ(s,a)$</p><p>对于确定性的游戏，估计价值函数是足够的</p><script type="math/tex; mode=display">q_*(s,a) = v_*(succ(s,a))</script><p>同样采用最小最大优化</p><script type="math/tex; mode=display">A_{t}=\underset{a}{\operatorname{argmax}} v_{*}\left(\operatorname{succ}\left(S_{t}, a\right)\right) \ for\  white\\ A_{t}=\underset{a}{\operatorname{argmin}} v_{*}\left(\operatorname{succ}\left(S_{t}, a\right)\right) \ for\  black</script><h3 id="Self-play-TD-in-Othello：-logistello"><a href="#Self-play-TD-in-Othello：-logistello" class="headerlink" title="Self-play TD in Othello： logistello"></a>Self-play TD in Othello： logistello</h3><p><img src="/img/15988757317734.jpg" alt=""></p><p>使用了策略迭代的方法：</p><ul><li>用2个代理进行对抗</li><li>用MC 评估 策略</li><li>Greedy 策略优化</li></ul><p>6：0战胜世界冠军</p><h2 id="TD-Gammon-非线性价值函数估计"><a href="#TD-Gammon-非线性价值函数估计" class="headerlink" title="TD Gammon: 非线性价值函数估计"></a>TD Gammon: 非线性价值函数估计</h2><p><img src="/img/15988766455259.jpg" alt=""></p><h3 id="自学习-TD-在西洋双陆棋-Backgammon"><a href="#自学习-TD-在西洋双陆棋-Backgammon" class="headerlink" title="自学习 TD 在西洋双陆棋 Backgammon"></a>自学习 TD 在西洋双陆棋 Backgammon</h3><ol><li>权重随机初始化</li><li>自学习训练</li><li>使用非线性TD 学习算法<script type="math/tex; mode=display">\begin{aligned}\delta_{t} &=v\left(S_{t+1}, \mathbf{w}\right)-v\left(S_{t}, \mathbf{w}\right) \\\Delta \mathbf{w} &=\alpha \delta_{t} \nabla_{\mathbf{w}} v\left(S_{t}, \mathbf{w}\right)\end{aligned}</script></li><li>Greedy 策略优化</li></ol><p>TD gammon 的几个层级：</p><ul><li>zero 专家经验</li><li>人造特征</li><li>n层极小极大搜索</li></ul><p>隐藏层个数、 训练代数，直接影响模型表现<br><img src="/img/15988796994616.jpg" alt=""></p><h1 id="5、联合强化学习和最大化搜索"><a href="#5、联合强化学习和最大化搜索" class="headerlink" title="5、联合强化学习和最大化搜索"></a>5、联合强化学习和最大化搜索</h1><h2 id="简单-TD-Simple-TD"><a href="#简单-TD-Simple-TD" class="headerlink" title="简单 TD Simple TD"></a>简单 TD Simple TD</h2><p>TD：向继承者的方向更新价值函数</p><p><img src="/img/15988798436692.jpg" alt=""></p><p>分为两步</p><ul><li>用TD learning 学习价值函数</li><li>用价值函数 进行 最小最大搜索</li></ul><script type="math/tex; mode=display">v_{+}\left(S_{t}, \mathbf{w}\right)=\operatorname{minimax}_{s \in \text {leaves}\left(S_{t}\right)} v(s, \mathbf{w})</script><p>在有些情景表现优异，有些糟糕</p><h2 id="TD-root"><a href="#TD-root" class="headerlink" title="TD root"></a>TD root</h2><p>TD root：从继承者 <strong>搜索值</strong>更新 <strong>价值函数</strong><br><img src="/img/15988802345558.jpg" alt=""></p><ul><li>搜索值 根据 根节点计算得到<script type="math/tex; mode=display">v_{+}\left(S_{t}, \mathbf{w}\right)=\underset{s \in \text { leaves }}{\operatorname{minimax}} \left(S_{t}\right) v(s, \mathbf{w})</script></li><li>从下一个状态的 搜索值 备份 值函数<script type="math/tex; mode=display">v\left(S_{t}, \mathbf{w}\right) \leftarrow v_{+}\left(S_{t+1}, \mathbf{w}\right)=v\left(l_{+}\left(S_{t+1}\right), \mathbf{w}\right)</script></li><li>$I_+(s)$是 从状态s 进行极小极大搜索后 的 <strong>叶</strong>节点值</li></ul><h2 id="TD-leaf"><a href="#TD-leaf" class="headerlink" title="TD leaf"></a>TD leaf</h2><p>TD leaf：从继承者的 <strong>搜索值</strong> 更新 <strong>搜索值</strong><br><img src="/img/15990406257991.jpg" alt=""></p><ul><li>搜索值 由当前和 下一个状态计算得到</li></ul><p>这个公式无法显示</p><pre><code>v_&amp;#123;+&amp;#125;\left(S_&amp;#123;t&amp;#125;, \mathbf&amp;#123;w&amp;#125;\right)=\underset&amp;#123;&amp;#123;s \in \text &amp;#123; leaves &amp;#125;\left(S_&amp;#123;t&amp;#125;\right)&amp;#125;&amp;#125;&amp;#123;\rm&amp;#123;minimax&amp;#125;&amp;#125; v(s, \mathbf&amp;#123;w&amp;#125;)\\v_&amp;#123;+&amp;#125;\left(S_&amp;#123;t+1&amp;#125;, \mathbf&amp;#123;w&amp;#125;\right)=\underset&amp;#123;&amp;#123;s \in \text &amp;#123; leaves &amp;#125;\left(S_&amp;#123;t+1&amp;#125;\right)&amp;#125;&amp;#125;&amp;#123;\rm&amp;#123;minimax&amp;#125;&amp;#125; v(s, \mathbf&amp;#123;w&amp;#125;)</code></pre><p><img src="/img/MommyTalk1599040691831.jpg" alt="MommyTalk1599040691831"></p><ul><li>t时刻的搜索值 由 t+1时刻的搜索值备份得到<script type="math/tex; mode=display">\begin{aligned}v_{+}\left(S_{t}, \mathbf{w}\right) & \leftarrow v_{+}\left(S_{t+1}, \mathbf{w}\right) \\\Longrightarrow v\left(l_{+}\left(S_{t}\right), \mathbf{w}\right) & \leftarrow v\left(l_{+}\left(S_{t+1}\right), \mathbf{w}\right)\end{aligned}</script></li></ul><h3 id="examples："><a href="#examples：" class="headerlink" title="examples："></a>examples：</h3><h4 id="TD-leaf-in-chess：-knightcap"><a href="#TD-leaf-in-chess：-knightcap" class="headerlink" title="TD leaf in chess： knightcap"></a>TD leaf in chess： knightcap</h4><ul><li>learning<ul><li>训练专家对手</li><li>使用TD leaf 学习权重</li></ul></li><li>搜索<ul><li>alpha-beta search </li></ul></li><li>Results<ul><li>master level 完成少数的游戏之后</li><li>不够高效 in 自学习</li><li>不够高效，受初始权重影响较大</li></ul></li></ul><h4 id="TD-leaf-in-Checkers：-Chinook"><a href="#TD-leaf-in-Checkers：-Chinook" class="headerlink" title="TD leaf in Checkers： Chinook"></a>TD leaf in Checkers： Chinook</h4><ul><li>初始的chinook采用手动调优的权重</li><li>后来的版本自训练</li><li>采用Td leaf 调整权重<ul><li>固定了专家</li></ul></li><li>自学习权重的表现 ＞ 人工调优权重的表现</li><li>超过人类水平</li></ul><h2 id="TreeStrap"><a href="#TreeStrap" class="headerlink" title="TreeStrap"></a>TreeStrap</h2><ul><li>TreeStrap：用深层的<strong>搜索值</strong> 更新 浅层的 <strong>搜索值</strong></li></ul><p><img src="/img/15989616364756.jpg" alt=""></p><ul><li>在所有节点 计算 极小、极大搜索</li><li>价值从搜索值备份得到，在同一个step，对所有节点</li></ul><script type="math/tex; mode=display">\begin{aligned}v\left(s, \mathbf{w}\right) & \leftarrow v_{+}\left(s, \mathbf{w}\right) \\\Longrightarrow v\left( s, \mathbf{w}\right) & \leftarrow v\left(l_{+}\left(s \right), \mathbf{w}\right)\end{aligned}</script><h3 id="Treestrap-in-chess-：meep"><a href="#Treestrap-in-chess-：meep" class="headerlink" title="Treestrap in chess ：meep"></a>Treestrap in chess ：meep</h3><ul><li>2k个特征，二进制线性组合价值函数</li><li>随机初始权重</li><li>权重调节方式：Treestrap</li><li>自驱动学习过程表现高效：利用率高</li><li>随机权重情况下表现良好</li></ul><h2 id="Simulation-based-Search"><a href="#Simulation-based-Search" class="headerlink" title="Simulation-based Search"></a>Simulation-based Search</h2><ul><li>自驱动RL 可以替代 搜索</li><li>基于仿真的游戏从根节点 $s_t$开始</li><li>应用RL 到 仿真经验<ul><li>MC control $\Rightarrow$ MC tree search</li><li>最高效的变体算法是 UCT 算法<ul><li>使用置信上界UCB 来平衡探索和利用</li></ul></li><li>自驱动 UCT 收敛于 极小极大价值函数</li><li>在完美信息游戏、不完美信息游戏均表现良好</li></ul></li></ul><h2 id="MCTS蒙特卡洛树搜索-表现in-games"><a href="#MCTS蒙特卡洛树搜索-表现in-games" class="headerlink" title="MCTS蒙特卡洛树搜索 表现in games"></a>MCTS蒙特卡洛树搜索 表现in games</h2><p><img src="/img/15989661936191.jpg" alt=""></p><h3 id="简单蒙特卡洛搜索-in-Maven（拼字游戏）"><a href="#简单蒙特卡洛搜索-in-Maven（拼字游戏）" class="headerlink" title="简单蒙特卡洛搜索 in Maven（拼字游戏）"></a>简单蒙特卡洛搜索 in Maven（拼字游戏）</h3><p><img src="/img/15989726953505.jpg" alt="-w336"></p><ul><li><p>学习 价值函数</p><ul><li>二进制价值函数</li><li>MC policy iteration</li></ul></li><li><p>搜索 价值函数，</p><ul><li>搜索n步</li><li>使用学到的价值函数评价 当前状态</li><li>x</li><li>选择高分动作</li><li>特定的endgame 用$B^*$</li></ul></li></ul><h1 id="6、在非完整信息中的强化学习"><a href="#6、在非完整信息中的强化学习" class="headerlink" title="6、在非完整信息中的强化学习"></a>6、在<strong>非</strong>完整信息中的强化学习</h1><h2 id="Game-tree-search-在不完美信息游戏中"><a href="#Game-tree-search-在不完美信息游戏中" class="headerlink" title="Game tree search 在不完美信息游戏中"></a>Game tree search 在不完美信息游戏中</h2><p><img src="/img/15990142381702.jpg" alt=""></p><p>真实的状态可能共享相同的信息状态空间</p><h2 id="Solution："><a href="#Solution：" class="headerlink" title="Solution："></a>Solution：</h2><ul><li>Iterative forward-search mehtods<ul><li>e.g. 反事实的 后悔值最小化</li></ul></li><li>自驱动RL</li><li>e.g. smooth UCT</li></ul><h3 id="Smooth-UCT-search"><a href="#Smooth-UCT-search" class="headerlink" title="Smooth UCT search"></a>Smooth UCT search</h3><ul><li>应用 MCTS 到 信息状态游戏树</li><li>UCT的变种，由博弈论的虚拟play启发<ul><li>代理agent根据对手的平均行为作出 动作 并 学习</li></ul></li><li><p>从节点的动作计数中 提取 平均策略</p><script type="math/tex; mode=display">\pi_{a v g}(a \mid s)=\frac{N(s, a)}{N(s)}</script></li><li><p>对每个节点，根据UCT概率选择动作</p><script type="math/tex; mode=display">A \sim\left\{\begin{array}{ll}\text { UCT }(S), & \text { with probability } \eta \\\pi_{\text {avg}}(\cdot \mid S), & \text { with probability } 1-\eta\end{array}\right.</script></li><li><p>经验</p><ul><li>Naive MCTS 发散</li><li>Smooth UCT 收敛到纳什平衡点</li></ul></li></ul><h1 id="7、结论"><a href="#7、结论" class="headerlink" title="7、结论"></a>7、结论</h1><p><img src="/img/15989735334405.jpg" alt="-w621"></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> 经典游戏示例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记9：探索和利用 exploration and exploitation</title>
      <link href="/posts/rl-9/"/>
      <url>/posts/rl-9/</url>
      
        <content type="html"><![CDATA[<h1 id="1、introduction"><a href="#1、introduction" class="headerlink" title="1、introduction"></a>1、introduction</h1><p>本章的主题是关于利用和探索的矛盾：</p><ul><li>Exploitation：利用当前已知信息做决策</li><li>Exploration：探索未知空间获取更多信息</li></ul><p>最佳的策略是用长期的眼光来看，放弃短期高回报<br>获取足够策略是让策略变成全局最优的必要条件</p><p>几个基本的探索方法：<br>主要分三类：</p><ol><li>随机</li><li>基于不确定性</li><li>信息状态空间</li></ol><ul><li><strong>朴素探索</strong>(Naive Exploration): 在贪婪搜索的基础上增加一个Ɛ以实现朴素探索；</li><li><strong>乐观初始估计</strong>(Optimistic Initialization): 优先选择当前被认为是最高价值的行为，除非新信息的获取推翻了该行为具有最高价值这一认知；</li><li><strong>不确定优先</strong>(Optimism in the Face of Uncertainty): 优先尝试不确定价值的行为；</li><li><strong>概率匹配</strong>（Probability Matching): 根据当前估计的概率分布采样行为；</li><li><strong>信息状态搜索</strong>(Information State Search): 将已探索的信息作为状态的一部分联合个体的状态组成新的状态，以新状态为基础进行前向探索。</li><li><strong>状态动作探索</strong>State-action exploration：系统地探索状态和动作空间，类似于查表法</li><li><strong>参数探索</strong>Parameter exploration：<ul><li>动作选择遵照策略$\pi (A|S,u)$</li><li>每隔一段时间，更新策略参数</li><li>优点：连续的探索</li><li>缺点：对状态/动作空间不直观</li></ul></li></ul><h1 id="2、多臂赌博机-Multi-Armed-Bandits"><a href="#2、多臂赌博机-Multi-Armed-Bandits" class="headerlink" title="2、多臂赌博机 Multi-Armed Bandits"></a>2、多臂赌博机 Multi-Armed Bandits</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>一个赌徒面前有N个赌博机,事先他不知道每台赌博机的真实盈利情况,他如何根据每次玩赌博机的结果来选择下次拉哪台或者是否停止赌博,来最大化自己的从头到尾的收益.<br><img src="/img/15984131897792.png" alt="多臂赌博机"></p><p>将MDP简化为$\langle \mathcal A,\mathcal R \rangle$</p><ul><li>动作空间A 已知</li><li>奖励映射函数$\mathcal {R^a(r)} = \mathcal P [R=r|A=a]$ 未知</li><li>目标是最大化累计奖励$\sum^t_{\tau=1} r_\tau$</li></ul><h3 id="后悔值-Regret"><a href="#后悔值-Regret" class="headerlink" title="后悔值 Regret"></a>后悔值 Regret</h3><ul><li>动作价值函数<script type="math/tex; mode=display">Q(a)=\mathbb{E}[r \mid a]</script></li><li>最优价值函数<script type="math/tex; mode=display">V^* = Q(a^*) = \underset{a\in A}{max}Q(a)</script></li><li>后悔机会（one step）<script type="math/tex; mode=display">l_t = \mathcal E[V^* - Q(a_t)]</script></li><li>后悔函数<script type="math/tex; mode=display">L_T = \mathcal E[\sum^t_{\tau = 1}V^* - Q(a_{a_\tau})]</script></li><li>最大化总计奖励 = 最小化 total regret</li></ul><h4 id="total-Regret的表达方式"><a href="#total-Regret的表达方式" class="headerlink" title="total Regret的表达方式"></a>total Regret的表达方式</h4><ul><li>$N_t(a)$是动作a的计数</li><li>gap $\Delta_a$是当前动作价值函数和最优值的偏差</li></ul><p>将上页的后悔价值函数表示为 计数 和 gap 的乘积</p><script type="math/tex; mode=display">\begin{aligned}L_{t} &=\mathbb{E}\left[\sum_{\tau=1}^{t} V^{*}-Q\left(a_{\tau}\right)\right] \\&=\sum_{a \in \mathcal{A}} \mathbb{E}\left[N_{t}(a)\right]\left(V^{*}-Q(a)\right) \\&=\sum_{a \in \mathcal{A}} \mathbb{E}\left[N_{t}(a)\right] \Delta_{a}\end{aligned}</script><p>好的算法让大gap对应的计数最小，但问题是，gaps未知？？？</p><h3 id="线性和次线性的regret"><a href="#线性和次线性的regret" class="headerlink" title="线性和次线性的regret"></a>线性和次线性的regret</h3><p>因为总计后悔值，是累加计算，只要有gap，就会随着时间步增长</p><ul><li>曲线线性增长，表明<ul><li>算法停止探索</li><li>算法卡在局部最优</li></ul></li></ul><p><img src="/img/15983345403824.jpg" alt="-w449"></p><h2 id="2-1-朴素探索-native-exploration"><a href="#2-1-朴素探索-native-exploration" class="headerlink" title="2.1 朴素探索 native exploration"></a>2.1 朴素探索 native exploration</h2><h3 id="greedy：卡在局部最优，总后悔线性增长"><a href="#greedy：卡在局部最优，总后悔线性增长" class="headerlink" title="greedy：卡在局部最优，总后悔线性增长"></a>greedy：卡在局部最优，总后悔线性增长</h3><ul><li>算法目标是估计价值函数$\hat Q_t(a) \approx Q(a)$</li><li><p>使用MC evaluation 估计价值函数，引入平均值的概念</p><script type="math/tex; mode=display">\hat{Q}_{t}(a)=\frac{1}{N_{t}(a)} \sum_{t=1}^{T} r_{t} \mathbf{1}\left(a_{t}=a\right)</script></li><li><p>选取最优动作</p><script type="math/tex; mode=display">a^*_t = \underset{a\in A}{\operatorname {argmax}} \hat Q_t(a)</script></li></ul><p>Greedy 可能卡在永远次优动作，总regret随时间步线性增长</p><h3 id="Solution：乐观初始化-Optimistic-initialisation"><a href="#Solution：乐观初始化-Optimistic-initialisation" class="headerlink" title="Solution：乐观初始化 Optimistic initialisation"></a><strong>Solution</strong>：乐观初始化 Optimistic initialisation</h3><p>理论上，还应该是total regret 线性增长，实际效果却很好，采用递归MC更新Q值</p><script type="math/tex; mode=display">\hat{Q}_{t}\left(a_{t}\right)=\hat{Q}_{t-1}+\frac{1}{N_{t}\left(a_{t}\right)}\left(r_{t}-\hat{Q}_{t-1}\right)</script><p>操作步骤：</p><ul><li>将V值初始化为最大值，$Q(a)= r_{max}$</li><li>act greedily<script type="math/tex; mode=display">A_t = \underset{a\in A}{\operatorname {argmax}} Q_t(a)</script></li><li>鼓励探索未知values<br>但是仍未能避免不幸的采样导致卡在次优</li></ul><h3 id="epsilon-greedy-：小概率随机动作，累加后悔值，导致总后悔值随时间步线性增长"><a href="#epsilon-greedy-：小概率随机动作，累加后悔值，导致总后悔值随时间步线性增长" class="headerlink" title="$\epsilon-greedy$：小概率随机动作，累加后悔值，导致总后悔值随时间步线性增长"></a>$\epsilon-greedy$：小概率随机动作，累加后悔值，导致总后悔值随时间步线性增长</h3><ul><li><p>特性：永远保持探索:</p><ul><li>概率（1-$\epsilon$）$A= \underset{a\in A}{argmax}Q(a)$</li><li>概率（$\epsilon$）随机动作</li></ul></li><li><p>$\epsilon$保证了最小的regret，满足</p></li></ul><script type="math/tex; mode=display">l_t \geq \frac{\epsilon}{A}\sum_{a\in A}\Delta_a</script><p>$\epsilon-greedy$ 仍然是线性总regret，因为会$\epsilon$随机采样到大gap</p><h3 id="Solution：选择策略让-epsilon-递减"><a href="#Solution：选择策略让-epsilon-递减" class="headerlink" title="Solution：选择策略让$\epsilon$递减"></a><strong>Solution</strong>：选择策略让$\epsilon$递减</h3><p>核心思想：假设我们现在知道每一个行为的最优价值$V^*$，那么我们可以根据行为的价值计算出所有行为的$\Delta_a$ 。可设置为：如果一个行为的差距越小，则尝试该行为的机会越多；如果一个行为的差距越大，则尝试该行为的几率越小。数学表达如下：</p><p>策略如下：</p><script type="math/tex; mode=display">\begin{array}{l}c>0 \\d=\min _{a \mid \Delta_{a}>0} \Delta_{i} \\\epsilon_{t}=\min \left\{1, \frac{c|\mathcal{A}|}{d^{2} t}\right\}\end{array}</script><p>小结：</p><ul><li>对数渐进regret</li><li>$\epsilon$调整策略需要提前知道gap，<strong>不符合实际</strong></li><li>对于任意多臂赌博机，都可找到次线性regret，不用R的信息</li></ul><h2 id="2-2-不确定性优先-optimism-in-the-face-of-uncertainty"><a href="#2-2-不确定性优先-optimism-in-the-face-of-uncertainty" class="headerlink" title="2.2 不确定性优先 optimism in the face of uncertainty"></a>2.2 不确定性优先 optimism in the face of uncertainty</h2><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><h4 id="总后悔值下限-lower-bound"><a href="#总后悔值下限-lower-bound" class="headerlink" title="总后悔值下限 lower bound"></a>总后悔值下限 lower bound</h4><p>当有噪声干扰，或者表现具有相似性，无法从反馈信息直接判断动作好坏</p><p>可以用$gap\Delta_a$， 和分布的相似程度KL散度 $KL(R^a||R^a*)$，来计算总后悔值的下限：</p><ul><li>差距越大，后悔值越大</li><li>奖励分布的相似程度越高，后悔值约低</li></ul><p><strong>定理</strong>：（Lai and robbins）<br>存在一个总后悔值的下限，没有哪一个算法能够做得比这个下限更好<br>渐进总regret 至少是步数的<strong>对数</strong></p><script type="math/tex; mode=display">\lim _{t \rightarrow \infty} L_{t} \geq \log t \sum_{a \mid \Delta_{a}>0} \frac{\Delta_{a}}{K L\left(\mathcal{R}^{a}|| \mathcal{R}^{a^{*}}\right)}</script><h4 id="置信上限-Upper-confidence-bound"><a href="#置信上限-Upper-confidence-bound" class="headerlink" title="置信上限 Upper confidence bound"></a>置信上限 Upper confidence bound</h4><p>为什么要分析置信上限，通过分析上限，可以将表现<strong>有潜力</strong>的动作选出<br>上限的选取，是基于概率的，选择要包容95%、99%的置信区间</p><ul><li>上限值$U_t(a)$ 以均值为基准，叠加在均值之上的</li><li><p><img src="/img/15984959743685.jpg" alt=""></p></li><li><p>对每个动作价值函数，进行估计上限$\hat U_t(a)$</p></li><li>例如：$Q(a) \leq \hat Q_t(a) + \hat U_t(a)$ 具有高概率</li><li>这取决于步的计数个数$N(a)$，采样到的个数越多，估计越准确<ul><li>小的 $N(a)$对应 大的 $\hat U_t(a)$，估计不确定</li><li>大的 $N(a)$对应 小的 $\hat U_t(a)$，估计<strong>准确</strong></li></ul></li><li>按照置信上限 upper confidence bound（UCN）选取最优动作<script type="math/tex; mode=display">a_t = \underset{a\in A}{\operatorname {argmax}}\left(\hat Q_t(a) + \hat U_t(a)\right)</script></li></ul><p>小结：随仿真进行，提高置信度</p><h4 id="霍夫丁不等式-Hoeffding’s-inequality"><a href="#霍夫丁不等式-Hoeffding’s-inequality" class="headerlink" title="霍夫丁不等式 Hoeffding’s inequality"></a>霍夫丁不等式 Hoeffding’s inequality</h4><p>提供了置信上限的计算方法，要求先对数据进行缩放，缩放到[0,1]<br><img src="/img/15983397722162.jpg" alt="-w422"></p><p>将不等式应用到奖励赌博机中</p><script type="math/tex; mode=display">\mathbb{P}\left[Q(a)>\hat{Q}_{t}(a)+U_{t}(a)\right] \leq e^{-2 N_{t}(a) U_{t}(a)^{2}}</script><h5 id="利用霍夫丁不等式计算UCB"><a href="#利用霍夫丁不等式计算UCB" class="headerlink" title="利用霍夫丁不等式计算UCB"></a>利用霍夫丁不等式计算UCB</h5><ul><li>选择允许的概率 P</li><li>计算上限值，$U_t(a)$<script type="math/tex; mode=display">\begin{aligned}e^{-2 N_{t}(a)} U_{t}(a)^{2} &=p \\U_{t}(a) &=\sqrt{\frac{-\log p}{2 N_{t}(a)}}\end{aligned}</script></li></ul><p>实际操作过程中，随着采样点增多，我们希望对Q的估计越来越确信，所以逐步减少p值<br>通过降低p，可以获取更多奖励，随着仿真进行，降低p值，e.g. $p = t^{-4}$</p><script type="math/tex; mode=display">U_{t}(a)=\sqrt{\frac{2 \log t}{N_{t}(a)}}</script><h4 id="UCB1"><a href="#UCB1" class="headerlink" title="UCB1"></a>UCB1</h4><p>利用霍夫丁不等式，let $p = t^{-4}$<br>UCB1算法如下：</p><script type="math/tex; mode=display">a_{t}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q(a)+\sqrt{\frac{2 \log t}{N_{t}(a)}}</script><p>定理：<br>UCB算法趋向于对数渐进 总regret</p><script type="math/tex; mode=display">\lim _{t \rightarrow \infty} L_{t} \leq 8 \log t \sum_{a \mid \Delta_{a}>0} \Delta_{a}</script><p>效果对比：<br><img src="/img/15983410378195.jpg" alt="-w843"></p><ul><li>小结：<ul><li>如果$\epsilon-greedy$调参合适，表现更好，参数差就是灾难</li><li>UCB不需要掌握任何信息也可以表现良好</li></ul></li></ul><p>UCB可以被应用到：</p><ul><li>伯恩斯坦 不等式</li><li>经验伯恩斯坦 不等式</li><li>切尔诺夫 不等式</li><li>azuma 不等式</li></ul><h4 id="贝叶斯-Bayesian-bandits"><a href="#贝叶斯-Bayesian-bandits" class="headerlink" title="贝叶斯 Bayesian bandits"></a>贝叶斯 Bayesian bandits</h4><p>与多臂赌博机不同，贝叶斯赌博机中，我们期望通过episode 的经验构建动作-奖励概率函数 和 Q值函数，用<strong>后验估计来指导最优动作的选取</strong></p><ul><li>特性：<ul><li>贝叶斯赌博机利用 已知的奖励经验，$p[R^a]$</li><li>构建Q值的概率分布，使用参数w，$p[Q|w]$<ul><li>e.g. Gaussians: $w = [\mu_1, \sigma^2,…]$ for each $a\in [1,k]$</li></ul></li><li>计算奖励的后验分布 $p[R|h_t]$，使用奖励的历史信息</li><li>使用后验去指导探索<ul><li>UCB</li><li>概率匹配（thompson sampling）</li></ul></li><li>先验知识准确的条件下，表现会更好</li></ul></li></ul><h4 id="贝叶斯UCB"><a href="#贝叶斯UCB" class="headerlink" title="贝叶斯UCB"></a>贝叶斯UCB</h4><ul><li><p>计算方法：</p><ul><li><p>假设奖励服从高斯分布$R_a(r) = N(r;\mu_a,\sigma^2_a)$<br><img src="/img/15983418636815.jpg" alt="-w400"></p></li><li><p>根据贝叶斯准则计算均值$\mu_a$ 和 方差 $\sigma^2_a$</p><script type="math/tex; mode=display">p\left[\mu_{a}, \sigma_{a}^{2} \mid h_{t}\right] \propto p\left[\mu_{a}, \sigma_{a}^{2}\right] \prod_{t \mid a_{t}=a} \mathcal{N}\left(r_{t} ; \mu_{a}, \sigma_{a}^{2}\right)</script></li><li><p>选择最大化标准差的$Q(a)$值</p></li></ul></li></ul><script type="math/tex; mode=display">a_{t}=\underset{a\in A}{\operatorname {argmax}} \mu_{a}+c \sigma_{a} / \sqrt{N(a)}</script><h3 id="2-1-概率匹配"><a href="#2-1-概率匹配" class="headerlink" title="2.1 概率匹配"></a>2.1 概率匹配</h3><p>选择动作：按照最有动作的概率挑选动作</p><script type="math/tex; mode=display">\pi\left(a \mid h_{t}\right)=\mathbb{P}\left[Q(a)>Q\left(a^{\prime}\right), \forall a^{\prime} \neq a \mid h_{t}\right]</script><p>按照纵坐标的大小去选</p><p><img src="/img/15983423942782.jpg" alt="-w416"></p><p>特点：</p><ul><li>面对不确定性时，概率匹配是最优的<ul><li>不确定行动，可能获取最大值</li></ul></li><li>无法得到解析的后验值</li></ul><h3 id="2-2-Thompson-Sampling"><a href="#2-2-Thompson-Sampling" class="headerlink" title="2.2 Thompson Sampling"></a>2.2 Thompson Sampling</h3><p>基于概率匹配的方法</p><script type="math/tex; mode=display">\begin{aligned}\pi\left(a \mid h_{t}\right) &=\mathbb{P}\left[Q(a)>Q\left(a^{\prime}\right), \forall a^{\prime} \neq a \mid h_{t}\right] \\&=\mathbb{E}_{\mathcal{R} \mid h_{t}}[\mathbf{1}(a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q(a))]\end{aligned}</script><p>操作步骤：</p><ol><li>使用贝叶斯法则计算后验分布$p[R|h_t]$</li><li>从后验分布中采样 R 值，根据奖励值期望，计算动作价值函数</li><li>选取最优动作，即可最大化Q(a) as $a_t = \underset{a\in A}{\operatorname{argmax}}Q(a)$</li></ol><p>汤姆森采样，同样满足Lai robbins 下限</p><p>采样在序列问题中，表现的很好，因为采样打破了序列的排列造成的干扰。</p><h2 id="2-3-信息状态空间搜索-Information-state-search"><a href="#2-3-信息状态空间搜索-Information-state-search" class="headerlink" title="2.3 信息状态空间搜索 Information state search"></a>2.3 信息状态空间搜索 Information state search</h2><h3 id="Value-information"><a href="#Value-information" class="headerlink" title="Value information"></a>Value information</h3><p>Value 可以指导 动作性选择</p><p>评价 value of information</p><ul><li>预算，获取信息的成本<ul><li>如果次数少，基于目前的选择；选择机会多，倾向于探索</li><li>长期的奖励 由于 即刻 奖励</li></ul></li><li>在不确定的情况下，信息增益高，如果什么都知道了，不需要获取信息</li><li>如果我们知道更多信息，就可以最优的平衡 利用 和 探索</li></ul><h3 id="信息状态空间-Information-state-space"><a href="#信息状态空间-Information-state-space" class="headerlink" title="信息状态空间 Information state space"></a>信息状态空间 Information state space</h3><p>信息状态空间包括了状态在内的历史信息，总结归纳出的信息，可以通过状态空间的信息预估奖励、下时刻的状态等</p><ul><li>每个时间步，都有信息状态$\tilde S = f(h_t)$，包括了历史信息的统计、摘要</li><li>每次动作，生成新信息状态，对应概率是$\tilde {\mathcal P}^a_{\tilde S, \tilde S’}$</li><li>每一步，都会计算概率分布，用先验估计的方法，预测下一步的状态和奖励</li></ul><p>定义<strong>增强MDP</strong> $\tilde {\mathcal M}$ as</p><script type="math/tex; mode=display">\tilde{\mathcal M} = \langle \tilde S, A, \tilde P, R, \gamma \rangle</script><ul><li>例子：伯努利赌博机<ul><li>对于伯努利赌博机，汇报函数满足beta分布：$R^a = B(\mu_a)$</li><li>e.g. 赢的概率是 $\mu_a$</li><li>目标是找到最高的$\mu_a$</li><li>信息状态空间是 $\tilde S = \langle \alpha, \beta \rangle$，相较于传统MDP的状态 S = 1 or 0<ul><li>$\alpha_a$计数为奖励=0</li><li>$\beta_a$计数为奖励=1</li></ul></li></ul></li></ul><p>这是一个infinite MDP，因为空间无限，但是可以用RL解决</p><p>Solution for information state space bandits：</p><ul><li>无模型强化学习 model-free RL<ul><li>Q-learning</li></ul></li><li>基于模型的贝叶斯强化学习 Bayes model-based RL<ul><li>Gittins indices（Bayes-adaptive RL）<br>找到关于先验分布的最优平衡点（利用 和 探索）</li></ul></li></ul><h4 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h4><p>beta 分布，根据计数，更新后验估计的分布函数</p><ol><li>从$beta(\alpha_a,\beta_a)$先验分布开始</li><li>每一步选取动作，根据结果更新后验估计 for $\mathcal {R^a}$<ul><li>$beta(\alpha_a+1,\beta_a)$ if r = 0;</li><li>$beta(\alpha_a,\beta_a+1)$ if r = 1;</li></ul></li><li>定义了状态转移函数$\tilde P$ for 贝叶斯自适应MDP</li><li>信息空间对应$beta(\alpha_a,\beta_a)$</li><li>每个状态转移 对应 一次贝叶斯模型更新</li></ol><p><img src="/img/15983559828012.jpg" alt="-w522"></p><h3 id="Gittins-indices-for-贝叶斯赌博机"><a href="#Gittins-indices-for-贝叶斯赌博机" class="headerlink" title="Gittins indices for 贝叶斯赌博机"></a>Gittins indices for 贝叶斯赌博机</h3><p>贝叶斯自适应 MDP 可以用 动态规划 求解，被称作 【gittins index】<br>精确求解贝叶斯自适应MDP是非常棘手的，信息状态空间太大</p><p>更高效的解决方法是：使用simulation-based search</p><pre><code>- 对信息状态空间进行前向搜索- 从当前状态开始，用simulation的方法</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>随机探索<ul><li>$\epsilon$-greedy</li><li>softmax</li><li>Gaussian noise</li></ul></li><li>基于不确定性的最优<ul><li>乐观初始化</li><li>UCB upper confidence bound</li><li>汤姆逊采样 thompson sampling</li></ul></li><li>信息状态空间<ul><li>Gittins indices</li><li>贝叶斯自适应 MDPs</li></ul></li></ul><h1 id="3、语境赌博机-Contextual-Bandits"><a href="#3、语境赌博机-Contextual-Bandits" class="headerlink" title="3、语境赌博机 Contextual Bandits"></a>3、语境赌博机 Contextual Bandits</h1><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><ul><li>相较于传统赌博机，考虑tuple$\langle A,S,R\rangle$，考虑了状态</li><li>状态分布未知，$S = \mathcal P[s]$</li><li>奖励函数分布未知，$R_s^a(r) = \mathcal P [r|s,a]$</li><li>在每一个时间步<ul><li>env 产生状态变量 $s_t \in S$</li><li>Agent 选择动作 $a_t\in A$</li><li>环境产生奖励 $r_t \sim R_{s_t}^{a_t}$</li></ul></li><li>目标：最大化累计奖励 $\sum^t_{\tau = 1}r_\tau$</li></ul><h2 id="线性-UCB"><a href="#线性-UCB" class="headerlink" title="线性 UCB"></a>线性 UCB</h2><h3 id="线性回归："><a href="#线性回归：" class="headerlink" title="线性回归："></a>线性回归：</h3><p>构建线性回归Q值 函数估计器，求解估计参数，在状态s下可以求得最优动作a<br><img src="/img/15983574142837.jpg" alt="-w546"></p><h3 id="线性-UCB-1"><a href="#线性-UCB-1" class="headerlink" title="线性 UCB"></a>线性 UCB</h3><p>最小二乘回归，使用参数$\theta$，估计动作价值函数<br>同样，可以估计Q值的方差$\sigma^2_\theta (s,a)$</p><ul><li>不确定性，来源于参数估计的误差</li></ul><p>定义UCB as， $U_\theta (s,a) = c \sigma$</p><ul><li>表示UCB背离了均值 c个标准差</li></ul><h4 id="几何解释："><a href="#几何解释：" class="headerlink" title="几何解释："></a>几何解释：</h4><p><img src="/img/15983578783024.jpg" alt="-w566"></p><h4 id="求解线性UCB"><a href="#求解线性UCB" class="headerlink" title="求解线性UCB"></a>求解线性UCB</h4><p><img src="/img/15983580085583.jpg" alt="-w559"></p><h1 id="4、MDPs"><a href="#4、MDPs" class="headerlink" title="4、MDPs"></a>4、MDPs</h1><p>前述的利用/探索的规则，虽然是基于赌博机问题开发的，但是扩展到 full MDPs过程，同样有效</p><ul><li>原始探索 e.g. $\epsilon-greedy$</li><li>乐观初始化</li><li>基于不确定性的最优</li><li>概率匹配</li><li>信息状态空间搜索</li></ul><p>Agent，不根据最大的平均Q值来选择动作，根据概率分布推测的Q值上限来选择动作，不放过潜力股<br>但是，一般的Q值只是在某个策略下的Q值，最好能考虑策略的上限，然而这是非常难的</p><h2 id="乐观初始化：-model-free-RL"><a href="#乐观初始化：-model-free-RL" class="headerlink" title="乐观初始化： model-free RL"></a>乐观初始化： model-free RL</h2><ul><li>初始化Q值，为$\frac{r_{max}}{1-\gamma}$</li><li>运行model-free RL算法<ul><li>MC control</li><li>Sarsa</li><li>Q-learning</li></ul></li><li>鼓励系统地探索<strong>状态</strong>和<strong>动作</strong></li></ul><h2 id="乐观初始化：-model-based-RL"><a href="#乐观初始化：-model-based-RL" class="headerlink" title="乐观初始化： model-based RL"></a>乐观初始化： model-based RL</h2><ul><li>构建最优模型 for MDP</li><li>初始化转移（i.e.过度到终止状态（with $r_{max}$）奖励）</li><li>用规划算法求解 最优 MDP<ul><li>policy iteration</li><li>value iteration</li><li>tree search</li><li>。。。</li></ul></li><li>鼓励系统地探索<strong>状态</strong>和<strong>动作</strong>空间</li><li>e.g. RMax 算法 （Brafman and Tennenholtz）</li></ul><h2 id="UCB：model-free-RL"><a href="#UCB：model-free-RL" class="headerlink" title="UCB：model-free RL"></a>UCB：model-free RL</h2><ul><li>最大化UCB on 价值函数(在策略$\pi$下)$Q^\pi(s,a)$<script type="math/tex; mode=display">a_{t}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q\left(s_{t}, a\right)+U\left(s_{t}, a\right)</script><ul><li>估计策略评估的不确定性，（简单）</li><li>在策略提高的过程中，忽略不确定性</li></ul></li><li>最大化UCB on 最优价值函数 $Q^*(s,a)$</li></ul><script type="math/tex; mode=display">a_{t}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q\left(s_{t}, a\right)+U_{1}\left(s_{t}, a\right)+U_{2}\left(s_{t}, a\right)</script><pre><code>- 估计策略评估的不确定性$U_1(s_t,a)$，（简单）- **加上** 策略评估的不确定性$U_2(s_t,a)$，（复杂）</code></pre><h2 id="Bayesian-model-based-RL"><a href="#Bayesian-model-based-RL" class="headerlink" title="Bayesian model-based RL"></a>Bayesian model-based RL</h2><ul><li>获取MDP 模型的后验估计</li><li>估计转移 和 奖励 概率函数分布，$p[P,R|h_t]$<ul><li>$h_t$是$s_1,a_1,…,s_t$等历史信息</li></ul></li><li>用后验估计去指引探索<ul><li>贝叶斯UCB</li><li>概率匹配</li></ul></li></ul><h2 id="汤姆逊采样：model-based-RL"><a href="#汤姆逊采样：model-based-RL" class="headerlink" title="汤姆逊采样：model-based RL"></a>汤姆逊采样：model-based RL</h2><ul><li>thompson sampling 采用概率匹配的方法<script type="math/tex; mode=display">\begin{aligned}\pi\left(s, a \mid h_{t}\right) &=\mathbb{P}\left[Q^{*}(s, a)>Q^{*}\left(s, a^{\prime}\right), \forall a^{\prime} \neq a \mid h_{t}\right] \\&=\mathbb{E}_{\mathcal{P}, \mathcal{R} \mid h_{t}}\left[\mathbf{1}\left(a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q^{*}(s, a)\right)\right]\end{aligned}</script></li><li>使用贝叶斯法则去计算后验估计$p[P,R|h_t]$</li><li>从后验估计中采样MDP</li><li>用规划方法求解最优价值函数$Q^*(s,a)$</li><li>从采样的MDP中，选取最有动作，$a_t = \underset{a\in A}{\operatorname{argmax}}Q^*(s_t,a)$</li></ul><h2 id="Bayes-adaptive-MDPs"><a href="#Bayes-adaptive-MDPs" class="headerlink" title="Bayes adaptive MDPs"></a>Bayes adaptive MDPs</h2><h3 id="信息空间搜索-in-MDPs"><a href="#信息空间搜索-in-MDPs" class="headerlink" title="信息空间搜索 in MDPs"></a>信息空间搜索 in MDPs</h3><ul><li>MDPs 可以被增强为 带有信息空间的MDPs</li><li>增强后的空间表示为$\langle S,\tilde S \rangle$<ul><li>S是MDP的原始状态</li><li>$\tilde S$是静态的历史状态</li></ul></li><li>每个动作造成一个转移<ul><li>去新状态s’的概率$\mathcal P^a_{s,s’}$</li><li>去新的信息空间$\tilde{s’}$</li></ul></li><li>定义MDP$\tilde M$ 表示为</li></ul><script type="math/tex; mode=display">\tilde M = \langle \tilde S, A \tilde P , R , \gamma \rangle</script><h3 id="自适应贝叶斯-MDPs"><a href="#自适应贝叶斯-MDPs" class="headerlink" title="自适应贝叶斯 MDPs"></a>自适应贝叶斯 MDPs</h3><ul><li>后验分布 for MDP model 是一个信息空间<script type="math/tex; mode=display">\tilde S_t = \mathcal P[\mathcal P, R|h_t]</script></li><li>增强的MDP $\langle S, \tilde S \rangle$被叫做自适应贝叶斯MDP</li><li>求解MDP，找出利用和 探索 之间的平衡（关于先验的）</li><li>然而，带有历史信息的自适应贝叶斯MDP，规模通常超级巨大</li><li>通常使用simulation-based search，即用采样的方法来学习</li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> 探索 exploration </tag>
            
            <tag> 利用 exploitation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解锁播放器的隐藏功能👀用过的都说好😎</title>
      <link href="/posts/iina-potplayer/"/>
      <url>/posts/iina-potplayer/</url>
      
        <content type="html"><![CDATA[<h1 id="动机🤔"><a href="#动机🤔" class="headerlink" title="动机🤔"></a>动机🤔</h1><ul><li><p>有时候想看视频，遭遇1-2min的广告，望而却步<br>“歪，我要看的视频也就3min好嘛？？？”<br><img src="/img/15979372311766.jpg" alt="-w832"></p></li><li><p>有时候你想看个新闻，却还要装个<code>Flash</code><br><img src="/img/15979364965955.jpg" alt="-w832"></p></li></ul><p>想起乔帮主说的话<br>“移动时代是低功耗设备、触摸屏界面和开放网络标准的时代，Flash 已经落伍。”<br>于是乎，mac和win 平台下，都有解😎</p><h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><h2 id="Mac下，果断的下载IINA就好了"><a href="#Mac下，果断的下载IINA就好了" class="headerlink" title="Mac下，果断的下载IINA就好了"></a>Mac下，果断的下载<a href="https://iina.io">IINA</a>就好了</h2><p><img src="/img/15979281180872.jpg" alt="-w1091"><br>作为一款万能的视频播放器，IINA mac 版的界面精美，功能齐全，支持Touch Bar、兼容 MPV 脚本、几乎支持所有格式、网络播放等，有中文支持</p><p>字幕搜索下载、<code>youtube-dl</code>下载视频，常规的功能无需赘述，贴两张图，你会知道他都能干什么</p><ul><li><p>不用装Flash了，电池又能多刚一会儿<br><img src="/img/15979740819192.jpg" alt="-w752"></p></li><li><p>不用再看广告<br>想起了9年前的<code>Black Mirror S1E2</code>里面男主<code>Bing</code>被科技支配，跳广告还要付费的悲惨人生</p></li></ul><p><img src="/img/15979763639018.jpg" alt=""><br>什么，你这不是<code>youku</code> <code>腾讯</code>的资源，怎么贴了个<code>youtube</code>？？？<br>是这样的，小跳昨天明明还测试成功了，今天截图的时候碰巧失效，猜测这好比<code>魔</code>之于<code>道</code>， <code>矛</code>之于<code>盾</code>的关系，互相针对，如果不可用的话，改天没准就可以了，不是还有<code>佛</code>呢么😂，或者，关注微信公众号【探物及理】回复【播放器】，获取其他Solution</p><p>安装方法：二选其一</p><pre><code>https://iina.iobrew cask install iina</code></pre><p>使用说明：</p><ul><li>IINA提供了浏览器插件，即点即用<br><img src="/img/15979750696424.jpg" alt=""></li><li>配合<code>Alfred</code> 命令 <code>iina url</code>，即可解锁<br><img src="/img/15979751018447.jpg" alt=""></li></ul><h3 id="福利赠送"><a href="#福利赠送" class="headerlink" title="福利赠送"></a>福利赠送</h3><p>如果你是一个小语种学习者，抑或对他国人文、地理……感兴趣，小跳这里附送一个网站<br><img src="/img/15979769788203.jpg" alt="-w659"></p><p>收集了来自世界各地的8000多个公共IPTV频道。<br>互联网协议电视（IPTV）是通过互联网协议（IP）网络传送电视内容的方法。<br>简言之，就是电视节目从网上看，其他再多，我们也不用知道</p><p><strong>效果如下：</strong><br><img src="/img/15979755673388.jpg" alt=""></p><h2 id="Windows下，我有Potplayer"><a href="#Windows下，我有Potplayer" class="headerlink" title="Windows下，我有Potplayer"></a>Windows下，我有Potplayer</h2><p>作为用户最多的Windows，自然跟得上，<code>Potplayer</code>可以帮助你实现以上同样功能</p><ul><li><p>通过打开链接的方式，添加视频源<br><img src="/img/15979770839021.jpg" alt="-w516"></p></li><li><p>千余家电视直播已在你手，清晰度1080P起，美哉<br><img src="/img/15979770135801.jpg" alt=""></p></li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IINA </tag>
            
            <tag> Potplayer </tag>
            
            <tag> iptv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>免费图床搭建:Github+Picgo+jsDelivr</title>
      <link href="/posts/img-bed/"/>
      <url>/posts/img-bed/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>简单说图床就是一个在网络上存储图片的地方，目的是为了节省本地服务器空间（<code>.md</code>和<code>.html</code>文件里图片是以链接的形式），加快图片打开速度，主要是个人博客和网站使用。</p><ul><li>微博图床：挂了已经</li><li>SM.MS：国外服务，慢</li><li>imgur：国外，被Q，慢</li><li>七牛云：需要注册国内域名，备案麻烦</li><li>阿里云：要花几块钱</li><li>腾讯云：比阿里贵</li></ul><p>但是，Github也有缺点，比如不用爬墙访问慢等，会导致国内访问网页的时候，图片刷新极慢，但是不要钱啊，配合jsDelivr可以白嫖成功。</p><p>另，不可能每次都开浏览器去上传图片，图床工具Picgo可以方便的帮你上传图片到图床，并且插件可以支持自定义域名，方便你用cdn工具来加速Github存放的图片访问。</p><h1 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h1><h2 id="新建Github仓库"><a href="#新建Github仓库" class="headerlink" title="新建Github仓库"></a>新建Github仓库</h2><p>登录/注册GitHub，新建一个仓库，填写好仓库名，仓库描述，根据需求选择是否为仓库初始化一个README.md描述文件。</p><p><img src="/img/15978320697562.jpg" alt=""></p><ul><li>仓库设置<ul><li>仓库名称</li><li>仓库公有/私有：因为图床需要被很多应用程序访问，所以需要设置成public的，切记</li><li>根据需要，决定是否初始化<code>readme.md</code></li></ul></li></ul><p><img src="/img/15978321283696.jpg" alt=""></p><h2 id="申请access-token给picgo"><a href="#申请access-token给picgo" class="headerlink" title="申请access token给picgo"></a>申请access token给picgo</h2><p>token是外部应用程序访问github的密钥，是picgo在github的身份证明<br>【注意】：需要注意的是token申请成功后，只会显示一次，需要自己妥善保存，否则要重新申请，最好配置完后在关闭picgo</p><ul><li>设置路径在<code>settings/Developer settings/generate new token</code></li></ul><p><img src="/img/15978321943320.jpg" alt=""></p><p><img src="/img/15978322447146.jpg" alt=""></p><ul><li>设置token<ul><li>填写token描述：方便知道哪个token干嘛的</li><li>token权限：这里只开放仓库repo的权限</li></ul></li></ul><p><img src="/img/15978322969831.jpg" alt=""></p><h2 id="配置Picgo"><a href="#配置Picgo" class="headerlink" title="配置Picgo"></a>配置Picgo</h2><ul><li>安装<ul><li>下载地址在这里<a href="https://github.com/Molunerfinn/PicGo">Picgo</a></li><li>也可以<code>brew cask install picgo</code> 大法</li></ul></li></ul><p>上图，我的设置如下：</p><ul><li>用户名<code>tolshao</code></li><li>仓库名<code>media</code></li><li>设定分支名：<code>master</code></li><li>设定Token：粘贴之前生成的<code>Token</code></li><li>存储路径<code>img/</code></li><li>自定义域名是仓库的别名，这里可以用<code>jsDelivr</code>来进行<code>cdn</code>加速，实现<code>https://raw.githubusercontent.com/tolshao/media/master/img/contact.jpg</code>到<code>https://cdn.jsdelivr.net/gh/tolshao/media/img/contact.jpg</code>的替换，实测快很多</li></ul><p><img src="/img/15978324636801.jpg" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/tolshao/media/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图床 </tag>
            
            <tag> Picgo </tag>
            
            <tag> jsDelivr </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记8：整合学习和规划</title>
      <link href="/posts/rl-8/"/>
      <url>/posts/rl-8/</url>
      
        <content type="html"><![CDATA[<h1 id="1、introduction"><a href="#1、introduction" class="headerlink" title="1、introduction"></a>1、introduction</h1><p>第7章节，讲了PG，从episode经验学习到 策略 policy<br>之前的章节，讲了从episode 经验学习到 价值函数</p><p>本章，从过去经验学习到环境模型</p><p>通过<strong>规划</strong>的手段，构建值函数或者策略</p><ul><li>Model-free<ul><li>没有模型</li><li>从经验中学习，得到<strong>价值函数</strong> </li></ul></li><li>Model-based<ul><li>有模型</li><li>根据模型规划价值函数</li></ul></li></ul><p>本讲指出解决这类问题的关键在于“前向搜索”和“采样”，通过将基于模拟的前向搜索与各种不依赖模型的强化学习算法结合，衍生出多个用来解决类似大规模问题的切实可行的算法，如：Dyna-2算法之类。</p><h1 id="2、Model-based-reinforcement-learning"><a href="#2、Model-based-reinforcement-learning" class="headerlink" title="2、Model-based reinforcement learning"></a>2、Model-based reinforcement learning</h1><p>当学习价值函数或策略变得很困难的时候，学习模型可能是一条不错的途径，像下棋这类活动，模型是相对直接的，相当于就是游戏规则。<br><img src="/img/15978228360645.png" alt=""></p><ul><li>优点<ul><li>用监督学习高效实现建模</li><li>推断模型不确定性</li></ul></li><li>缺点<ul><li>第一步建模，第二步构建价值函数，两个过程带来双重的<strong>估计误差</strong></li></ul></li></ul><h2 id="模型，我们要做什么"><a href="#模型，我们要做什么" class="headerlink" title="模型，我们要做什么"></a>模型，我们要做什么</h2><ul><li>MDP模型包括$\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}\rangle$</li><li>假设状态空间$\mathcal{S}$、动作空间$\mathcal{A}$已知<br>我们期望构建一个模型 $\mathcal{M} = \langle \mathcal{P}_{\eta}, \mathcal{R}_{\eta} \rangle$ 来表示状态转移概率P和奖励R<script type="math/tex; mode=display">\begin{array}{l}S_{t+1} \sim \mathcal{P}_{\eta}\left(S_{t+1} \mid S_{t}, A_{t}\right) \\R_{t+1}=\mathcal{R}_{\eta}\left(R_{t+1} \mid S_{t}, A_{t}\right)\end{array}</script></li></ul><p>假设状态转移和 奖励 无关</p><script type="math/tex; mode=display">\mathbb{P}\left[S_{t+1}, R_{t+1} \mid S_{t}, A_{t}\right]=\mathbb{P}\left[S_{t+1} \mid S_{t}, A_{t}\right] \mathbb{P}\left[R_{t+1} \mid S_{t}, A_{t}\right]</script><h2 id="Model-learning"><a href="#Model-learning" class="headerlink" title="Model learning"></a>Model learning</h2><ul><li>Goal：构建一个模型model $\mathcal{M} = \{S_1, A_1, R_2 …m S_T\}$ </li><li>构建<strong>监督学习</strong>问题，根据状态和动作，估计奖励和下一时刻的状态<script type="math/tex; mode=display">\begin{aligned}S_{1}, A_{1} & \rightarrow R_{2}, S_{2} \\S_{2}, A_{2} & \rightarrow R_{3}, S_{3} \\& \vdots \\S_{T-1}, A_{T-1} & \rightarrow R_{T}, S_{T}\end{aligned}</script></li></ul><ul><li>奖励r的估计是<strong>回归</strong>问题</li><li>状态s’的估计是<strong>密度估计</strong>问题</li><li>选择损失函数，例如 MSE， KL divergency（相对熵）</li><li>优化参数$\eta$最小化loss func</li></ul><h3 id="模型实例"><a href="#模型实例" class="headerlink" title="模型实例"></a>模型实例</h3><ul><li>Table lookup model</li><li>Linear expectation model</li><li>Linear Gaussian model</li><li>Gaussian Process model</li><li>Deep Belief Network model</li></ul><h4 id="例子-Table-lookup-example"><a href="#例子-Table-lookup-example" class="headerlink" title="例子 Table lookup example"></a>例子 Table lookup example</h4><p>模型是显示MDP，表示为$\hat P, \hat R$<br>采样、奇数，即可得到概率的估计值</p><script type="math/tex; mode=display">\begin{aligned}\hat{\mathcal{P}}_{s, s^{\prime}}^{a} &=\frac{1}{N(s, a)} \sum_{t=1}^{T} 1\left(S_{t}, A_{t}, S_{t+1}=s, a, s^{\prime}\right) \\\hat{\mathcal{R}}_{s}^{a} &=\frac{1}{N(s, a)} \sum_{t=1}^{T} 1\left(S_{t}, A_{t}=s, a\right) R_{t}\end{aligned}</script><p>特点：</p><ul><li>每一步都需要记录，格式：$\langle S_t, A_t, R_{t+1}, S_{t+1} \rangle$ </li><li>随机采样$\langle s,a,\cdot,\cdot \rangle$，进行模型估计</li></ul><p><img src="/img/15979160045155.jpg" alt="-w526"></p><h2 id="Planning-with-a-model"><a href="#Planning-with-a-model" class="headerlink" title="Planning with a model"></a>Planning with a model</h2><ul><li>根据已知模型$\mathcal M_\eta = \langle \mathcal{P_\eta},\mathcal{R_\eta}\rangle$</li><li>Solbe the MDP $\langle S, A, \mathcal{P_\eta},\mathcal{R_\eta}\rangle$ </li><li>使用规划方法<ul><li>值迭代</li><li>策略迭代</li><li>树搜索</li><li>其他</li></ul></li></ul><h3 id="Sample-based-planning"><a href="#Sample-based-planning" class="headerlink" title="Sample-based planning"></a>Sample-based planning</h3><p>要素2个点：</p><ul><li>1、只使用模型去产生采样点</li><li>2、用model-free RL 去学习 samples<ul><li>MC control</li><li>Sarsa</li><li>Q-learning</li></ul></li></ul><p>特点</p><ul><li>model产生的sample 比 从env 中sample更高效</li></ul><h3 id="Planning-with-an-inaccurate-Model"><a href="#Planning-with-an-inaccurate-Model" class="headerlink" title="Planning with an inaccurate Model"></a>Planning with an inaccurate Model</h3><p>显然，模型的准确性，直接制约了Model-based RL 性能的上限</p><p>Solution：</p><ul><li>模型错误的情况，直接model-free RL，并且从env中采样</li><li>明确模型不确定性的原因</li></ul><h1 id="3、integrated-architectures"><a href="#3、integrated-architectures" class="headerlink" title="3、integrated architectures"></a>3、integrated architectures</h1><p>本节将把基于模型的学习和不基于模型的学习结合起来，形成一个整合的架构，利用两者的优点来解决复杂问题。从两种渠道进行采样</p><ul><li>Env 真实MDP</li><li>Model 估计MDP</li></ul><p>实际经历： $S^{\prime} \sim P_{s, s^{\prime}}^{a}, \quad R=R_{s}^{a}$</p><p>模拟经历：$S^{\prime} \sim P_{\eta}\left(S^{\prime} \mid S, A\right), \quad R=R_{\eta}(R \mid S, A)$</p><h2 id="集成学习和规划-Dyna"><a href="#集成学习和规划-Dyna" class="headerlink" title="- 集成学习和规划 Dyna"></a>- 集成<strong>学习</strong>和<strong>规划</strong> <strong>Dyna</strong></h2><h3 id="Dyna"><a href="#Dyna" class="headerlink" title="Dyna"></a>Dyna</h3><p>Dyna是并列、综合与model-free、model-based的学习方法</p><ul><li>Model-free RL<ul><li>无模型</li><li>从真实环境Env采样，<strong>学习</strong>价值函数</li></ul></li><li>Model-based RL<ul><li>从真实环境Env中学习，建模Model</li><li>从Model虚拟采样，<strong>规划</strong>价值函数</li></ul></li><li>Dyna <ul><li>从真实环境Env中学习，建模Model</li><li>根据Env 和 Model采样，同时<strong>学习</strong> 并 <strong>规划</strong> 价值函数</li></ul></li></ul><p><img src="/img/15979964474346.jpg" alt="-w381"></p><h3 id="Dyna-Q-算法框图"><a href="#Dyna-Q-算法框图" class="headerlink" title="Dyna-Q 算法框图"></a>Dyna-Q 算法框图</h3><p><img src="/img/15979966869160.jpg" alt="-w498"><br>a,b,c,d,和e都是从实际经历中学习，d过程是学习价值函数，e过程是学习模型。<br>在f步，给以个体一定时间（或次数）的思考。在思考环节，个体将使用模型，在之前观测过的状态空间中随机采样一个状态，同时从这个状态下曾经使用过的行为中随机选择一个行为，将两者带入模型得到新的状态和奖励，依据这个来再次更新行为价值和函数。</p><p>类似监督学习里，用<strong>数据增强</strong>，来丰富数据集。</p><ul><li>例子<br>将规划引入RL之后，规划比学习具有更小的抖动和噪声，稳定性好<br><img src="/img/15979971359056.jpg" alt="-w445"></li></ul><h4 id="Dyna-Q-with-不准确模型"><a href="#Dyna-Q-with-不准确模型" class="headerlink" title="Dyna-Q with 不准确模型"></a>Dyna-Q with 不准确模型</h4><p>由于Dyna综合了实际和模拟两种情况，在实际环境改变时，错误的模型会被逐渐更正。</p><ul><li>环境剧烈改变<br><img src="/img/15979972098731.jpg" alt="-w341"><br>我们可以发现，不同算法对环境改变的适应性，相差悬殊</li><li>环境柔和改变<br><img src="/img/15979972677252.jpg" alt="-w358"></li></ul><p>Q+ 算法，奖励函数里 鼓励 episode 探索新的状态</p><h1 id="4、simulation-based-search"><a href="#4、simulation-based-search" class="headerlink" title="4、simulation-based search"></a>4、simulation-based search</h1><p>搜索相对于规划，区别之一就是，不搜索整个空间，用<strong>采样</strong>的方法来优化</p><h2 id="Forward-search"><a href="#Forward-search" class="headerlink" title="Forward search"></a>Forward search</h2><p>两个重要特征：</p><ul><li>不求解整个MDP，以<strong>当前</strong>状态为起始状态</li><li>使用基于<strong>采样</strong>的规划</li></ul><p>两点都可以显著降低计算量</p><p><img src="/img/15979976765702.jpg" alt="-w388"></p><p>步骤：</p><ul><li><strong>Simulate</strong> episode 从<strong>当前时刻</strong>开始<script type="math/tex; mode=display">\left\{s_{t}^{k}, A_{t}^{k}, R_{t+1}^{k}, \ldots, S_{T}^{k}\right\}_{k=1}^{K} \sim \mathcal{M}_{\nu}</script></li><li>应用 <strong>model-free RL</strong> to 仿真并学习 episodes<ul><li>MC control $\rightarrow$ MC search</li><li>Sarsa $\rightarrow$ TD search</li></ul></li></ul><p>总结：仿真+学习 = 搜索</p><h2 id="MC-search"><a href="#MC-search" class="headerlink" title="MC search"></a>MC search</h2><h3 id="Simple-MC-search"><a href="#Simple-MC-search" class="headerlink" title="Simple MC search"></a>Simple MC search</h3><p>选择动作a，根据的Q值，<strong>只包含</strong>当前的状态$s_t$和动作集A</p><ol><li>给定模型 $\mathcal{M_v}$ 和simulation policy $\pi$</li><li>For <strong>each</strong> action $a\in A$<ul><li>simulate k episodes 从当前状态(real)$s_t$<script type="math/tex; mode=display">\left\{s_{t}, a, R_{t+1}^{k}, S_{t+1}^{k}, A_{t+1}^{k}, \ldots, S_{T}^{k}\right\}_{k=1}^{K} \sim \mathcal{M}_{\nu}, \pi</script></li><li>根据平均值评价动作(MC evaluation)<script type="math/tex; mode=display">Q\left(s_{t}, a\right)=\frac{1}{K} \sum_{k=1}^{K} G_{t} \stackrel{P}{\rightarrow} q_{\pi}\left(s_{t}, a\right)</script></li></ul></li><li>选择最优动作<script type="math/tex; mode=display">a_{t}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q\left(s_{t}, a\right)</script></li></ol><p>小结：值函数，只根据当前状态s 和 策略 $\pi$产生，因此，可能不是最优的。</p><h3 id="MC-tree-search-evaluation"><a href="#MC-tree-search-evaluation" class="headerlink" title="MC tree search (evaluation)"></a>MC tree search (evaluation)</h3><p>对于真正的MC tree search，Q值的构建，包含了所有状态s和动作a<br>估计步骤：</p><ul><li>给定模型 $\mathcal{M_v}$ 和simulation policy $\pi$</li><li>simulate k episodes 从当前状态(real)$s_t$<script type="math/tex; mode=display">\left\{s_{t}, A_{t}^{k}, R_{t+1}^{k}, S_{t+1}^{k}, \ldots, S_{T}^{k}\right\}_{k=1}^{K} \sim \mathcal{M}_{\nu}, \pi</script></li><li>构建搜索树，包含访问过的状态和动作</li><li>评价状态 $Q(s,a)$，用平均值方法<script type="math/tex; mode=display">Q(s, a)=\frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{u=t}^{T} \mathbf{1}\left(S_{u}, A_{u}=s, a\right) G_{u} \stackrel{P}{\rightarrow} q_{\pi}(s, a)</script></li><li>搜索结束后，根据最大值函数，选择最优动作<script type="math/tex; mode=display">a_{t}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q\left(s_{t}, a\right)</script></li></ul><p>小结：完整版的MCTS，比简单版，记录了(s,a)对，不是($s_t,a$)对，顺便将状态空间和动作空间的值函数进行了更新，丰富了信息。</p><h3 id="MC-tree-search-Simulation"><a href="#MC-tree-search-Simulation" class="headerlink" title="MC tree search (Simulation)"></a>MC tree search (Simulation)</h3><p>【区别】：搜索树并不包括整个状态行为对空间的Q值，因此在改进策略时要分情况对待</p><ol><li>树内确定性策略（Tree Policy）：对于在搜索树中存在的状态行为对，策略的更新倾向于最大化Q值，这部分策略随着模拟的进行是可以得到持续改进的；</li><li>树外默认策略（Default Policy）：对于搜索树中不包括的状态，可以使用固定的随机策略。</li></ol><p>也就是说，每一次从当前状态到终止状态的模拟都包括两个阶段：状态在搜索树内和状态在搜索树外。两个状态对应的策略分别是树内的确定性（最大化Q）策略和树外的默认（随机）策略。</p><p>随着不断地重复模拟，状态行为对的价值将得到持续地得到评估。同时基于$\epsilon-greedy(Q)$的搜索策略使得搜索树将不断的扩展，策略也将不断得到改善。</p><p>上述方法就相当在某一个状态$s_t$时，针对模拟的经历使用蒙特卡罗控制来寻找以当前状态$s_t$为根状态的最优策略。之前的讲解已经解释过：这种方法最终将找到最优策略。我们来看一个具体的例子：</p><p>仿真步骤：</p><ul><li>MCTS中，策略$\pi$ <strong>提升</strong></li><li>每个仿真包含两个阶段（in-tree, out-of-tree）<ul><li>Tree policy(improves): 选择最优Q值的动作</li><li>Default policy(fixed): 选择随机动作</li></ul></li><li>重复(each simulation)<ul><li>用MC evaluation <strong>评估</strong> 状态Q(S,A)</li><li>用$\epsilon - greedy(Q)$ <strong>提升</strong>策略$\pi$</li></ul></li><li>收敛到最优搜索树，$Q(S,A)\rightarrow q_*(S,A)$</li></ul><h4 id="例子：围棋Go"><a href="#例子：围棋Go" class="headerlink" title="例子：围棋Go"></a>例子：围棋Go</h4><h5 id="Position-evaluation-in-go"><a href="#Position-evaluation-in-go" class="headerlink" title="Position evaluation in go"></a>Position evaluation in go</h5><p><img src="/img/15980590659894.jpg" alt="-w388"></p><h5 id="MC-evaluation-in-go"><a href="#MC-evaluation-in-go" class="headerlink" title="MC evaluation in go"></a>MC evaluation in go</h5><p><img src="/img/15980591268724.jpg" alt="-w321"></p><p>MCTS 步骤：</p><ul><li>第一次迭代：如下图所示，五角形表示的状态是个体第一次访问的状态，也是第一次被录入搜索树的状态。我们构建搜索树：将当前状态录入搜索树中。使用基于蒙特卡罗树搜索的策略（两个阶段），由于当前搜索树中只有当前状态，全程使用的应该是一个搜索第二阶段的默认随机策略，基于该策略产生一个直到终止状态的完整Episode。图中那些菱形表示中间状态和方格表示的终止状态，在此次迭代过程中并不录入搜索树。终止状态方框内的数字1表示（黑方）在博弈中取得了胜利。此时我们就可以更新搜索树种五角形的状态价值，以分数1/1表示从当前五角形状态开始模拟了一个Episode，其中获胜了1个Episode。这是第一次迭代过程。<br><img src="/img/15981779725773.png" alt=""></li><li>第二次迭代：如下图所示，当前状态仍然是树内的圆形图标指示的状态，从该状态开始决定下一步动作。根据目前已经访问过的状态构建搜索树，依据模拟策略产生一个行为模拟进入白色五角形表示的状态，并将该状态录入搜索树，随后继续该次模拟的对弈直到Episode结束，结果显示黑方失败，因此我们可以更新新加入搜索树的五角形节点的价值为0/1，而搜索树种的圆形节点代表的当前状态其价值估计为1/2，表示进行了2次模拟对弈，赢得了1次，输了1次。第二次迭代结束。<br><img src="/img/15981779845040.png" alt=""><br>经过前两次的迭代，当位于当前状态（黑色圆形节点）时，当前策略会认为选择某行为进入上图中白色五角形节点状态对黑方不利，策略将得到更新：当前状态时会个体会尝试选择其它行为。</li><li>第三次迭代：如下图，假设选择了一个行为进入白色五角形节点状态，将该节点录入搜索树，模拟一次完整的Episode，结果显示黑方获胜，此时更新新录入节点的状态价值为1/1，同时更新其上级节点的状态价值，这里需要更新当前状态的节点价值为2/3，表明在当前状态下已经模拟了3次对弈，黑方获胜2次。<br><img src="/img/15981779894073.png" alt=""><br>随着迭代次数的增加，在搜索树里录入的节点开始增多，树内每一个节点代表的状态其价值数据也越来越丰富。在搜索树内依据Ɛ-greedy策略会使得当个体出于当前状态（圆形节点）时更容易做出到达图中五角形节点代表的状态的行为。</li><li>第四次：如下图，当个体位于当前（圆形节点）状态时，树内策略使其更容易进入左侧的蓝色圆形节点代表的状态，此时录入一个新的节点（五角形节点），模拟完Episode提示黑方失败，更新该节点以及其父节点的状态价值。该次迭代结束。<br><img src="/img/15981780807274.png" alt=""></li><li>第五次迭代：如下图，更新后的策略使得个体在当前状态时仍然有较大几率进入其左侧圆形节点表示的状态，在该节点，个体避免了进入刚才失败的那次节点，录入了一个新节点，基于模拟策略完成一个完整Episode，黑方获得了胜利，同样的更新搜索树内相关节点代表的状态价值。<br><img src="/img/15981780915387.png" alt=""></li></ul><p>随着迭代次数增加：</p><ul><li>当个体处于当前状态时，其<strong>搜索树将越来越深</strong>，那些能够引导个体获胜的搜索树内的节点将会被充分的探索，其节点代表的状态价值也越来越有说服力；</li><li>搜索树内的节点越来越多，代表着搜索树外的节点将逐渐减少，少量的随机行为并不会影响个体整体的决策效果。</li></ul><h5 id="MCTS-优点"><a href="#MCTS-优点" class="headerlink" title="MCTS 优点"></a>MCTS 优点</h5><ul><li><strong>选择性好</strong>，好的结果被<strong>优先选择</strong></li><li><strong>动态</strong>的评估状态（立足当前状态），不是动态规划（没有离线的评估整个状态空间）</li><li>使用<strong>采样</strong>方法，不在全部状态空间搜索，避免了维度灾难</li><li>采样所以具有<strong>黑盒</strong>特性，对黑盒模型同样有效</li><li>可以异步、并行计算，<strong>高效</strong></li></ul><h2 id="TD-search"><a href="#TD-search" class="headerlink" title="TD search"></a>TD search</h2><ul><li>simulation-based search</li><li>自举特性Bootstrapping， TD而不是MC</li><li>MCTS 将 MC control 应用于 子MDP from now</li><li>TD search 将 Sarsa 应用于 子MDP from now</li></ul><h3 id="MC-vs-TD-search"><a href="#MC-vs-TD-search" class="headerlink" title="MC vs TD search"></a>MC vs TD search</h3><ul><li>对于 model-free RL， Bootstrapping 特性 行之有效<ul><li>TD 减小方差，但是增加偏差</li><li>TD 比MC 更高效</li><li>$TD(\lambda)$比MC更高效</li></ul></li><li>对于 simulation-based search， Bootstrapping 特性 同样有用<ul><li>特性同上</li></ul></li></ul><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol><li>根据当前真实状态$s_t$， 开始仿真episodes</li><li>估计动作价值函数$Q(s,a)$</li><li>每个仿真步骤，用Sarsa更新动作价值函数<script type="math/tex; mode=display">\Delta Q(S, A)=\alpha\left(R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right)</script></li><li>根据Q值选择最优动作<br>e.g. $\epsilon - greedy$</li></ol><p>在上述过程，优化函数估计器 for Q</p><p>相比于MC搜索，TD搜索不必模拟Episode到终止状态，其仅聚焦于某一个节点的状态，这对于一些有回路或众多旁路的节点来说更加有意义，这是因为在使用下一个节点估计当前节点的价值时，下一个节点的价值信息可能已经是经过充分模拟探索了的，在此基础上更新的当前节点价值会更加准确。</p><h3 id="Dyna-2"><a href="#Dyna-2" class="headerlink" title="Dyna-2"></a>Dyna-2</h3><p>与Dyna-Q不同，dyna-Q只有一套参数，虽然经验有2个source（env和model）</p><p>Dyna-2 特征权重来源于两部分</p><ul><li>Long-term memory<ul><li>长期记忆 从真实经验中获取， 方法使用TD learning<br>通用的知识，可以应用于任何episode</li></ul></li><li><p>Short-term（working） memory</p><ul><li>短期记忆 从仿真经验中获取， 方法用TD search<br>特定的局部知识，对当前状态有效</li></ul></li><li><p>总的价值函数 = 两部分之和</p></li></ul><p>显而易见，将仿真产生数据应用于搜索，并将搜索结合到学习中，<strong>增强了数据</strong>，会产生显著的提升<br><img src="/img/15980627120125.jpg" alt="-w444"></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> 学习 Learning </tag>
            
            <tag> 规划 Planning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo 进阶设置指南（持续更新）</title>
      <link href="/posts/hexo-advanced/"/>
      <url>/posts/hexo-advanced/</url>
      
        <content type="html"><![CDATA[<h1 id="让hexo渲染MathJax复杂公式-默认的渲染引擎复杂公式会报错"><a href="#让hexo渲染MathJax复杂公式-默认的渲染引擎复杂公式会报错" class="headerlink" title="让hexo渲染MathJax复杂公式(默认的渲染引擎复杂公式会报错)"></a>让hexo渲染MathJax复杂公式(默认的渲染引擎复杂公式会报错)</h1><h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>对复杂公式的支持不够好，简单公式可以显示，复杂编译错误，验证表明，问题不是mathjax.js导致，是默认hexo引擎编译导致html文本转义错误。<br><img src="/img/15972822338745.jpg" alt="-w192"><br><img src="/img/15972822421519.jpg" alt="-w894"></p><h3 id="Reason"><a href="#Reason" class="headerlink" title="Reason"></a>Reason</h3><p>Hexo默认使用”hexo-renderer-marked”引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线’_’代表斜体，会被渲染引擎处理为<code>&lt;em&gt;</code>标签</p><p>因为类Latex格式书写的数学公式下划线 ‘_’ 表示下标，有特殊的含义，如果被强制转换为<code>&lt;em&gt;</code>标签，那么MathJax引擎在渲染数学公式的时候就会出错。例如，<code>x_i</code>在开始被渲染的时候，处理为<code>x&lt;em&gt;i&lt;/em&gt;</code>，这样MathJax引擎就认为该公式有语法错误，因为不会渲染。</p><p>类似的语义冲突的符号还包括’*’, ‘{‘, ‘}’, ‘\’等。</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><ul><li>更换默认的Hexo下 Markdown渲染引擎<br>marked -&gt; kramed</li></ul><pre><code class="lang-dash">npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save</code></pre><ul><li><p>更改hexo转义冲突<br>找到根目录<code>node_modules\kramed\lib\rules\inline.js</code></p><ul><li><p>修改11行，取消对<code>\,{,}</code>的转义escape</p><pre><code class="lang-dash">//  escape: /^\\([\\`*&amp;#123;&amp;#125;\[\]()#$+\-.!_&gt;])/,escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</code></pre></li><li><p>修改20行em</p><pre><code class="lang-dash">//  em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</code></pre></li></ul></li><li><p>三连 <code>hexo cl &amp;&amp; hexo g &amp;&amp; hexo s</code>查看效果</p><h2 id="Debug"><a href="#Debug" class="headerlink" title="Debug"></a>Debug</h2></li><li><p>如果出问题，在主题<code>_config.yml</code>下设置<code>mathjax</code>为<code>true</code></p></li><li>文章也要开启<code>mathjax</code></li></ul><pre><code class="lang-yml">---title: 我是标题date: 2020-08-15 23:18:50tags:mathjax: true--</code></pre><h2 id="其他解决办法"><a href="#其他解决办法" class="headerlink" title="其他解决办法"></a>其他解决办法</h2><ul><li>服务器端的渲染<ul><li><a href="https://math.now.sh/home">math.now.sh</a></li><li><a href="https://github.com/MakerGYT/markdown-it-latex2img">markdown-it-latex2img</a></li></ul></li></ul><h1 id="SEO-搜索引擎优化"><a href="#SEO-搜索引擎优化" class="headerlink" title="SEO 搜索引擎优化"></a>SEO 搜索引擎优化</h1><p><a href="https://zhuanlan.zhihu.com/p/150718629">Google search console 大全</a></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记7：策略梯度 Policy Gradient</title>
      <link href="/posts/rl-7/"/>
      <url>/posts/rl-7/</url>
      
        <content type="html"><![CDATA[<p>之前的策略优化，用的基本都是$\epsilon$-greedy的policy improve方法，这里介绍policy gradient法，不基于v、q函数</p><h1 id="1-introduction"><a href="#1-introduction" class="headerlink" title="1. introduction"></a>1. introduction</h1><p>策略梯度是以$P(a|s)$入手，概率$\pi(s,a)$的形式，同样是model free的<br><img src="/img/15973090963916.jpg" alt=""></p><script type="math/tex; mode=display">\pi_{\theta}(s, a)=\mathbb{P}[a \mid s, \theta]</script><p>调整策略的概率分布，寻找最优策略$\pi_*$</p><p><img src="/img/15971227558962.jpg" alt="-w497"></p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul><li>优点：<ul><li>更好收敛性</li><li>高维、连续动作空间高效</li><li>从随机策略中学习</li></ul></li><li>缺点：<ul><li>会限于<strong>局部最优</strong>，而不是全局最优</li><li>评价策略的过程：低效、高方差</li></ul></li><li>随机策略有时是最优策略，基于价值函数的策略有时会限于局部最优</li></ul><h2 id="Policy-Objective-function-策略目标函数"><a href="#Policy-Objective-function-策略目标函数" class="headerlink" title="Policy Objective function 策略目标函数"></a>Policy Objective function 策略目标函数</h2><p>对于不同的任务，需要建立针对性的3种目标函数</p><ul><li>1.Start Value：任务有始有终<script type="math/tex; mode=display">J_{1}(\theta)=V^{\pi_{\theta}}\left(s_{1}\right)=\mathbb{E}_{\pi_{\theta}}\left[v_{1}\right]</script></li><li>2.Average Value：连续任务，不停止。对所有状态求平均，d是状态s在策略$\pi\theta$下的分布函数。根据V值求P<script type="math/tex; mode=display">J_{a v V}(\theta)=\sum_{s} d^{\pi_{\theta}}(s) V^{\pi_{\theta}}(s)</script></li><li><p>3.Average reward per time-step：连续任务，不停止。d是状态s在策略$\pi\theta$下的分布函数。根据R值求P</p><script type="math/tex; mode=display">J_{a v R}(\theta)=\sum_{s} d^{\pi_{\theta}}(s) \sum_{a} \pi_{\theta}(s, a) \mathcal{R}_{s}^{a}</script></li><li><p>Objective：优化目标函数<br>find $\theta$ 最大化 $J(\theta)$</p></li></ul><h1 id="2-Finite-Difference-PG-有限差分策略梯度"><a href="#2-Finite-Difference-PG-有限差分策略梯度" class="headerlink" title="2. Finite Difference PG 有限差分策略梯度"></a>2. Finite Difference PG 有限差分策略梯度</h1><p>对每个维度的权重，分别进行查分求梯度，然后迭代权重，至最优<br><img src="/img/15971425601126.jpg" alt="-w449"></p><p>特点：</p><ul><li>n次运算，求得n维的梯度</li><li>简单、噪声、偶尔高效</li><li>通用性好，任意策略可用，即使策略目标函数不可微</li></ul><h1 id="3-MC-PG-蒙特卡洛策略梯度"><a href="#3-MC-PG-蒙特卡洛策略梯度" class="headerlink" title="3. MC PG 蒙特卡洛策略梯度"></a>3. MC PG 蒙特卡洛策略梯度</h1><p>要求：策略目标函数可微分，梯度可计算<br>引入了似然比概念</p><h2 id="Likelihood-ratios"><a href="#Likelihood-ratios" class="headerlink" title="Likelihood ratios"></a>Likelihood ratios</h2><h3 id="Score-function（not-value-function）"><a href="#Score-function（not-value-function）" class="headerlink" title="Score function（not value function）"></a>Score function（not value function）</h3><ul><li><p>Trick here： 用似然比 Likelihood ratios<br>将$\pi$梯度，变为$\pi$ 乘以 log 的梯度<br>函数在某个变量θ处的<strong>梯度</strong>等于该处<strong>函数值</strong>与该函数的<strong>对数函数</strong>在此处<strong>梯度</strong>的乘积 $d log(y) = dy/y$，默认底数为$e$</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\theta} \pi_{\theta}(s, a) &=\pi_{\theta}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(s, a)}{\pi_{\theta}(s, a)} \\&=\pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a)\end{aligned}</script></li><li><p>Score function: $\nabla_{\theta} \log \pi_{\theta}(s, a)$</p></li></ul><h3 id="Softmax-policy：策略概率按照指数分配"><a href="#Softmax-policy：策略概率按照指数分配" class="headerlink" title="Softmax policy：策略概率按照指数分配"></a>Softmax policy：策略概率按照指数分配</h3><p>线性组合 -&gt; softmax = $\pi$ 是状态s下，采取每个a的概率<br><img src="/img/15971450080893.jpg" alt="-w451"></p><p>通过取对数，拆分为加法，进而表示为</p><h3 id="Gaussian-polisy：策略概率按照距离分配"><a href="#Gaussian-polisy：策略概率按照距离分配" class="headerlink" title="Gaussian polisy：策略概率按照距离分配"></a>Gaussian polisy：策略概率按照距离分配</h3><ul><li>In continuous action spaces, a Gaussian policy is natural </li><li>Mean is a linear combination of state features $μ(s) = φ(s)^{T}θ$</li><li>Variance may be fixed σ , or can also parametrised Policy is Gaussian, a ∼ N (μ(s), σ2)</li><li>The score function is<script type="math/tex; mode=display">\nabla_{\theta} \log \pi_{\theta}(s, a)=\frac{(a-\mu(s)) \phi(s)}{\sigma^{2}}</script></li></ul><p>在实际应用中，要注意梯度消失现象，可以利用代数方法求解</p><h2 id="Policy-Gradient-theorem-策略梯度定理"><a href="#Policy-Gradient-theorem-策略梯度定理" class="headerlink" title="Policy Gradient theorem 策略梯度定理"></a>Policy Gradient theorem 策略梯度定理</h2><h3 id="One-step-MDPs"><a href="#One-step-MDPs" class="headerlink" title="One-step MDPs"></a>One-step MDPs</h3><p>没有序列，整个任务过程只有一步</p><ul><li>状态s~d(s)</li><li>r = R_{s,a}</li></ul><p>用似然比，计算策略梯度：<br>先给出奖励函数，三种形式一样如下：</p><script type="math/tex; mode=display">\begin{aligned}J(\theta) &=\mathbb{E}_{\pi_{\theta}}[r] \\&=\sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(s, a) \mathcal{R}_{s, a} \\\end{aligned}</script><p>相应梯度表示为：</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\theta} J(\theta) &=\sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a) \mathcal{R}_{s, a} \\&=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) r\right]\end{aligned}</script><h3 id="对于多步的标准MDPs"><a href="#对于多步的标准MDPs" class="headerlink" title="对于多步的标准MDPs"></a>对于多步的标准MDPs</h3><p>用Q函数，替代R函数，即可表征序列到End的奖励<br>对于下列都适用：</p><ul><li>初始价值函数：$J = J_1$</li><li>平均奖励函数： $J_{avR}$</li><li>平均价值函数： $\frac{1}{1-\gamma}J_{avV}$</li></ul><p>对任意可微策略，梯度都为</p><script type="math/tex; mode=display">\nabla_{\theta} J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) Q^{\pi_{\theta}}(s, a)\right]</script><h3 id="MCPG-蒙特卡洛策略梯度法"><a href="#MCPG-蒙特卡洛策略梯度法" class="headerlink" title="MCPG 蒙特卡洛策略梯度法"></a>MCPG 蒙特卡洛策略梯度法</h3><ul><li>采样 替代 期望<br>上节式子表示为Score function 和 Q 值在策略$\pi$下的<strong>期望</strong><br>在MC中，在每个episode在$\pi$下，产生的序列就满足分布，整个过程都迭代完后的值，自然就是期望，因此梯度前没有概率分布函数，用常值替代。</li></ul><p><img src="/img/15971998167024.jpg" alt="-w433"></p><ul><li>特点：<ul><li>动作平滑</li><li>收敛性好，但是慢</li><li>方差大</li></ul></li></ul><h1 id="4-Actor-Critic-PG-AC策略梯度"><a href="#4-Actor-Critic-PG-AC策略梯度" class="headerlink" title="4. Actor-Critic PG AC策略梯度"></a>4. Actor-Critic PG AC策略梯度</h1><p>ACPG提出，解决PG方差大的问题<br>为了加快更新速度，我们希望把回合更新变为，每步更新，需要Q的估计值（指导策略更新），引入函数逼近器，取代PG中的Q采样</p><script type="math/tex; mode=display">Q_W(s,a) \approx Q^{\pi \theta}(s,a)</script><p>AC算法的主要部分 </p><ul><li>Critic：用来估计价值函数，（更新q(s,a,w)的参数w）</li><li>Actor：生成动作，（在critic的指引下，更新$\pi_\theta$的参数$\theta$）</li></ul><p>AC算法遵循， <strong>近似</strong>策略梯度法</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\theta} J(\theta) & \approx \mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) Q_{w}(s, a)\right] \\\Delta \theta &=\alpha \nabla_{\theta} \log \pi_{\theta}(s, a) Q_{w}(s, a)\end{aligned}</script><p>例子：简单线性价值函数的AC算法</p><ul><li>Critic 线性组合，TD(0)</li><li>Actor  PG更新</li><li><p>逐步更新，在线实时<br><img src="/img/15976384741868.jpg" alt="-w377"></p></li><li><p>AC算法中的偏差</p><ul><li>对PG的估计引入了偏差</li><li>正确选择价值函数，有利于减小、消灭偏差，but how？？？</li></ul></li></ul><h2 id="Compatible-function-approximation-兼容函数估计"><a href="#Compatible-function-approximation-兼容函数估计" class="headerlink" title="Compatible function approximation 兼容函数估计"></a>Compatible function approximation 兼容函数估计</h2><p>上节线性近似的价值函数引入了偏差，小心设计的Q函数满足：</p><p>1.近似价值函数的梯度完全等同于策略函数对数的梯度，即不存在重名情况：</p><script type="math/tex; mode=display">\nabla_{w} Q_{w}(s, a)=\nabla_{\theta} \log \pi_{\theta}(s, a)</script><p>2.价值函数参数w使得均方差最小：</p><script type="math/tex; mode=display">\varepsilon=\mathbb{E}_{\pi_{\theta}}\left[\left(Q^{\pi_{\theta}}(s, a)-Q_{w}(s, a)\right)^{2}\right]</script><p>此时策略梯度是准确的，满足</p><script type="math/tex; mode=display">\nabla_{\theta} J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) Q_{w}(s, a)\right]</script><ul><li>证明过程，（参数梯度 = 0）<br><img src="/img/15976423506794.jpg" alt="-w483"></li></ul><h3 id="Tricks——Advantage-function-critic"><a href="#Tricks——Advantage-function-critic" class="headerlink" title="Tricks——Advantage function critic"></a>Tricks——Advantage function critic</h3><p>核心思想：减去一个baseline，将MSE的减数和被减数都 往 0 方向拉，减小偏差<br>Advantage function = PG减去B(s)，好的B(s)是状态价值函数，V(s)是和<strong>策略无关</strong>的值，所以<strong>不改变梯度</strong>的期望的值<br><img src="/img/15976428419935.jpg" alt="-w465"></p><h4 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h4><ul><li><p>通过两个估计函数 和 两套参数，分别估计V、Q，进而估计A<br><img src="/img/15976451232295.jpg" alt="-w476"></p></li><li><p>直接用v值运算<br>但是并不需要用2个估计函数，因为<strong>TD误差是Q-V的无偏估计</strong></p></li></ul><p><img src="/img/15976451043580.jpg" alt="-w457"></p><h3 id="不同时间尺度下——Eligibility-Traces"><a href="#不同时间尺度下——Eligibility-Traces" class="headerlink" title="不同时间尺度下——Eligibility Traces"></a>不同时间尺度下——Eligibility Traces</h3><p>几种时间尺度下的更新算法</p><ul><li><p>针对<strong>Critic</strong>过程使用TD(λ)<br><img src="/img/15976458664595.jpg" alt="-w469"></p></li><li><p>针对<strong>Actor</strong>过程使用TD(λ)<br><img src="/img/15976475762634.jpg" alt="-w512"></p></li></ul><p><img src="/img/15976476902568.jpg" alt="-w473"></p><p>将TD(λ)的后向视角算法应用于实际问题时，可以在线实时更新，而且不需要完整的Episode。</p><p>PG是理论上是基于真实价值函数V(s)的<br>但是critic给PG提供了V的估计值，进而PG用估计的梯度去更新参数θ<br>结论是梯度也能使Actor更新到最优策略$\pi$</p><h3 id="Natural-policy-gradient"><a href="#Natural-policy-gradient" class="headerlink" title="Natural policy gradient"></a>Natural policy gradient</h3><p>高斯策略：按照期望和概率执行动作<br>缺点：对梯度估计不利，收敛性不好</p><p>Solution：Natural PG</p><ul><li>参数化独立</li><li><p>估计值与真值近似</p></li><li><p>对目标函数改写（噪声为0下）</p><script type="math/tex; mode=display">\nabla_{\theta}^{\text {nat}} \pi_{\theta}(s, a)=G_{\theta}^{-1} \nabla_{\theta} \pi_{\theta}(s, a)</script></li><li>Fisher information matrix 费雪信息矩阵<br>G反应了对估计值的准确程度，值越大，梯度更新越小，抑制噪声<script type="math/tex; mode=display">G_{\theta}=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a)^{T}\right]</script></li></ul><ul><li><p>Compatible function approximation</p><script type="math/tex; mode=display">\nabla_{w} A_{w}(s, a)=\nabla_{\theta} \log \pi_{\theta}(s, a)</script></li><li><p>简化为</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\theta} J(\theta) &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) A^{\pi_{\theta}}(s, a)\right] \\&=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a)^{T} w\right] \\&=G_{\theta} w \\\nabla_{\theta}^{n a t} J(\theta) &=w\end{aligned}</script></li><li><p><strong>用Critic参数，更新Actor参数</strong></p></li></ul><h1 id="AC算法的进行actor梯度更新的可选形式"><a href="#AC算法的进行actor梯度更新的可选形式" class="headerlink" title="AC算法的进行actor梯度更新的可选形式"></a>AC算法的进行actor梯度更新的可选形式</h1><p><img src="/img/16051921162903.jpg" alt=""></p><h1 id="只用值函数估计解决优势函数估计"><a href="#只用值函数估计解决优势函数估计" class="headerlink" title="只用值函数估计解决优势函数估计"></a>只用值函数估计解决优势函数估计</h1><p><img src="/img/16052492307800.jpg" alt=""></p><h2 id="总结PG"><a href="#总结PG" class="headerlink" title="总结PG"></a>总结PG</h2><p><img src="/img/15976510887080.jpg" alt="-w519"></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> 策略梯度 Policy gradient </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从0 -&gt; 1，拥有你的免费个人博客之“打个前站”</title>
      <link href="/posts/blog-setup/"/>
      <url>/posts/blog-setup/</url>
      
        <content type="html"><![CDATA[<h1 id="为什么想写个博客耍？"><a href="#为什么想写个博客耍？" class="headerlink" title="为什么想写个博客耍？"></a>为什么想写个博客耍？</h1><p>我们在生活和工作中会遇到的各种问题，现在基本都能从互联网上找到答案，因为个体相较于群体，所能接触到的面，太窄，也太小了。以前常去“百度知道”去搜答案，上网的门槛逐步降低之后，“知道”也不知道了，碎片化的问答里总是充斥着各种水军、广告，令人窒息。<br><img src="/img/15968069169200.jpg" alt="你说得对"></p><p>现在，你询问搜索引擎的问题，大多会在“知乎”、“博客”找到答案，与“知道”不同的是，详实且完整，足可以指引你前进。</p><p>然后就突发奇想，自己也写个吧？既然也在能力范围之内，就找资料去做了，正好也可以把最近写的笔记什么的归档起来。以前，没有读书写作的习惯，靠着还不算傻的脑瓜也把义务教育兑付了，往后才意识到，不是笔记没有用，而是你不知道它有用。也不知道是脑子笨了，还是知识多了，总容易忘东西，才意识到：写作的过程就是思考的量化，写不出来就是你不会，你不懂，你忘了，如果把自己的碎片拼成一篇博文，别人看了，能有所启发，甚至帮他解决了别地儿没解决的问题，那就更棒了。</p><p>博客同样也是一个记录生活的地方，比起微博，也更像回事儿。因为微博并不适用于所有的场合，碎片化的文字，更适合用来打发公交、地铁的闲暇。博客上的东西比微博，多少还是更有含金量的，当然这只针对大部分情况，如果有人在微博写书，咱也不能有啥意见，毕竟用微博(Twitter)治国的人，还是有的。<br><img src="/img/15968070216543.jpg" alt="你必须承认, 我独当一面"></p><p>前前后后花费了2天时间。就感慨，很多时候，事情能做成，大概有两种情况：</p><ol><li>你不得不做：老板追你，朋友挺你，亲人需要你</li><li>你想做：发自内心的想</li></ol><p>不过博客搭起来只是第一步，能坚持下去，你才算赢了，希望你的博客能见证你的成长</p><h1 id="能做到啥样"><a href="#能做到啥样" class="headerlink" title="能做到啥样"></a>能做到啥样</h1><p>🥮是指明灯，指引你的前行，让你总是充满着动力<br>po几个网友的样品图，啥样式的基本都全了<br><img src="/img/15968016972657.jpg" alt="简约系"></p><p><img src="/img/15968017968960.jpg" alt="高级系"></p><p><img src="/img/15968021026230.jpg" alt="粉系"></p><p>如果这些都不能满足你，可以来这看一下，<a href="https://hexo.io/themes/">Hexo.io/theme</a>，该挑花你眼。</p><p>虽然搭建过程可能对新手来说不是很友好，不过先看一下效果图，我想你早就干劲儿满满了</p><h1 id="路有三条，我择其一"><a href="#路有三条，我择其一" class="headerlink" title="路有三条，我择其一"></a>路有三条，我择其一</h1><p>目前常规的博客制造方法，一般有三种：</p><ul><li>个人主页注册：<br>指的是在现有的博客网站CSDN、Cnblog等注册即用，门槛低，自定义程度低，部分充斥广告⛔️</li><li>静态网站生成：<br>利用hexo等框架生成静态网站，然后上传到Github等平台（或者自己服务器）托管展示，稍微有点麻烦，上限最高，You can do whatever you like😊!!!</li><li>内容管理系统：<br>带有后台管理的博客系统，有现成的可以装（wordpress、ghost等），装好了就和第一个一样。缺点：需要配置空间（服务器）、数据库以及域名等，经费在燃烧🔥</li></ul><p>如果你非说还有一种，手写html？？？<br>大型劝退现场🙄</p><pre><code class="lang-html">&lt;div class="container row center-align" style="margin-bottom: 15px !important;"&gt;        &lt;div class="col s12 m8 l8 copy-right"&gt;            Copyright&amp;nbsp;&amp;copy;            &lt;span id="year"&gt;2020&lt;/span&gt;            &lt;a href="/about" target="_blank"&gt;Tolshao&lt;/a&gt;            |&amp;nbsp;Powered by&amp;nbsp;&lt;a href="https://hexo.io/" target="_blank"&gt;Hexo&lt;/a&gt;            |&amp;nbsp;Theme&amp;nbsp;&lt;a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank"&gt;Matery&lt;/a&gt;            &lt;br&gt;            &amp;nbsp;&lt;i class="fas fa-chart-area"&gt;&lt;/i&gt;&amp;nbsp;站点总字数:&amp;nbsp;&lt;span                class="white-color"&gt;31.5k&lt;/span&gt;&amp;nbsp;字</code></pre><p>我只能说，连牛顿也是踩在巨人肩膀上的，以我的半斤八两，就不去硌别人脚了😂</p><p>时间有限，有关blog搭建的技术部分还未整理完成<br>就相约在下次的推送和大家再见</p><p>欢迎催更👀</p><h2 id="需要用到的工具-todo"><a href="#需要用到的工具-todo" class="headerlink" title="需要用到的工具(todo)"></a>需要用到的工具(todo)</h2><ul><li>Hexo</li></ul><ul><li><a href="https://git-scm.com/book/zh/v2/起步-安装-Git">Git</a></li></ul><ul><li>Node.js</li></ul><ul><li>SSH配置</li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ios黄页：可算让iPhone好用了点儿</title>
      <link href="/posts/ios-yellow-page/"/>
      <url>/posts/ios-yellow-page/</url>
      
        <content type="html"><![CDATA[<h1 id="张小跳-ios黄页"><a href="#张小跳-ios黄页" class="headerlink" title="张小跳-ios黄页"></a>张小跳-ios黄页</h1><p>分享一个ios黄页工具，领取方式见文末</p><h1 id="黄页是什么"><a href="#黄页是什么" class="headerlink" title="黄页是什么"></a>黄页是什么</h1><p>说白了，可以理解为指电话号码簿，几乎世界每一个城市都有过这种纸张为载体所印制的电话号码本。</p><p>【百度百科】定义：黄页是国际通用按企业性质和产品类别编排的工商企业电话号码簿，以刊登企业名称、地址、电话号码为主体内容，相当于一个城市或地区的工商企业的户口本，国际惯例用黄色纸张印制，故称黄页。黄页，起源于北美洲，1880年世界上第一本黄页电话号簿在美国问世，至今已有100多年的历史。<br>以前，它长这个样子：<br><img src="/img/15959053121203.jpg" alt="-w412"></p><p>后来，它长这个样子，上网了</p><p><img src="/img/15959047443292.jpg" alt="w500"></p><p>随着互联网等信息技术的发展，黄页逐渐退出了历史舞台，人们的浏览器被“百度”、“Bing”、“Sogou”、“Google”等搜索引擎替代，这符合人们的行为认知，也极大地提高了效率</p><p>再后来，手机成为了每个人随时随地就能上网的工具，手机厂商发现这里能赚“5毛钱”，所以Android的黄页应运而生</p><p><img src="/img/15959064265953.jpg" alt="-w341"></p><p>经过了服务商的整合，确实是要比某度搜索要快的，毕竟……<br><img src="/img/15959063542994.jpg" alt="-w701"><br>我们也不好多说什么了<br>不过作为外地开发商，Apple在这一点做的属实落后了半个世纪，所以</p><h1 id="干货奉上"><a href="#干货奉上" class="headerlink" title="干货奉上"></a>干货奉上</h1><p>导入常用联系人头像，优化 iOS 来电、信息界面体验。</p><p><img src="/img/15970556799481.png" alt=""></p><p>很醒目有木有，以后短信、电话更方便了</p><h2 id="使用指南"><a href="#使用指南" class="headerlink" title="使用指南"></a>使用指南</h2><ol><li>私信公众号“黄页”下载 <code>黄页.zip</code>；</li><li>解压后，根据不同平台的指南导入 <code>vcf</code> 文件至 iCloud 中，推荐单独创建「黄页」分组方便管理和隐藏。</li></ol><hr><h2 id="号码收录"><a href="#号码收录" class="headerlink" title="号码收录"></a>号码收录</h2><p>由于不同地区不同运营商的 106 短信推送号段存在差异，项目不作收录，建议将本项目作为一个基础模板，导入联系人后可以按以下方式自行补充其余号码</p><p><img src="https://user-images.githubusercontent.com/2666735/59747105-ccd33480-92aa-11e9-90e0-93f295dcb504.png" alt="Screenshot"></p><h2 id="图标设计"><a href="#图标设计" class="headerlink" title="图标设计"></a>图标设计</h2><ul><li>采用 <code>PNG</code> 编码</li><li>画布大小 <code>200w200h</code></li><li>logo 居中放置<ul><li>圆形尺寸 140w140h</li><li>正矩形尺寸 120w120h</li><li>长矩形尺寸 160w80h</li><li>无 svg 需要使用 Inkscape 改绘转换</li><li>特殊情况特殊处理</li></ul></li><li>图像大小压缩在 <code>20 kB</code> 内</li></ul><p><img src="/img/15970557154990.png" alt=""></p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><ul><li>项目来自github: metowolf/vCards</li><li><a href="http://www.114best.com/">114 百事通</a>提供查询接口</li><li><a href="https://haoma.baidu.com/yellowPage">百度手机卫士</a>提供查询接口</li><li><a href="https://www.kexinhaoma.org/">中国可信号码数据中心</a>提供查询接口</li></ul><p>====<br>推荐朋友关注公众号“探物及理”，谢谢各位<br>ios黄页，可以后台回复“黄页”领取。</p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ios </tag>
            
            <tag> 黄页 </tag>
            
            <tag> iPhone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么数值仿真里要用RK4（龙格库塔法）</title>
      <link href="/posts/rk4/"/>
      <url>/posts/rk4/</url>
      
        <content type="html"><![CDATA[<p>小跳最近在搭建一个数值仿真环境，由于需要用到python里面的一些库，所以不得不把simulink的模型搬过来，我们都知道在simulink里，仿真的时候设置仿真步长和微分方程求解器是必要的步骤。但是为什么要设置这个小跳却早已忘记了。</p><p>一年级的时候搬砖搬多了，数分课也没好好上，回头一看，这么简单的东西，当时竟然整的稀里糊涂的。</p><h1 id="为什么要用RK4"><a href="#为什么要用RK4" class="headerlink" title="为什么要用RK4"></a>为什么要用RK4</h1><p>先po一张图，直观感受一下仿真的误差。<br><img src="/img/15956439603281.jpg" alt="-w752"></p><p>对于给定线性常微分方程</p><script type="math/tex; mode=display">\dot x = x</script><p>易得，其解是</p><script type="math/tex; mode=display">x(t) = Ce^t</script><p>RK4是龙格库塔法曲线，None是一阶解法$x(t+dt) = x(t)+\dot x dt$<br>可以看到，线性常微分方程误差尚且如此之大，那么推广到非线性微分方程，像这种形式</p><script type="math/tex; mode=display">\dot x = f(x,t) = tx^2 - \frac{x}{t}...</script><p>那肯定误差直接起飞了。解析解求起来也挺麻烦，这里就不再引入分析了。</p><p>接下来把定义回顾一下，贴一下代码，有需自取，希望对大家有所帮助。</p><h1 id="定义回顾"><a href="#定义回顾" class="headerlink" title="定义回顾"></a>定义回顾</h1><p>数值分析中，龙格－库塔法（Runge-Kutta methods）是用于非线性常微分方程的解的重要的一类隐式或显式迭代法。这些技术由数学家卡尔·龙格和马丁·威尔海姆·库塔于1900年左右发明。该方法主要是在已知方程导数和初值信息，利用计算机仿真时应用，省去求解微分方程的复杂过程。</p><p>令初值问题表述如下。</p><script type="math/tex; mode=display">y' = f(t,y), y(t_0) = y_0</script><p>则，对于该问题的RK4由如下方程给出：</p><script type="math/tex; mode=display">y_{n+1}=y_{n}+\frac{h}{6}\left(k_{1}+2 k_{2}+2 k_{3}+k_{4}\right) \\</script><p>其中</p><script type="math/tex; mode=display">\begin{matrix}k_{1}=f\left(t_{n}, y_{n}\right) \\ k_{2}=f\left(t_{n}+\frac{h}{2}, y_{n}+\frac{h}{2} k_{1}\right) \\k_{3}=f\left(t_{n}+\frac{h}{2}, y_{n}+\frac{h}{2} k_{2}\right) \\k_{4}=f\left(t_{n}+h, y_{n}+h k_{3}\right)\end{matrix}</script><p>式中，$h$为仿真步长，满足$h&lt;\epsilon_1 \rightarrow error&lt;\epsilon_2$</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><pre><code class="lang-python">import numpy as npimport matplotlib.pyplot as pltimport sympy as sy# 这里介绍一个符号运算的方法，可以用来求解方程什么的def diff_eq(t,x):    return sy.diff(x(t),t,1) - x(t)t = sy.symbols('t')x = sy.Function('x')sy.pprint(sy.dsolve(diff_eq(t,x),x(t)))def dot_x(t,x):    return xdef rk4(f,t,x,h):    k1 = f(t,x);    k2 = f(t+0.5*h,x + 0.5*h*k1)    k3 = f(t+0.5*h,x + 0.5*h*k2)    k4 = f(t+h,x + h*k3)    return h/6*(k1+2*k2+2*k3+k4)t_list = np.arange(0,5,0.1);#print(t)x1_list = np.exp(t_list)x2_list = []x3_list = []h = 0.1x2 = 1;x3 = 1;for t in t_list:#    print(t,idx)    x2_list.append(x2)    x3_list.append(x3)    x2 = x2 + rk4(dot_x,t, x2, h)    x3 = x3 + dot_x(t,x3) * h                error_2 = x1_list - x2_listerror_3 = x1_list - x3_listplt.figure()plt.subplot(2,1,1)plt.plot(t_list,x1_list, 'b-',label='Real')plt.plot(t_list,x2_list,'r--', label = 'RK4')plt.plot(t_list,x3_list,'g--', label = 'None')plt.legend()plt.subplot(2,1,2)plt.plot(t_list,error_2, 'r--',label='Error_RK4')plt.plot(t_list,error_3, 'g--',label='Error_none')plt.legend()plt.xlabel('Time(s)')plt.show()</code></pre><h1 id="闲话"><a href="#闲话" class="headerlink" title="闲话"></a>闲话</h1><p>这里推荐一个提高效率的工具Matplotlib cheat sheet</p><p>对于一个经常画图的科研狗来说，这张图真是太太太太有必要了，因为时常遇到以下场景，不记得colormap名字，打开文档查一番，不记得线宽关键词，打开文档查一番，不记得marker名字，打开文档查一番。。。。。等等等等<br><img src="/img/15956450473625.jpg" alt=""></p><p>所以，有了这张图，在平常画图的时候中遇到的95%需要查文档的问题都可以在这张图中找到答案。</p><p>===<br>这个速查表，可以关注微信公众号“探物及理”后台回复“python画图”领取。</p><p>===<br><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RK4 </tag>
            
            <tag> 数值仿真 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记6：值函数估计Value function Approximation</title>
      <link href="/posts/rl-6/"/>
      <url>/posts/rl-6/</url>
      
        <content type="html"><![CDATA[<h1 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h1><h2 id="v、q表的问题"><a href="#v、q表的问题" class="headerlink" title="v、q表的问题"></a>v、q表的问题</h2><ul><li>解决离散化的s,a,导致q-table存储量、运算量大</li><li>解决连续s、a的表示问题</li></ul><h2 id="solution"><a href="#solution" class="headerlink" title="solution"></a>solution</h2><p>用带权重估计函数，估计v or q</p><script type="math/tex; mode=display">\begin{aligned}\hat{v}(s, \mathbf{w}) & \approx v_{\pi}(s) \\\text { or } \hat{q}(s, a, \mathbf{w}) & \approx q_{\pi}(s, a)\end{aligned}</script><h2 id="函数估计器"><a href="#函数估计器" class="headerlink" title="函数估计器"></a>函数估计器</h2><p>可谓函数逼近，需要函数式可微分的</p><ul><li>线性组合</li><li>神经网络</li></ul><p>这些<strong>不可微</strong></p><ul><li>决策树 decision tree</li><li>领域Nearest neighbour</li><li>傅里叶/ 小波Fourier/ wavelet bases</li></ul><h1 id="incremental-methods-递增方法"><a href="#incremental-methods-递增方法" class="headerlink" title="incremental methods 递增方法"></a>incremental methods 递增方法</h1><h2 id="Gradient-descent-梯度下降"><a href="#Gradient-descent-梯度下降" class="headerlink" title="Gradient descent 梯度下降"></a>Gradient descent 梯度下降</h2><p>值函数估计：随机梯度下降法SGD<br><img src="/img/15965268722996.jpg" alt="-w523"></p><h2 id="Table-lookup-是-GD的一种特例"><a href="#Table-lookup-是-GD的一种特例" class="headerlink" title="Table lookup 是 GD的一种特例"></a>Table lookup 是 GD的一种特例</h2><p>类似于机器学习的分类问题，将状态值写成0、1向量</p><p><img src="/img/15965277694513.jpg" alt="-w290"></p><h2 id="Find-a-target-for-value-function-approximation"><a href="#Find-a-target-for-value-function-approximation" class="headerlink" title="Find a target for value function approximation"></a>Find a target for value function approximation</h2><p>把估计函数作为一个监督学习<br>目标是谁呢，通过MC、TD方法，设定目标<br><img src="/img/15965279560154.jpg" alt="-w439"></p><h2 id="生成训练集"><a href="#生成训练集" class="headerlink" title="生成训练集"></a>生成训练集</h2><h3 id="For-linear-MC"><a href="#For-linear-MC" class="headerlink" title="For linear MC"></a>For linear MC</h3><p><img src="/img/15965286821264.jpg" alt="-w335"></p><ul><li>无偏目标估计</li><li>局部最优</li></ul><h3 id="For-linear-TD（0）"><a href="#For-linear-TD（0）" class="headerlink" title="For linear TD（0）"></a>For linear TD（0）</h3><p><img src="/img/15965287396993.jpg" alt="-w479"></p><ul><li>收敛趋向全局最优</li></ul><h3 id="For-linear-TD（-lambda-）"><a href="#For-linear-TD（-lambda-）" class="headerlink" title="For linear TD（$\lambda$）"></a>For linear TD（$\lambda$）</h3><p><img src="/img/15970471950069.jpg" alt="-w427"></p><p>$\delta$ scalar number<br>$E_t$ 维度和s维度一致</p><ul><li>前后向 相等</li></ul><h1 id="Incremental-Control-Algorithms"><a href="#Incremental-Control-Algorithms" class="headerlink" title="Incremental Control Algorithms"></a>Incremental Control Algorithms</h1><p>用q函数，替代v函数<br><img src="/img/15970492924377.jpg" alt="-w492"></p><p><img src="/img/15970496242583.jpg" alt="-w530"></p><h2 id="收敛性分析"><a href="#收敛性分析" class="headerlink" title="收敛性分析"></a>收敛性分析</h2><ul><li><p>预测学习</p><ul><li>On-policy：一般边训练，边执行，(s,a)是当前policy产生的</li><li>off-policy：离线训练，通过训练其他策略或者agent产生的(s,a)训练集</li></ul></li></ul><p><img src="/img/15970511491893.jpg" alt="-w527"><br>引入Gradient TD，完全满足贝尔曼方程，无差<br><img src="/img/15970539060941.jpg" alt="-w540"></p><ul><li>控制学习</li></ul><p><img src="/img/15970539922026.jpg" alt="-w503"></p><p>（√）表示在最优值函数附近振荡</p><h1 id="batch-methods"><a href="#batch-methods" class="headerlink" title="batch methods"></a>batch methods</h1><h2 id="For-least-squares-prediction"><a href="#For-least-squares-prediction" class="headerlink" title="For least squares prediction"></a>For least squares prediction</h2><p>LS定义，估计误差平方，求和<br><img src="/img/15970588911111.jpg" alt="-w495"></p><p>相当于经历重现（experience replay）</p><ul><li>从history中sample一个batch</li><li>用SGD更新参数w</li></ul><p><img src="/img/15970589362830.jpg" alt="-w526"><br>找到使LS最小的权重$w^\pi$</p><h2 id="Experience-Replay-in-Deep-Q-Networks-DQN"><a href="#Experience-Replay-in-Deep-Q-Networks-DQN" class="headerlink" title="Experience Replay in Deep Q-Networks (DQN)"></a>Experience Replay in Deep Q-Networks (DQN)</h2><h3 id="Two-features"><a href="#Two-features" class="headerlink" title="Two features"></a>Two features</h3><ul><li><strong>Experience Relpay</strong>：minibatch的数据采样自memory-D</li><li><strong>Fixed Q-targets</strong>：$w^-$ 在一个更新batch内 ，保持不变，让更新过程更稳定<br><img src="/img/15970708646292.jpg" alt="-w610"></li></ul><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ol><li>Take action at according to ε-greedy policy</li><li>Store transition (st,at,rt+1,st+1) in replay memory D </li><li>Sample random mini-batch of transitions (s,a,r,s′) from D </li><li>Compute Q-learning targets w.r.t. old, fixed parameters $w^− $</li><li>Optimise MSE between Q-network and Q-learning targets</li></ol><script type="math/tex; mode=display">\mathcal{L}_{i}\left(w_{i}\right)=\mathbb{E}_{s, a, r, s^{\prime}} \sim \mathcal{D}_{i}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; w_{i}^{-}\right)-Q\left(s, a ; w_{i}\right)\right)^{2}\right]</script><ol><li>用SGD更新<br>伪算法：<br><img src="/img/15971441168183.jpg" alt=""><br><strong>注意：</strong></li></ol><ul><li>Fixed Q-target $\theta$，每C steps 更新一次</li><li>Experience Replay： minibatch 从 memory D 采样</li></ul><p>Features：</p><ul><li>随机采样，打破了状态之间的联系</li><li>冻结参数，增加了算法的<strong>稳定性</strong>，选q的网络参数回合制更新</li></ul><ul><li>例子<br>DQN in Atari（构成）<ul><li>input: state s （4 frames pictures）</li><li>output: Q(s,a) </li><li>CNN： mapping input(s) to output(Q)</li></ul></li></ul><h3 id="LS-最小二乘法-总结"><a href="#LS-最小二乘法-总结" class="headerlink" title="LS 最小二乘法  总结"></a>LS 最小二乘法  总结</h3><ul><li>Experience replay -&gt; LS solution</li><li>迭代次数太多</li><li>用线性估计$\hat v(s,w) = x(s)^Tw$</li><li>直接求解LS</li></ul><h2 id="LSP-直接求解"><a href="#LSP-直接求解" class="headerlink" title="LSP 直接求解"></a>LSP 直接求解</h2><p>对于线性近似函数：</p><script type="math/tex; mode=display">\hat v(s,w) = x(s)^T w</script><p>最终的平衡状态，梯度=0<br>求解方程，得到w值关于状态s和v真值的函数关系<br><img src="/img/15970717288619.jpg" alt="-w592"><br>However，真值<strong>不知道</strong><br>缺点是复杂度高，引入了矩阵的逆</p><h3 id="Other-algorithms"><a href="#Other-algorithms" class="headerlink" title="Other algorithms"></a>Other algorithms</h3><p><img src="/img/15971205641184.jpg" alt="-w574"><br><img src="/img/15971205882497.jpg" alt="-w598"><br><img src="/img/15971207382795.jpg" alt="-w603"></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> 值函数估计 value function approximation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras &amp; Tensorflow 笔记</title>
      <link href="/posts/keras-tensorflow/"/>
      <url>/posts/keras-tensorflow/</url>
      
        <content type="html"><![CDATA[<p>Keras是一个高层神经网络API，Keras由纯Python编写而成并基于Tensorflow、Theano以及CNTK后端。Keras为支持快速实验而生，能够把你的idea迅速转换为结果，如果你有如下需求，请选择Keras：</p><ul><li>简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性）</li><li>支持CNN和RNN，或二者的结合</li><li>无缝CPU和GPU切换</li></ul><p>有串联式和函数式两种建模方式，串联式建模方式</p><ul><li>串联式Sequential：</li></ul><pre><code class="lang-python">model = Sequential()model.add(Dense(32, input_dim=784))model.add(Activation('relu'))</code></pre><ul><li>函数式：</li></ul><pre><code>def model_name(input_shape, output_shape):    inputs = Input(shape = input_shape, dtype = , name = '')    x = Dense(64 , activation='relu')(inputs)    x = Dense(64,activation='relu')(x)    predictions = Dense(output_shape,activation='softmax')(x)    model = Model(inputs=inputs, outputs=predictions)    return model</code></pre><h1 id="学习一般操作步骤"><a href="#学习一般操作步骤" class="headerlink" title="学习一般操作步骤"></a>学习一般操作步骤</h1><p>from keras import Model ……</p><ol><li>Generate：<br><code>model = Model(inputs = input, outputs = output)</code></li><li>Compile：<br>model.compile(配置优化器，学习率，误差等参数)</li><li>Fit \ Train：<br><code>model.fit(X_train, Y_train, (X_dev, Y_dev),metric = [])</code></li><li>Evaluate \ test：<br><code>model.predict(X_test, Y_test)</code><br>fit和predict函数有返回值的，最好用一个变量来接住，方便查看预测过程中的变量信息history。</li></ol><h1 id="Tricks-and-Snippets"><a href="#Tricks-and-Snippets" class="headerlink" title="Tricks and Snippets"></a>Tricks and Snippets</h1><h2 id="模型可视化"><a href="#模型可视化" class="headerlink" title="模型可视化"></a>模型可视化</h2><h3 id="命令行打印：keras自带的summary函数"><a href="#命令行打印：keras自带的summary函数" class="headerlink" title="命令行打印：keras自带的summary函数"></a>命令行打印：keras自带的summary函数</h3><p><code>model.summary()</code><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15954994831618.jpg?x-oss-process=style/blog" alt="-w582"></p><h3 id="调用库，打印保存图片"><a href="#调用库，打印保存图片" class="headerlink" title="调用库，打印保存图片"></a>调用库，打印保存图片</h3><p>使用方法：</p><pre><code class="lang-python">Keras.utils.plot_model plot_model(model,to_file='a.png')</code></pre><p>结果如下，还可以保存为pdf等格式<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15955035723718.jpg?x-oss-process=style/blog" alt="-w425"></p><h3 id="PlotNeuralNet绘制latex风格的网络图"><a href="#PlotNeuralNet绘制latex风格的网络图" class="headerlink" title="PlotNeuralNet绘制latex风格的网络图"></a><a href="https://github.com/HarisIqbal88/PlotNeuralNet">PlotNeuralNet</a>绘制latex风格的网络图</h3><p>例图：<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15955111765714.jpg?x-oss-process=style/blog" alt="-w456"></p><p>使用方法：</p><ol><li>下载github源文件，安装pycore库，将目录中的python包，拷贝至<code>/usr/local/lib/python3.7/site-packages/pycore/</code></li><li>根据example搭建网络结构的python文件</li><li>运行python文件生成filename.tex文件</li><li><code>pdflatex filename.tex</code>，此步骤需要提前拷贝源文件layers中sty文件至tex文件目录，用pdflaetx编译需要texlive环境，请提前安装。</li></ol><h3 id="model-fit函数调用"><a href="#model-fit函数调用" class="headerlink" title="model.fit函数调用"></a>model.fit函数调用</h3><p>这个方法最为硬核，其中mandb还可以横纵向对比多个模型的各个参数，并方便debug和optimize<br>使用方法：</p><pre><code class="lang-python">from rl.callbacks import WandbLoggerimport tensorboardmodel.fig(巴拉巴拉, callbacks = [函数])</code></pre><p>``，在网页localhost可视化</p><ul><li>TensorBoard</li><li>Mandb：callbacks=[WandbLogger()]，需要提前进行wandb初始化并在config中定义需要log的变量。<br>Tensorboard作者没有去尝试，这里就先贴一张Wandb的可视化结果：<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15955742598518.jpg?x-oss-process=style/blog" alt="-w1436"></li></ul><h3 id="Netron软件"><a href="#Netron软件" class="headerlink" title="Netron软件"></a>Netron软件</h3><p>下载安装，导入keras模型.h5即可食用，也支持tf、pytorch等多种模型，界面如下<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15954993270061.jpg?x-oss-process=style/blog" alt="-w866"></p><h3 id="超参数调节"><a href="#超参数调节" class="headerlink" title="超参数调节"></a>超参数调节</h3><p>超参数就是模型权重以外的其他参数，比如层种类，层深度、宽度，优化器类型、学习率大小等等，它们都影响着模型的表现和上限，超参数一动，就是一个新的模型了。但是超参数却没有像构建神经网络一样有可遵照的理论指导，一直以来都是从业人员的难点。<br>虽然网上已经有很多关于超参数调节的帖子，但大多都为经验之谈，是研究人员在实践中摸索、发现并总结的。就像控制理论里最简单PID调节一样，三个参数就能调的人头大，有些模型遵照经验去调还可能不work。<br>其实这点早就为我们想到了，作者找到了几个超参数调节器</p><h4 id="keras-tunner"><a href="#keras-tunner" class="headerlink" title="keras tunner"></a><a href="https://blog.csdn.net/wmq104/article/details/105740497">keras tunner</a></h4><p>根据验证集的表现自动优化超参数<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15955749309209.jpg?x-oss-process=style/blog" alt=""></p><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15955749728608.jpg?x-oss-process=style/blog" alt=""></p><h4 id="keras-lr-finder"><a href="#keras-lr-finder" class="headerlink" title="keras-lr-finder"></a>keras-lr-finder</h4><p>使用方法：安装python库keras_lr_finder</p><p>代码：引用库，包装模型，绘制结果</p><pre><code>import keras_lr_finder# model is a Keras modellr_finder = LRFinder(model)# Train a model with batch size 512 for 5 epochs# with learning rate growing exponentially from 0.0001 to 1lr_finder.find(x_train, y_train, start_lr=0.0001, end_lr=1, batch_size=512, epochs=5)# Plot the loss, ignore 20 batches in the beginning and 5 in the endlr_finder.plot_loss(n_skip_beginning=20, n_skip_end=5)</code></pre><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15948027436905.png?x-oss-process=style/blog" alt=""></p><pre><code># Plot rate of change of the loss# Ignore 20 batches in the beginning and 5 in the end# Smooth the curve using simple moving average of 20 batches# Limit the range for y axis to (-0.02, 0.01)lr_finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01))</code></pre><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15948027304867.png?x-oss-process=style/blog" alt=""></p><h4 id="利用scikit-learn交互网格搜索超参数"><a href="#利用scikit-learn交互网格搜索超参数" class="headerlink" title="利用scikit-learn交互网格搜索超参数"></a><a href="https://blog.csdn.net/happytofly/article/details/80124813">利用scikit-learn交互网格搜索超参数</a></h4><h1 id="设置备忘"><a href="#设置备忘" class="headerlink" title="设置备忘"></a>设置备忘</h1><h2 id="Keras下载的预训练数据存放目录"><a href="#Keras下载的预训练数据存放目录" class="headerlink" title="Keras下载的预训练数据存放目录"></a>Keras下载的预训练数据存放目录</h2><p><code>root\\.keras\models</code></p><h1 id="错误记录"><a href="#错误记录" class="headerlink" title="错误记录"></a>错误记录</h1><ul><li><strong>非</strong>张量运算变量运算用内置函数，+ - 操作会把张量 转为 Tensorflow，报错</li><li>实数，不用tf. 或者 K. 函数库运算，报错“张量”</li><li>张量一定用内置函数，python支持@ + - 等操作，但是偶尔报错</li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 ML </tag>
            
            <tag> keras </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习22张精炼图笔记总结</title>
      <link href="/posts/deep-learning-summary/"/>
      <url>/posts/deep-learning-summary/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习精炼图笔记总结"><a href="#深度学习精炼图笔记总结" class="headerlink" title="深度学习精炼图笔记总结"></a>深度学习精炼图笔记总结</h1><p>本文转自知乎（Sophia）公众号【计算机视觉联盟】<br>笔记图片由 TessFerrandez 整理，这套信息图优美地记录了深度学习课程的知识与亮点。因此它不仅仅适合初学者了解深度学习，还适合机器学习从业者和研究者复习基本概念。这不仅仅是一份课程笔记，同时还是一套信息图与备忘录。</p><p>从深度学习基础、卷积网络和循环网络三个方面介绍该笔记</p><h1 id="1-深度学习基本概念"><a href="#1-深度学习基本概念" class="headerlink" title="1. 深度学习基本概念"></a>1. 深度学习基本概念</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-1-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-1-1024"></p><p>监督学习：所有输入数据都有确定的对应输出数据，在各种网络架构中，输入数据和输出数据的节点层都位于网络的两端，训练过程就是不断地调整它们之间的网络连接权重。</p><p>左上：列出了各种不同网络架构的监督学习，比如标准的神经网络（NN）可用于训练房子特征和房价之间的函数，卷积神经网络（CNN）可用于训练图像和类别之间的函数，循环神经网络（RNN）可用于训练语音和文本之间的函数。</p><p>左下：分别展示了 NN、CNN 和 RNN 的简化架构。这三种架构的前向过程各不相同，NN 使用的是权重矩阵（连接）和节点值相乘并陆续传播至下一层节点的方式；CNN 使用矩形卷积核在图像输入上依次进行卷积操作、滑动，得到下一层输入的方式；RNN 记忆或遗忘先前时间步的信息以为当前计算过程提供长期记忆。</p><p>右上：NN 可以处理结构化数据（表格、数据库等）和非结构化数据（图像、音频等）。</p><p>右下：深度学习能发展起来主要是由于大数据的出现，神经网络的训练需要大量的数据；而大数据本身也反过来促进了更大型网络的出现。深度学习研究的一大突破是新型激活函数的出现，用 ReLU 函数替换sigmoid 函数可以在反向传播中保持快速的梯度下降过程，sigmoid 函数在正无穷处和负无穷处会出现趋于零的导数，这正是梯度消失导致训练缓慢甚至失败的主要原因。要研究深度学习，需要学会「idea—代码—实验—idea」的良性循环。</p><h1 id="2-logistic-回归"><a href="#2-logistic-回归" class="headerlink" title="2. logistic 回归"></a>2. logistic 回归</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-2-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-2-1024"></p><p>左上：logistic 回归主要用于二分类问题，如图中所示，logistic 回归可以求解一张图像是不是猫的问题，其中图像是输入（x），猫（1）或非猫（0）是输出。我们可以将 logistic 回归看成将两组数据点分离的问题，如果仅有线性回归（激活函数为线性），则对于非线性边界的数据点（例如，一组数据点被另一组包围）是无法有效分离的，因此在这里需要用非线性激活函数替换线性激活函数。在这个案例中，我们使用的是 sigmoid 激活函数，它是值域为（0, 1）的平滑函数，可以使神经网络的输出得到连续、归一（概率值）的结果，例如当输出节点为（0.2, 0.8）时，判定该图像是非猫（0）。</p><p>左下：神经网络的训练目标是确定最合适的权重 w 和偏置项 b，那这个过程是怎么样的呢？</p><p>这个分类其实就是一个优化问题，优化过程的目的是使预测值 y hat 和真实值 y 之间的差距最小，形式上可以通过寻找目标函数的最小值来实现。所以我们首先确定目标函数（损失函数、代价函数）的形式，然后用梯度下降逐步更新 w、b，当损失函数达到最小值或者足够小时，我们就能获得很好的预测结果。</p><p>右上：损失函数值在参数曲面上变化的简图，使用梯度可以找到最快的下降路径，学习率的大小可以决定收敛的速度和最终结果。学习率较大时，初期收敛很快，不易停留在局部极小值，但后期难以收敛到稳定的值；学习率较小时，情况刚好相反。一般而言，我们希望训练初期学习率较大，后期学习率较小，之后会介绍变化学习率的训练方法。</p><p>右下：总结整个训练过程，从输入节点 x 开始，通过前向传播得到预测输出 y hat，用 y hat 和 y 得到损失函数值，开始执行反向传播，更新 w 和 b，重复迭代该过程，直到收敛。</p><h1 id="3-浅层网络的特点"><a href="#3-浅层网络的特点" class="headerlink" title="3. 浅层网络的特点"></a>3. 浅层网络的特点</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-3-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-3-1024"></p><p>左上：浅层网络即隐藏层数较少，如图所示，这里仅有一个隐藏层。</p><p>左下：这里介绍了不同激活函数的特点：</p><p>sigmoid：sigmoid 函数常用于二分分类问题，或者多分类问题的最后一层，主要是由于其归一化特性。sigmoid 函数在两侧会出现梯度趋于零的情况，会导致训练缓慢。<br>tanh：相对于 sigmoid，tanh 函数的优点是梯度值更大，可以使训练速度变快。<br>ReLU：可以理解为阈值激活（spiking model 的特例，类似生物神经的工作方式），该函数很常用，基本是默认选择的激活函数，优点是不会导致训练缓慢的问题，并且由于激活值为零的节点不会参与反向传播，该函数还有稀疏化网络的效果。<br>Leaky ReLU：避免了零激活值的结果，使得反向传播过程始终执行，但在实践中很少用。<br>右上：为什么要使用激活函数呢？更准确地说是，为什么要使用非线性激活函数呢？</p><p>上图中的实例可以看出，没有激活函数的神经网络经过两层的传播，最终得到的结果和单层的线性运算是一样的，也就是说，没有使用非线性激活函数的话，无论多少层的神经网络都等价于单层神经网络（不包含输入层）。</p><p>右下：如何初始化参数 w、b 的值？</p><p>当将所有参数初始化为零的时候，会使所有的节点变得相同，在训练过程中只能学到相同的特征，而无法学到多层级、多样化的特征。解决办法是随机初始化所有参数，但仅需少量的方差就行，因此使用 Rand（0.01）进行初始化，其中 0.01 也是超参数之一。</p><h1 id="4-深度神经网络的特点"><a href="#4-深度神经网络的特点" class="headerlink" title="4. 深度神经网络的特点"></a>4. 深度神经网络的特点</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-4-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-4-1024"></p><p>左上：神经网络的参数化容量随层数增加而指数式地增长，即某些深度神经网络能解决的问题，浅层神经网络需要相对的指数量级的计算才能解决。</p><p>左下：CNN 的深度网络可以将底层的简单特征逐层组合成越来越复杂的特征，深度越大，其能分类的图像的复杂度和多样性就越大。RNN 的深度网络也是同样的道理，可以将语音分解为音素，再逐渐组合成字母、单词、句子，执行复杂的语音到文本任务。</p><p>右边：深度网络的特点是需要大量的训练数据和计算资源，其中涉及大量的矩阵运算，可以在 GPU 上并行执行，还包含了大量的超参数，例如学习率、迭代次数、隐藏层数、激活函数选择、学习率调整方案、批尺寸大小、正则化方法等。</p><h1 id="5-偏差与方差"><a href="#5-偏差与方差" class="headerlink" title="5. 偏差与方差"></a>5. 偏差与方差</h1><p>那么部署你的机器学习模型需要注意些什么？下图展示了构建 ML 应用所需要的数据集分割、偏差与方差等问题。<br><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-5-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-5-1024"></p><p>如上所示，经典机器学习和深度学习模型所需要的样本数有非常大的差别，深度学习的样本数是经典 ML 的成千上万倍。因此训练集、开发集和测试集的分配也有很大的区别，当然我们假设这些不同的数据集都服从同分布。</p><p>偏差与方差问题同样是机器学习模型中常见的挑战，上图依次展示了由高偏差带来的欠拟合和由高方差带来的过拟合。一般而言，解决高偏差的问题是选择更复杂的网络或不同的神经网络架构，而解决高方差的问题可以添加正则化、减少模型冗余或使用更多的数据进行训练。</p><p>当然，机器学习模型需要注意的问题远不止这些，但在配置我们的 ML 应用中，它们是最基础和最重要的部分。其它如数据预处理、数据归一化、超参数的选择等都在后面的信息图中有所体现。</p><h1 id="6-正则化"><a href="#6-正则化" class="headerlink" title="6. 正则化"></a>6. 正则化</h1><p>正则化是解决高方差或模型过拟合的主要手段，过去数年，研究者提出和开发了多种适合机器学习算法的正则化方法，如数据增强、L2 正则化（权重衰减）、L1 正则化、Dropout、Drop Connect、随机池化和提前终止等。<br><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-6-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-6-1024"></p><p>如上图左列所示，L1 和 L2 正则化也是是机器学习中使用最广泛的正则化方法。L1 正则化向目标函数添加正则化项，以减少参数的绝对值总和；而 L2 正则化中，添加正则化项的目的在于减少参数平方的总和。根据之前的研究，L1 正则化中的很多参数向量是稀疏向量，因为很多模型导致参数趋近于 0，因此它常用于特征选择设置中。此外，参数范数惩罚 L2 正则化能让深度学习算法「感知」到具有较高方差的输入 x，因此与输出目标的协方差较小（相对增加方差）的特征权重将会收缩。</p><p>在中间列中，上图展示了 Dropout 技术，即暂时丢弃一部分神经元及其连接的方法。随机丢弃神经元可以防止过拟合，同时指数级、高效地连接不同网络架构。一般使用了 Dropout 技术的神经网络会设定一个保留率 p，然后每一个神经元在一个批量的训练中以概率 1-p 随机选择是否去掉。在最后进行推断时所有神经元都需要保留，因而有更高的准确度。</p><p>Bagging 是通过结合多个模型降低泛化误差的技术，主要的做法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。而 Dropout 可以被认为是集成了大量深层神经网络的 Bagging 方法，因此它提供了一种廉价的 Bagging 集成近似方法，能够训练和评估值数据数量的神经网络。</p><p>最后，上图还描述了数据增强与提前终止等正则化方法。数据增强通过向训练数据添加转换或扰动来人工增加训练数据集。数据增强技术如水平或垂直翻转图像、裁剪、色彩变换、扩展和旋转通常应用在视觉表象和图像分类中。而提前终止通常用于防止训练中过度表达的模型泛化性能差。如果迭代次数太少，算法容易欠拟合（方差较小，偏差较大），而迭代次数太多，算法容易过拟合（方差较大，偏差较小）。因此，提前终止通过确定迭代次数解决这个问题。</p><h1 id="7-最优化"><a href="#7-最优化" class="headerlink" title="7. 最优化"></a>7. 最优化</h1><p>最优化是机器学习模型中非常非常重要的模块，它不仅主导了整个训练过程，同时还决定了最后模型性能的好坏和收敛需要的时长。以下两张信息图都展示了最优化方法需要关注的知识点，包括最优化的预备和具体的最优化方法。<br><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-7-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-7-1024"></p><p>以上展示了最优化常常出现的问题和所需要的操作。首先在执行最优化前，我们需要归一化输入数据，而且开发集与测试集归一化的常数（均值与方差）与训练集是相同的。上图也展示了归一化的原因，因为如果特征之间的量级相差太大，那么损失函数的表面就是一张狭长的椭圆形，而梯度下降或最速下降法会因为「锯齿」现象而很难收敛，因此归一化为圆形有助于减少下降方向的震荡。</p><p>后面的梯度消失与梯度爆炸问题也是十分常见的现象。「梯度消失」指的是随着网络深度增加，参数的梯度范数指数式减小的现象。梯度很小，意味着参数的变化很缓慢，从而使得学习过程停滞。梯度爆炸指神经网络训练过程中大的误差梯度不断累积，导致模型权重出现很大的更新，在极端情况下，权重的值变得非常大以至于出现 NaN 值。</p><p>梯度检验现在可能用的比较少，因为我们在 TensorFlow 或其它框架上执行最优化算法只需要调用优化器就行。梯度检验一般是使用数值的方法计算近似的导数并传播，因此它能检验我们基于解析式算出来的梯度是否正确。</p><p>下面就是具体的最优化算法了，包括最基本的小批量随机梯度下降、带动量的随机梯度下降和 RMSProp 等适应性学习率算法。<br><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-8-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-8-1024"></p><p>小批量随机梯度下降（通常 SGD 指的就是这种）使用一个批量的数据更新参数，因此大大降低了一次迭代所需的计算量。这种方法降低了更新参数的方差，使得收敛过程更为稳定；它也能利用流行深度学习框架中高度优化的矩阵运算器，从而高效地求出每个小批数据的梯度。通常一个小批数据含有的样本数量在 50 至 256 之间，但对于不同的用途也会有所变化。</p><p>动量策略旨在加速 SGD 的学习过程，特别是在具有较高曲率的情况下。一般而言，动量算法利用先前梯度的指数衰减滑动平均值在该方向上进行修正，从而更好地利用历史梯度的信息。该算法引入了变量 v 作为参数在参数空间中持续移动的速度向量，速度一般可以设置为负梯度的指数衰减滑动平均值。</p><p>上图后面所述的 RMSProp 和 Adam 等适应性学习率算法是目前我们最常用的最优化方法。RMSProp 算法（Hinton，2012）修改 AdaGrad 以在非凸情况下表现更好，它改变梯度累积为指数加权的移动平均值，从而丢弃距离较远的历史梯度信息。RMSProp 是 Hinton 在公开课上提出的最优化算法，其实它可以视为 AdaDelta 的特例。但实践证明 RMSProp 有非常好的性能，它目前在深度学习中有非常广泛的应用。</p><p>Adam 算法同时获得了 AdaGrad 和 RMSProp 算法的优点。Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。</p><h1 id="8-超参数"><a href="#8-超参数" class="headerlink" title="8. 超参数"></a>8. 超参数</h1><p>以下是介绍超参数的信息图，它在神经网络中占据了重要的作用，因为它们可以直接提升模型的性能。<br><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-9-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-9-1024"></p><p>众所周知学习率、神经网络隐藏单元数、批量大小、层级数和正则化系数等超参数可以直接影响模型的性能，而怎么调就显得非常重要。目前最常见的还是手动调参，开发者会根据自身建模经验选择「合理」的超参数，然后再根据模型性能做一些小的调整。而自动化调参如随机过程或贝叶斯优化等仍需要非常大的计算量，且效率比较低。不过近来关于使用强化学习、遗传算法和神经网络等方法搜索超参数有很大的进步，研究者都在寻找一种高效而准确的方法。</p><p>目前的超参数搜索方法有：</p><p>依靠经验：聆听自己的直觉，设置感觉上应该对的参数然后看看它是否工作，不断尝试直到累趴。<br>网格搜索：让计算机尝试一些在一定范围内均匀分布的数值。<br>随机搜索：让计算机尝试一些随机值，看看它们是否好用。<br>贝叶斯优化：使用类似 MATLAB bayesopt 的工具自动选取最佳参数——结果发现贝叶斯优化的超参数比你自己的机器学习算法还要多，累觉不爱，回到依靠经验和网格搜索方法上去。</p><p>因为篇幅有限，后面的展示将只简要介绍信息图，相信它们对各位读者都十分有帮助。</p><h1 id="9-结构化机器学习过程"><a href="#9-结构化机器学习过程" class="headerlink" title="9. 结构化机器学习过程"></a>9. 结构化机器学习过程</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-10-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-10-1024"></p><p>我们需要按过程或结构来设定我们的机器学习系统，首先需要设定模型要达到的目标，例如它的预期性能是多少、度量方法是什么等。然后分割训练、开发和测试集，并预期可能到达的优化水平。随后再构建模型并训练，在开发集和测试集完成验证后就可以用于推断了。</p><h1 id="10-误差分析"><a href="#10-误差分析" class="headerlink" title="10. 误差分析"></a>10. 误差分析</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-11-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-11-1024"></p><p>在完成训练后，我们可以分析误差的来源而改进性能，包括发现错误的标注、不正确的损失函数等。</p><h1 id="11-训练集、开发集与测试集"><a href="#11-训练集、开发集与测试集" class="headerlink" title="11. 训练集、开发集与测试集"></a>11. 训练集、开发集与测试集</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-12-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-12-1024"></p><p>上图展示了三个分割数据集及其表现所需要注意的地方，也就是说如果它们间有不同的正确率，那么我们该如何修正这些「差别」。例如训练集的正确率明显高于验证集与测试集表明模型过拟合，三个数据集的正确率都明显低于可接受水平可能是因为欠拟合。</p><h1 id="12-其它学习方法"><a href="#12-其它学习方法" class="headerlink" title="12. 其它学习方法"></a>12. 其它学习方法</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-13-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-13-1024"></p><p>机器学习和深度学习当然不止监督学习方法，还有如迁移学习、多任务学习和端到端的学习等。</p><p>卷积网络</p><h1 id="13-卷积神经网络基础"><a href="#13-卷积神经网络基础" class="headerlink" title="13. 卷积神经网络基础"></a>13. 卷积神经网络基础</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-14-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-14-1024"></p><p>计算机视觉任务涉及的数据体量是特别大的，一张图像就有上千个数据点，更别提高分辨率图像和视频了。这时用全连接网络的话，参数数量太大，因而改用卷积神经网络（CNN），参数数量可以极大地减小。CNN 的工作原理就像用检测特定特征的过滤器扫描整张图像，进行特征提取，并逐层组合成越来越复杂的特征。这种「扫描」的工作方式使其有很好的参数共享特性，从而能检测不同位置的相同目标（平移对称）。</p><p>卷积核对应的检测特征可以从其参数分布简单地判断，例如，权重从左到右变小的卷积核可以检测到黑白竖条纹的边界，并显示为中间亮，两边暗的特征图，具体的相对亮暗结果取决于图像像素分布和卷积核的相对关系。卷积核权重可以直接硬编码，但为了让相同的架构适应不同的任务，通过训练得到卷积核权重是更好的办法。</p><p>卷积运算的主要参数：</p><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-15-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-15-1024"></p><p>padding：直接的卷积运算会使得到的特征图越来越小，padding 操作会在图像周围添加 0 像素值的边缘，使卷积后得到的特征图大小和原图像（长宽，不包括通道数）相同。</p><p>常用的两个选项是：『VALID』，不执行 padding；『SAME』，使输出特征图的长宽和原图像相同。</p><p>stride：两次卷积操作之间的步长大小。</p><p>一个卷积层上可以有多个卷积核，每个卷积核运算得到的结果是一个通道，每个通道的特征图的长宽相同，可以堆叠起来构成多通道特征图，作为下一个卷积层的输入。</p><p>深度卷积神经网络的架构：</p><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-16-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-16-1024"></p><p>深度卷积神经网络的架构主要以卷积层、池化层的多级堆叠，最后是全连接层执行分类。池化层的主要作用是减少特征图尺寸，进而减少参数数量，加速运算，使其目标检测表现更加鲁棒。</p><h1 id="14-经典卷积神经网络"><a href="#14-经典卷积神经网络" class="headerlink" title="14. 经典卷积神经网络"></a>14. 经典卷积神经网络</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-17-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-17-1024"></p><p>LeNet·5：手写识别分类网络，这是第一个卷积神经网络，由 Yann LeCun 提出。<br>AlexNet：图像分类网络，首次在 CNN 引入 ReLU 激活函数。<br>VGG-16：图像分类网络，深度较大。</p><h1 id="15-特殊卷积神经网络"><a href="#15-特殊卷积神经网络" class="headerlink" title="15. 特殊卷积神经网络"></a>15. 特殊卷积神经网络</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-18-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-18-1024"></p><p>ResNet：引入残差连接，缓解梯度消失和梯度爆炸问题，可以训练非常深的网络。<br>Network in Network：使用 1x1 卷积核，可以将卷积运算变成类似于全连接网络的形式，还可以减少特征图的通道数，从而减少参数数量。<br>Inception Network：使用了多种尺寸卷积核的并行操作，再堆叠成多个通道，可以捕捉多种规模的特征，但缺点是计算量太大，可以通过 1x1 卷积减少通道数。</p><h1 id="16-实践建议"><a href="#16-实践建议" class="headerlink" title="16. 实践建议"></a>16. 实践建议</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-19-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-19-1024"></p><p>使用开源实现：从零开始实现时非常困难的，利用别人的实现可以快速探索更复杂有趣的任务。<br>数据增强：通过对原图像进行镜像、随机裁剪、旋转、颜色变化等操作，增加训练数据量和多样性。<br>迁移学习：针对当前任务的训练数据太少时，可以将充分训练过的模型用少量数据微调获得足够好的性能。<br>基准测试和竞赛中表现良好的诀窍：使用模型集成，使用多模型输出的平均结果；在测试阶段，将图像裁剪成多个副本分别测试，并将测试结果取平均。</p><h1 id="17-目标检测算法"><a href="#17-目标检测算法" class="headerlink" title="17. 目标检测算法"></a>17. 目标检测算法</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-20-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-20-1024"></p><p>目标检测即使用边界框检测图像中物体的位置，Faster R-CNN、R-FCN 和 SSD 是三种目前最优且应用最广泛的目标检测模型，上图也展示了 YOLO 的基本过程。</p><h1 id="18-人脸识别"><a href="#18-人脸识别" class="headerlink" title="18. 人脸识别"></a>18. 人脸识别</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-21-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-21-1024"></p><p>人脸识别有两大类应用：人脸验证（二分分类）和人脸识别（多人分类）。</p><p>当样本量不足时，或者不断有新样本加入时，需要使用 one-shot learning，解决办法是学习相似性函数，即确定两张图像的相似性。比如在 Siamese Network 中学习人脸识别时，就是利用两个网络的输出，减少同一个人的两个输出的差别，增大不同人的两个输出之间的差别。</p><h1 id="19-风格迁移"><a href="#19-风格迁移" class="headerlink" title="19. 风格迁移"></a>19. 风格迁移</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-22-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-22-1024"></p><p>风格迁移是一个热门话题，它会在视觉上给人耳目一新的感觉。例如你有一副图，然后将另一幅图的风格特征应用到这幅图上，比如用一位著名画家或某一副名画的风格来修改你的图像，因此我们可以获得独特风格的作品。</p><p>循环网络</p><h1 id="20-循环神经网络基础"><a href="#20-循环神经网络基础" class="headerlink" title="20. 循环神经网络基础"></a>20. 循环神经网络基础</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-23-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-23-1024"></p><p>如上所示，命名实体识别等序列问题在现实生活中占了很大的比例，而隐马尔可夫链等传统机器学习算法只能作出很强的假设而处理部分序列问题。但近来循环神经网络在这些问题上有非常大的突破，RNN 隐藏状态的结构以循环形的形式成记忆，每一时刻的隐藏层的状态取决于它的过去状态，这种结构使得 RNN 可以保存、记住和处理长时期的过去复杂信号。</p><p>循环神经网络（RNN）能够从序列和时序数据中学习特征和长期依赖关系。RNN 具备非线性单元的堆叠，其中单元之间至少有一个连接形成有向循环。训练好的 RNN 可以建模任何动态系统；但是，训练 RNN 主要受到学习长期依赖性问题的影响。</p><p>以下展示了 RNN 的应用、问题以及变体等：</p><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-23-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-24-1024"></p><p>循环神经网络在语言建模等序列问题上有非常强大的力量，但同时它也存在很严重的梯度消失问题。因此像 LSTM 和 GRU 等基于门控的 RNN 有非常大的潜力，它们使用门控机制保留或遗忘前面时间步的信息，并形成记忆以提供给当前的计算过程。</p><h1 id="21-NLP-中的词表征"><a href="#21-NLP-中的词表征" class="headerlink" title="21. NLP 中的词表征"></a>21. NLP 中的词表征</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-25-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-25-1024"></p><p>词嵌入在自然语言处理中非常重要，因为不论执行怎样的任务，将词表征出来都是必须的。上图展示了词嵌入的方法，我们可以将词汇库映射到一个 200 或 300 维的向量，从而大大减少表征词的空间。此外，这种词表征的方法还能表示词的语义，因为词义相近的词在嵌入空间中距离相近。</p><p>除了以上所述的 Skip Grams，以下还展示了学习词嵌入的常见方法：</p><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-26-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-26-1024"></p><p>GloVe 词向量是很常见的词向量学习方法，它学到的词表征可进一步用于语句分类等任务。</p><h1 id="22-序列到序列"><a href="#22-序列到序列" class="headerlink" title="22. 序列到序列"></a>22. 序列到序列</h1><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-27-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-27-1024"></p><p>序列到序列的方法使用最多的就是编码器解码器框架，其它还有束搜索等模块的介绍。</p><p>编码器解码器架构加上注意力机制可以解决非常多的自然语言处理问题，以下介绍了 BLEU 分值和注意力机制。它们在机器翻译的架构和评估中都是不能缺少的部分。</p><p><img src="/img/notes-from-coursera-deep-learning-courses-by-andrew-ng-28-1024.jpg" alt="notes-from-coursera-deep-learning-courses-by-andrew-ng-28-1024"></p><p>以上是所有关于吴恩达深度学习专项课程的信息图，由于它们包含的信息较多，我们只介绍了一部分，还有很多内容只是简单的一笔带过。所以各位读者最好可以下载该信息图，并在后面的学习过程中慢慢理解与优化。</p><p>===<br>关注微信公众号【探物及理】回复“深度学习笔记”下载</p><p>===<br><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记5：无模型控制 Model-free control</title>
      <link href="/posts/rl-5/"/>
      <url>/posts/rl-5/</url>
      
        <content type="html"><![CDATA[<p>适用于：</p><ul><li>MDP model 未知：经验的采样可以获取</li><li>MDP model 已知：无法使用（e.g.原子级动力学），采样可以使用</li></ul><p>策略、非策略学习：</p><ul><li>On-policy：动作采样来自policy $\pi$</li><li>Off-policy：采样来自采样μ 或 来自于其他策略$\pi$，</li></ul><h1 id="On-policy-MC-control"><a href="#On-policy-MC-control" class="headerlink" title="On-policy MC control"></a>On-policy MC control</h1><p>贪婪策略梯度法如果用V(s)，需要MDP已知<br>对于已知MDP，可以通过策略迭代的方法，DP到最优策略<br><img src="/img/15972213696955.jpg" alt="-w536"></p><p>要实现不基于模型的控制，需要满足两个条件：</p><ul><li>引入q(s,a)函数，而不是v(s)</li><li>探索，避免局部最优，引入$\epsilon$，使$\pi$以小概率随机选择剩余动作，避免每次都选择已知较优动作</li></ul><h2 id="model-free-policy-using-action-value-function"><a href="#model-free-policy-using-action-value-function" class="headerlink" title="model-free policy using action-value function"></a>model-free policy using action-value function</h2><p>用Q（s，a），不需要已知MDP<br><img src="/img/15972213763038.jpg" alt="-w492"></p><p>每个箭头对应一个段，Prediction一次，Control一次<br><img src="/img/15972215371325.jpg" alt="-w343"></p><h2 id="GLIE-MC-control（Greedy-in-the-Limit-with-Infinite-Exploration）"><a href="#GLIE-MC-control（Greedy-in-the-Limit-with-Infinite-Exploration）" class="headerlink" title="GLIE MC control（Greedy in the Limit with Infinite Exploration）"></a>GLIE MC control（Greedy in the Limit with Infinite Exploration）</h2><p>保证试验进行一定次数是，所有a-s状态都被访问到很多次<br><img src="/img/15964296154206.jpg" alt="-w514"></p><p>随实验次数进行，减小$\epsilon$值<br><img src="/img/15964297028232.jpg" alt="-w505"></p><h1 id="ON-policy-TD-learning"><a href="#ON-policy-TD-learning" class="headerlink" title="ON-policy TD learning"></a>ON-policy TD learning</h1><ul><li>TD与MC control 区别，希望引入TD的特性到on-policy learning<br><img src="/img/15965079350024.jpg" alt="-w509"></li></ul><h2 id="Sasra"><a href="#Sasra" class="headerlink" title="Sasra"></a>Sasra</h2><h3 id="Sasra（one-step）"><a href="#Sasra（one-step）" class="headerlink" title="Sasra（one-step）"></a>Sasra（one-step）</h3><p>由贝尔曼公式推导<br><img src="/img/15964411373536.jpg" alt="-w451"></p><h3 id="算法实现过程"><a href="#算法实现过程" class="headerlink" title="算法实现过程"></a>算法实现过程</h3><p><img src="/img/15964401337385.jpg" alt="-w535"></p><p>要保证Q值收敛，需要服从下列2个条件</p><ul><li>策略符合GLIE特性</li><li>计算步长满足如图：<br><img src="/img/15964405409905.jpg" alt="-w443"></li></ul><h3 id="n-step-Sarsa"><a href="#n-step-Sarsa" class="headerlink" title="n-step Sarsa"></a>n-step Sarsa</h3><p>与TD（λ）类似，扩展q的视野</p><p><img src="/img/15964411018416.jpg" alt="-w576"></p><h2 id="Forward-view-Sarsa-λ"><a href="#Forward-view-Sarsa-λ" class="headerlink" title="Forward view Sarsa(λ)"></a>Forward view Sarsa(λ)</h2><p><img src="/img/15964499708276.jpg" alt="-w644"></p><h2 id="Backward-view-Sarsa-λ"><a href="#Backward-view-Sarsa-λ" class="headerlink" title="Backward view Sarsa(λ)"></a>Backward view Sarsa(λ)</h2><p>在正向视角中，迭代一次Q值，需要完整的一次episode<br>为了解决这个问题，引入迹的概念，实现incremental update<br><img src="/img/15964503883163.jpg" alt="-w621"></p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><img src="/img/15965030748824.jpg" alt="-w529"></p><p>Attention：迹E是属于episode的，切换episode后，E要归零</p><h1 id="Off-policy-learning"><a href="#Off-policy-learning" class="headerlink" title="Off-policy learning"></a>Off-policy learning</h1><ul><li><p>需求</p><ul><li>从人类和其他agents的表现中学习</li><li>从old policies $\pi_1, \pi_2…$中学习</li><li>从随机策略中，学习到最优策略</li><li>从一个策略中，学习到多个策略</li></ul></li><li><p>采样不同分布</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}_{X \sim P}[f(X)] &=\sum P(X) f(X) \\&=\sum Q(X) \frac{P(X)}{Q(X)} f(X) \\&=\mathbb{E}_{X \sim Q}\left[\frac{P(X)}{Q(X)} f(X)\right]\end{aligned}</script></li></ul><h2 id="off-policy-MC-learning"><a href="#off-policy-MC-learning" class="headerlink" title="off-policy MC learning"></a>off-policy MC learning</h2><p>引入了概率缩放系数，判断两个策略动作概率函数<br><img src="/img/15965086023837.jpg" alt="-w484"></p><ul><li>缺点：<ul><li>方差会增加</li><li>$\mu =0$无法计算</li></ul></li></ul><h2 id="off-policy-TD-learning"><a href="#off-policy-TD-learning" class="headerlink" title="off-policy TD learning"></a>off-policy TD learning</h2><p>利用期望分布的概念，在更新目标前x一个系数，对当前策略的置信度<br><img src="/img/15965088051921.jpg" alt="-w426"></p><ul><li>优点：<ul><li>低方差</li><li>单步策略需要相似</li></ul></li></ul><h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>采用Q(s,a) instead of V(s)</li><li>不需要重要性采样 系数</li><li>下次动作用 $A_{t+1} ∼ μ(·|S_t)$</li><li>动作服从策略 as $A′ ∼ π(·|S_t)$</li></ul><p>更新方程如下</p><script type="math/tex; mode=display">Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R_{t+1}+\gamma Q\left(S_{t+1}, A^{\prime}\right)-Q\left(S_{t}, A_{t}\right)\right)</script><h2 id="off-policy-control-with-Q-learning"><a href="#off-policy-control-with-Q-learning" class="headerlink" title="off-policy control with Q-learning"></a>off-policy control with Q-learning</h2><p>在学习过程中：</p><ul><li>Q值的Update Target 被优化，target policies是$\epsilon-greedy$的</li><li>Actor 执行的策略$\pi$被优化，执行$\epsilon-greedy$</li></ul><p>采用下式更新策略：</p><script type="math/tex; mode=display">\pi\left(S_{t+1}\right)=\underset{a^{\prime}}{\operatorname{argmax}} Q\left(S_{t+1}, a^{\prime}\right)</script><p>Q-learning target 简化为：</p><script type="math/tex; mode=display">\begin{aligned}& R_{t+1}+\gamma Q\left(S_{t+1}, A^{\prime}\right) \\=& R_{t+1}+\gamma Q\left(S_{t+1}, \underset{a^{\prime}}{\operatorname{argmax}} Q\left(S_{t+1}, a^{\prime}\right)\right) \\=& R_{t+1}+\max _{a^{\prime}} \gamma Q\left(S_{t+1}, a^{\prime}\right)\end{aligned}</script><p><img src="/img/15965124141748.jpg" alt="-w535"></p><p>迭代使$Q(s,a) \rightarrow q_* (s,a)$<br>Attention：在迭代过程中，动作采用$\epsilon-greedy$策略，保证对位置环境的探索</p><h3 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h3><p><img src="/img/15965124556612.jpg" alt="-w538"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="DP-TD的关系"><a href="#DP-TD的关系" class="headerlink" title="DP TD的关系"></a>DP TD的关系</h2><p><img src="/img/15965127185793.jpg" alt="-w619"><br><img src="/img/15965134695436.jpg" alt="-w633"></p><h2 id="Q-learning-和-SARSA区别"><a href="#Q-learning-和-SARSA区别" class="headerlink" title="Q-learning 和 SARSA区别"></a>Q-learning 和 SARSA区别</h2><p><img src="/img/15965139448881.jpg" alt="-w711"></p><p>区别在于：</p><ul><li><p>Q-learning：</p><ul><li>update： $greedy$策略，评估过程的A’没有实际执行</li><li>control：$\epsilon-greedy$策略</li></ul></li><li><p>SARSA：更新和执行都用$\epsilon-greedy$策略</p></li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> 无模型控制 model-free control </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-Coursera笔记</title>
      <link href="/posts/deep-learning/"/>
      <url>/posts/deep-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="AI-gt-机器学习分类图"><a href="#AI-gt-机器学习分类图" class="headerlink" title="AI->机器学习分类图"></a>AI-&gt;机器学习分类图</h1><p><img src="/img/15936818055208.jpg" alt="-w669"></p><h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><h2 id="几种网络结构分类"><a href="#几种网络结构分类" class="headerlink" title="几种网络结构分类"></a>几种网络结构分类</h2><p>NN——回归预测<br>CNN（convolution NN）卷积神经网络——图片<br>RNN (Recurrent Neural Network）递归神经网络——声音、语言处理<br>LSTM长短期记忆网络——</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>sigmoid<br>ReLU——rectified linear unit 修正线性单元</p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>Cost function 每个样本的误差均值<br>Loss function 单个样本的误差<br>贝叶斯误差<br>泛化——提取特征的能力？<br>归一化，不同变量分布尺度调整一致<br>正则化，减少过拟合<br>正交化，调整变量，不影响其他变量<br>迁移学习，把model从一个task1 应用到 task2<br>玻尔兹曼机-无监督学习</p><h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="连式法则"><a href="#连式法则" class="headerlink" title="连式法则"></a>连式法则</h2><p><img src="/img/15937407378448.jpg" alt="-w720"></p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><strong>General methodology</strong><br>As usual you will follow the Deep Learning methodology to build the model:</p><ol><li>Initialize parameters / Define hyperparameters</li><li>Loop for num_iterations:<br> a. Forward propagation<br> b. Compute cost function<br> c. Backward propagation<br> d. Update parameters (using parameters, and grads from backprop) </li><li>Use trained parameters to predict labels<br>Let’s now implement those two models!</li></ol><h2 id="2、加速训练的方法"><a href="#2、加速训练的方法" class="headerlink" title="2、加速训练的方法"></a>2、加速训练的方法</h2><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><ol><li>L2，二范数</li><li>L1，绝对值——容易造成<strong>稀疏化</strong><br><img src="/img/15938728967496.jpg" alt="-w594"></li><li>dropout随机失活 正则化<br>对于神经网络来说，用其中的一部分预测结果，等同于正则化的效果。不要让网络过分依赖某个神经元<br>一般在靠前的层较低的存活率<br>输入层和后面的层，存活率较高<br><img src="/img/15938730869088.jpg" alt="-w815"></li><li>数据扩充Data augmentation</li><li>Early stopping<br><img src="/img/15939156078394.jpg" alt="-w554"><br>发</li></ol><p>扩展到高维，同样的道理，L2 的限定区域是平滑的，与中心点等距；而 L1 的限定区域是包含凸点的，尖锐的。这些凸点更接近 Ein 的最优解位置，而在这些凸点上，很多 wj 为 0。</p><h3 id="归一化输入变量X"><a href="#归一化输入变量X" class="headerlink" title="归一化输入变量X"></a>归一化输入变量X</h3><h3 id="参数初始化——避免梯度爆炸-消失"><a href="#参数初始化——避免梯度爆炸-消失" class="headerlink" title="参数初始化——避免梯度爆炸/消失"></a>参数初始化——避免梯度爆炸/消失</h3><ul><li>随机初始化 打破 对称性</li><li>初始化值不要太小或者太大，否则</li></ul><h3 id="梯度检验-Gradient-checking，但是不能和随机失活一起使用"><a href="#梯度检验-Gradient-checking，但是不能和随机失活一起使用" class="headerlink" title="梯度检验 Gradient checking，但是不能和随机失活一起使用"></a>梯度检验 Gradient checking，但是不能和随机失活一起使用</h3><h2 id="3、寻优方法加速训练"><a href="#3、寻优方法加速训练" class="headerlink" title="3、寻优方法加速训练"></a>3、寻优方法加速训练</h2><h3 id="batch-Gradient-Descent"><a href="#batch-Gradient-Descent" class="headerlink" title="batch Gradient Descent"></a>batch Gradient Descent</h3><h3 id="mini-batch-Gradient-Descent"><a href="#mini-batch-Gradient-Descent" class="headerlink" title="mini-batch Gradient Descent"></a>mini-batch Gradient Descent</h3><h3 id="stochastic-SGD随机梯度下降法"><a href="#stochastic-SGD随机梯度下降法" class="headerlink" title="stochastic SGD随机梯度下降法"></a>stochastic SGD随机梯度下降法</h3><h3 id="Exponentially-weighted-averages-指数加权滑动平均"><a href="#Exponentially-weighted-averages-指数加权滑动平均" class="headerlink" title="Exponentially weighted averages 指数加权滑动平均"></a>Exponentially weighted averages 指数加权滑动平均</h3><p>类似于信号滤波，造成延迟</p><p><strong>bias correction</strong><br>初始值：$v_0$设置为0导致的<br>处理办法，✖️$\frac{1}{(1-\beta)^t}$</p><h3 id="动量法-GD-with-momentum"><a href="#动量法-GD-with-momentum" class="headerlink" title="动量法 GD with momentum"></a>动量法 GD with momentum</h3><p>与指数加权滑动平均类似<br>对梯度加权滤波<img src="/img/15939964494351.jpg" alt="-w621"></p><h3 id="Root-mean-square-prop均方根传递"><a href="#Root-mean-square-prop均方根传递" class="headerlink" title="Root mean square prop均方根传递"></a>Root mean square prop均方根传递</h3><p>压制导数过大的项，使各个特征值上的导数尽可能<br>$(dw)^2$是element operation<br><img src="/img/15939972113707.jpg" alt="-w534"></p><h3 id="Adam算法——Adaptive-moment-estimation自适应矩估计"><a href="#Adam算法——Adaptive-moment-estimation自适应矩估计" class="headerlink" title="Adam算法——Adaptive moment estimation自适应矩估计"></a>Adam算法——Adaptive moment estimation自适应矩估计</h3><p>将 带动量GD 和 均方根RMS-prop 算法 结合<br><img src="/img/15939976636434.jpg" alt="-w785"><br>$\alpha$<br>$\beta_1 = 0.9$<br>$\beta_2 = 0.999$<br>$\epsilon = 10^{-8}$</p><h3 id="learning-rate-decay"><a href="#learning-rate-decay" class="headerlink" title="learning rate decay"></a>learning rate decay</h3><p><img src="/img/15939980880189.jpg" alt="-w712"></p><h2 id="优化过程问题"><a href="#优化过程问题" class="headerlink" title="优化过程问题"></a>优化过程问题</h2><p><img src="/img/15939982916740.jpg" alt="-w775"></p><h2 id="4、超参数调参过程Tuning-process"><a href="#4、超参数调参过程Tuning-process" class="headerlink" title="4、超参数调参过程Tuning process"></a>4、超参数调参过程Tuning process</h2><ol><li>try random values: don’t use grid</li><li>区域定位搜索过程，粗搜索，确定密度高的地方</li><li>搜索尺度，Log分布，对指数的幂平均</li><li><strong>BN归一化</strong>Normalizing activations in a network<br>通过γ和β，任意改变Z值的分布<br>原理：减少隐藏层 变量值分布的不确定性<br><img src="/img/15940226217605.jpg" alt="-w274"></li></ol><p>Predict时，用训练集得到的参数，进行同样的缩放</p><h2 id="多class回归分类"><a href="#多class回归分类" class="headerlink" title="多class回归分类"></a>多class回归分类</h2><h3 id="softmax-regression"><a href="#softmax-regression" class="headerlink" title="softmax regression"></a>softmax regression</h3><p>将线性变量的概率，用e幂增大分辨率，归一化到0-1<br>激活函数为<br>$func(Z) = np.exp(Z) \sum^{n^{l}} e^{Z_i}$</p><h3 id="Hardmax-regression"><a href="#Hardmax-regression" class="headerlink" title="Hardmax regression"></a>Hardmax regression</h3><p>将变量归一化到[1 0 0 0]</p><h1 id="Structuring-ML-project"><a href="#Structuring-ML-project" class="headerlink" title="Structuring ML project"></a>Structuring ML project</h1><h2 id="正交化Orthogonalization"><a href="#正交化Orthogonalization" class="headerlink" title="正交化Orthogonalization"></a>正交化Orthogonalization</h2><p>针对某个问题，作出调整，不改变其他特性<br>Training -&gt; Dev -&gt; Test -&gt; Real world</p><h2 id="评价准则"><a href="#评价准则" class="headerlink" title="评价准则"></a>评价准则</h2><h3 id="单一评价指标"><a href="#单一评价指标" class="headerlink" title="单一评价指标"></a>单一评价指标</h3><p>例如分类器存在多个评价指标<br>Precision精度 &amp; Recall查准率，<br>$F sore = \frac{PR}{P+R}$<br>多分类器，一般用均值</p><h3 id="优化和满意度矩阵Satisficing-and-Optimizing-metric"><a href="#优化和满意度矩阵Satisficing-and-Optimizing-metric" class="headerlink" title="优化和满意度矩阵Satisficing and Optimizing metric"></a>优化和满意度矩阵Satisficing and Optimizing metric</h3><p>精度、运行时间</p><h3 id="ML的上限"><a href="#ML的上限" class="headerlink" title="ML的上限"></a>ML的上限</h3><p>理论值：贝叶斯最优误差<br>人类performance距离上限不远，一旦ML表现超过人类，人类很难根据偏差和方差，指导算法提高。</p><h3 id="避免-偏差-和方差"><a href="#避免-偏差-和方差" class="headerlink" title="避免 偏差 和方差"></a>避免 偏差 和方差</h3><p><img src="/img/15941050602099.jpg" alt="-w1011"></p><h2 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h2><p>在错误集合中找到主要影响因素，对训练集做适应性改造</p><ul><li>大小</li><li>其他种类</li><li>清晰度</li><li>滤镜</li></ul><h2 id="数据分布改变（Train、D、T）"><a href="#数据分布改变（Train、D、T）" class="headerlink" title="数据分布改变（Train、D、T）"></a>数据分布改变（Train、D、T）</h2><h3 id="错误标签solution"><a href="#错误标签solution" class="headerlink" title="错误标签solution"></a>错误标签solution</h3><p>Robust，如果error比较大，则主要成分不是少量的错误标签</p><h3 id="新标签"><a href="#新标签" class="headerlink" title="新标签"></a>新标签</h3><p>训练集保留原数据<br>dev 和 test 集 去除原标签，得到新数据的精度<br><img src="/img/15941131936082.jpg" alt="-w663"></p><h3 id="不同分布下的变差和方差"><a href="#不同分布下的变差和方差" class="headerlink" title="不同分布下的变差和方差"></a>不同分布下的变差和方差</h3><p>添加新样本后，D、T分布改变，其误差已经无法反应变差和方差<br>从训练集T中，选出一小部分，作为Train-Dev集，验证训练，计算偏差和方差<br><img src="/img/15941140515319.jpg" alt="-w819"></p><p>如果误差在D、T集下降了，说明测试集较为简单<br>横坐标：原集合、新集合<br>纵坐标：人performance、新训练model、原训练model（训练集未加入新样本）<br><img src="/img/15941143755868.jpg" alt="-w1161"></p><h3 id="解决数据分布不匹配办法"><a href="#解决数据分布不匹配办法" class="headerlink" title="解决数据分布不匹配办法"></a>解决数据分布不匹配办法</h3><ul><li>获取数据</li><li>人工生成数据（参与合成成分数量级与被合成的一致、避免过度拟合）</li></ul><h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>在相似的任务重，将Task1训练好的模型Model1，稍作修改生成用于Task2的Model2，让新任务的模型参考之前模型已经学习到的经验。</p><p>预训练（pre-training）for Model1<br>细微训练（fine-tunning）for Model2</p><p>根据数据量，决定需要训练的层数<br>数据量较小，只训练Model1的末层</p><h4 id="限制条件"><a href="#限制条件" class="headerlink" title="限制条件"></a>限制条件</h4><p><img src="/img/15941159051232.jpg" alt="-w629"></p><h2 id="多任务学习Multi-task-learning"><a href="#多任务学习Multi-task-learning" class="headerlink" title="多任务学习Multi-task learning"></a>多任务学习Multi-task learning</h2><p>相比于多类别分类器，y向量不一定只有一个1，存在多个1</p><h3 id="限制条件-1"><a href="#限制条件-1" class="headerlink" title="限制条件"></a>限制条件</h3><p>任务之间的相似性<br><img src="/img/15941165734379.jpg" alt="-w569"></p><p>规模不够大时，多任务学习比单项学习  损害准确率</p><h2 id="端到端学习End-to-end"><a href="#端到端学习End-to-end" class="headerlink" title="端到端学习End-to-end"></a>端到端学习End-to-end</h2><p>Start -&gt; End，复杂任务不需要中间的各个模块<br>缺陷是数据量需求巨大</p><ul><li>传统方法，分模块，串联执行，完成任务</li></ul><p><img src="/img/15941175308325.jpg" alt="-w542"></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 DL </tag>
            
            <tag> 机器学习 ML </tag>
            
            <tag> Coursera </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>控制理论笔记-2</title>
      <link href="/posts/advance-control-2/"/>
      <url>/posts/advance-control-2/</url>
      
        <content type="html"><![CDATA[<h1 id="高级控制理论"><a href="#高级控制理论" class="headerlink" title="高级控制理论"></a>高级控制理论</h1><p><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-1.jpg" alt="Advanced控制理论-1"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-3.jpg" alt="Advanced控制理论-3"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-4.jpg" alt="Advanced控制理论-4"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-5.jpg" alt="Advanced控制理论-5"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-6.jpg" alt="Advanced控制理论-6"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-7.jpg" alt="Advanced控制理论-7"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-8.jpg" alt="Advanced控制理论-8"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-9.jpg" alt="Advanced控制理论-9"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-10.jpg" alt="Advanced控制理论-10"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-11.jpg" alt="Advanced控制理论-11"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-12.jpg" alt="Advanced控制理论-12"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-13.jpg" alt="Advanced控制理论-13"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-14.jpg" alt="Advanced控制理论-14"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-15.jpg" alt="Advanced控制理论-15"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-16.jpg" alt="Advanced控制理论-16"><br><img src="/img/Advanced%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA-17.jpg" alt="Advanced控制理论-17"></p><h2 id="Dr-can-ARC-步骤"><a href="#Dr-can-ARC-步骤" class="headerlink" title="Dr_can   ARC 步骤"></a>Dr_can   ARC 步骤</h2><p><img src="/img/DR_can_NARC%E8%87%AA%E9%80%82%E5%BA%94%E5%8F%8D%E6%AD%A5%E6%8E%A7%E5%88%B6%E6%AD%A5%E9%AA%A4.jpg" alt="DR_can_NARC自适应反步控制步骤"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 控制理论笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级控制理论 </tag>
            
            <tag> 经典控制理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络CNN（convolutional）</title>
      <link href="/posts/cnn/"/>
      <url>/posts/cnn/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络CNN（convolutional）"><a href="#卷积神经网络CNN（convolutional）" class="headerlink" title="卷积神经网络CNN（convolutional）"></a>卷积神经网络CNN（convolutional）</h1><p>卷积运算：原图像*卷积核=新图像，经常用来做边缘检测<br>人造核：手动指定权重，改善效果<br><img src="/img/15941711725552.jpg" alt="-w615"></p><p>指定核权重为变量，通过反向传播，学习卷积核的权重<br>补白和步幅决定了卷积后的</p><h2 id="补白Padding"><a href="#补白Padding" class="headerlink" title="补白Padding"></a>补白Padding</h2><ul><li>Valid convolution：p = 0<br>$n\times n * f\times f -&gt; (n-f+1)\times (n-f+1)$</li><li>Same convolution：n = n<br>$(n+2p)\times (n+2p) * f\times f -&gt; n\times n$<br>得到填充边缘宽度$p = \frac{f-1}{2}$<br>所以一般卷积核大小是奇数<h2 id="步幅strides"><a href="#步幅strides" class="headerlink" title="步幅strides"></a>步幅strides</h2>s&gt;1，图像也变小<br><img src="/img/15941720564251.jpg" alt="-w600"></li></ul><h2 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h2><p>对于RGB三通道图像，nc个滤波器，卷积叠加，得到深度为nc的图像<br><img src="/img/15941727037025.jpg" alt="-w710"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/img/15941735838131.jpg" alt="-w697"></p><h3 id="趋势：缩减图片尺度，增加深度"><a href="#趋势：缩减图片尺度，增加深度" class="headerlink" title="趋势：缩减图片尺度，增加深度"></a>趋势：缩减图片尺度，增加深度</h3><p><img src="/img/15941743777673.jpg" alt="-w693"></p><h2 id="CNN分类"><a href="#CNN分类" class="headerlink" title="CNN分类"></a>CNN分类</h2><h3 id="卷积层Conv："><a href="#卷积层Conv：" class="headerlink" title="卷积层Conv："></a>卷积层Conv：</h3><p><img src="/img/15941767758613.jpg" alt="-w695"></p><h3 id="池化层Pool：减少图片宽度，用卷积核进行特征提取"><a href="#池化层Pool：减少图片宽度，用卷积核进行特征提取" class="headerlink" title="池化层Pool：减少图片宽度，用卷积核进行特征提取"></a>池化层Pool：减少图片宽度，用卷积核进行特征提取</h3><p>欠<strong>采样</strong>（下采样），特征降维，压缩数据和参数，减小过拟合<br>只有超参数，没有参数<br>主要分类：</p><ul><li>最大池化</li><li>平均池化</li></ul><p><img src="/img/15941793237584.jpg" alt="-w730"></p><h3 id="全连接层Fc：一般用来输出"><a href="#全连接层Fc：一般用来输出" class="headerlink" title="全连接层Fc：一般用来输出"></a>全连接层Fc：一般用来输出</h3><p><img src="/img/15941799685961.jpg" alt="-w743"><br><img src="/img/15941799984582.jpg" alt="-w732"><br><img src="/img/15941803310656.jpg" alt="-w661"></p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul><li>优势</li></ul><ol><li>参数共享：将卷积核的参数共享给每组被卷积对象运算</li><li>稀疏性联系：输出的值只与小部分输入相关<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3>CNN从前到后，维度缩减，参数增多</li></ol><h2 id="CNN案例"><a href="#CNN案例" class="headerlink" title="CNN案例"></a>CNN案例</h2><h3 id="经典CNN"><a href="#经典CNN" class="headerlink" title="经典CNN"></a>经典CNN</h3><ul><li><p>LeNet-5（sigmoid激活，softmax分类）<br><img src="/img/15942602914708.jpg" alt="-w767"></p></li><li><p>AlexNet</p></li></ul><p><img src="/img/15942603354885.jpg" alt="-w772"></p><ul><li>VGGNet<br><img src="/img/15942604999932.jpg" alt="-w775"></li></ul><h3 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h3><p>传统的plain network存在梯度指数现象<br><img src="/img/15942710882961.png" alt="-w596"></p><p>为了改善深度网络的梯度爆炸（消失）现象，使深度神经网训练可能</p><ul><li>Residual block<br><img src="/img/15942613601777.jpg" alt="-w777"></li></ul><p><img src="/img/15942613429827.jpg" alt="-w770"><br>维数不一致的问题，可以通过构建权重矩阵，填充0元素或者其他方法进行适配</p><p><img src="/img/15942620573952.jpg" alt="-w723"></p><h2 id="1x1-convolution"><a href="#1x1-convolution" class="headerlink" title="1x1 convolution"></a>1x1 convolution</h2><p>对image每个像素进行非线性函数映射，通过n个kernel，映射为n个特征，用于<strong>缩减图像特征深度</strong><br><img src="/img/15942632478404.jpg" alt="-w662"></p><h3 id="用法，生成中间量，减少运算量"><a href="#用法，生成中间量，减少运算量" class="headerlink" title="用法，生成中间量，减少运算量"></a>用法，生成中间量，减少运算量</h3><p>直接5x5卷积<br><img src="/img/15942634806609.jpg" alt="-w576"><br>采用1x1卷积中间量，再用5x5卷积<br><img src="/img/15942635070290.jpg" alt="-w685"></p><h2 id="Inception-network"><a href="#Inception-network" class="headerlink" title="Inception network"></a>Inception network</h2><h3 id="Main-idea"><a href="#Main-idea" class="headerlink" title="Main idea"></a>Main idea</h3><p><img src="/img/15942632298297.jpg" alt="-w667"></p><h3 id="Inception-module"><a href="#Inception-module" class="headerlink" title="Inception module"></a>Inception module</h3><p><img src="/img/15942639201675.jpg" alt="-w678"></p><h3 id="Inception-network-1"><a href="#Inception-network-1" class="headerlink" title="Inception network"></a>Inception network</h3><ul><li>Inception module 的串联</li><li>branches用于在中间预测结果，效果不差<br><img src="/img/15942640307997.jpg" alt="-w678"></li></ul><h2 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h2><p>步骤</p><ol><li>下载源码，模型，权重参数</li><li>修改末层结构，softmax分类或者</li><li>冻结前层参数</li><li>训练自己模型</li></ol><h2 id="Data-Augmentation数据增强"><a href="#Data-Augmentation数据增强" class="headerlink" title="Data Augmentation数据增强"></a>Data Augmentation数据增强</h2><p>可以预先处理，<br>也可以与训练并行处理</p><h3 id="1、形状"><a href="#1、形状" class="headerlink" title="1、形状"></a>1、形状</h3><ul><li>镜像Mirroring</li><li>随机裁剪Random Cropping</li><li>旋转Rotation</li><li>倾斜Shearing</li><li>扭曲Local warping</li></ul><h3 id="2、色彩Color-shifting"><a href="#2、色彩Color-shifting" class="headerlink" title="2、色彩Color shifting"></a>2、色彩Color shifting</h3><ul><li>增减RGB通道值，改变量随机</li><li>PCA，干扰主要元素</li></ul><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>CV（computer vision）中，目标检测是并列与图像分类的一个重要应用<br>分类 -&gt; 分类+定位 = 目标检测 -&gt; 多目标检测<br><img src="/img/15942815091519.jpg" alt="-w788"></p><ul><li>输出和cost function构造<br>输出$Y = [p_c,b_x,b_y,b_h,b_w,c_1,c_2……]$<br>包含<strong>有没有</strong>物体、<strong>坐标</strong>和<strong>物体类别</strong></li><li>cost function<br>当$p_c$=1，计算均方根误差<br>$p_c$= 0,其他的不计算</li></ul><h2 id="Landmark-Detection"><a href="#Landmark-Detection" class="headerlink" title="Landmark Detection"></a>Landmark Detection</h2><p>人脸特征标记<br>姿态特征标记<br>特征构建Y = $[是不是脸,b_x{1},b_y{1},……,b_x{n},b_{yn}]$</p><h2 id="目标检测几种思路"><a href="#目标检测几种思路" class="headerlink" title="目标检测几种思路"></a>目标检测几种思路</h2><h3 id="Sliding-windows-detection"><a href="#Sliding-windows-detection" class="headerlink" title="Sliding windows detection"></a>Sliding windows detection</h3><ol><li>滑窗获取图像</li><li>传给Classification</li><li>增大windows size 和 stride 重复1</li></ol><h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ul><li>小步长，计算量高</li><li>大步长，计算粗略，精度差<h3 id="用卷积实现Turning-FC-layer-into-convolutional-layers"><a href="#用卷积实现Turning-FC-layer-into-convolutional-layers" class="headerlink" title="用卷积实现Turning FC layer into convolutional layers"></a>用卷积实现Turning FC layer into convolutional layers</h3>用1x1卷积实现权重线性组合<br><img src="/img/15942833041786.jpg" alt="-w670"></li></ul><p><img src="/img/15942837667651.jpg" alt="-w669"></p><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>定位不精确</p><h2 id="输出更精确的边界框——YOLO"><a href="#输出更精确的边界框——YOLO" class="headerlink" title="输出更精确的边界框——YOLO"></a>输出更精确的边界框——YOLO</h2><p>YOLO算法  you only look once<br>构造输出Y.shape = (h,w,l)<br>h是高度图像分割的个数<br>w是宽度图像分割的个数<br>l方向是物体定位信息$[p_c,b_x,b_y,b_h,b_w,c_1,c_2……]$<br><img src="/img/15942851221558.jpg" alt="-w668"></p><h2 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h2><ol><li>低于阈值，直接丢弃</li><li>找出最大值</li><li>与最大值重叠区域IoU&gt;0.5，丢弃。多目标检测的话，运行多次IoU算法</li></ol><p><img src="/img/15942863921030.jpg" alt="-w671"></p><h2 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h2><p>重叠物体检测，重建Y<br>矮胖，瘦长<br><img src="/img/15942868946435.jpg" alt="-w673"></p><h1 id="人脸识别——单样本学习"><a href="#人脸识别——单样本学习" class="headerlink" title="人脸识别——单样本学习"></a>人脸识别——单样本学习</h1><ul><li><p>Verification验证<br>input： 图片 姓名 140311199402071213<br>output：是否对应</p></li><li><p>Recognition识别<br>n个人的数据库<br>input：图片<br>output：140311199402071213 of person（如果图片在库里）</p></li></ul><p>相似度函数$d(img1,img2)$，函数和照片没关系</p><h2 id="孪生网络Siamese-network"><a href="#孪生网络Siamese-network" class="headerlink" title="孪生网络Siamese network"></a>孪生网络Siamese network</h2><p>核心思想：NN生成编码，比较不同样本编码的范数<br>CNN的参数相同，<br><img src="/img/15942958881883.jpg" alt="-w663"></p><h3 id="三重损耗函数-Triple-loss"><a href="#三重损耗函数-Triple-loss" class="headerlink" title="三重损耗函数 Triple loss"></a>三重损耗函数 Triple loss</h3><p>防止NN输出退化解，布置小于0<br>$\alpha$ is margin<br><img src="/img/15942962863559.jpg" alt="-w662"></p><h3 id="another相似性函数"><a href="#another相似性函数" class="headerlink" title="another相似性函数"></a>another相似性函数</h3><p><img src="/img/15943010618021.jpg" alt="-w413"></p><p><img src="/img/15943009025922.jpg" alt="-w672"></p><h1 id="风格转换Styler-transfer"><a href="#风格转换Styler-transfer" class="headerlink" title="风格转换Styler transfer"></a>风格转换Styler transfer</h1><p><img src="/img/15943016778419.jpg" alt="-w670"></p><h2 id="可视化hidden-unit"><a href="#可视化hidden-unit" class="headerlink" title="可视化hidden unit"></a>可视化hidden unit</h2><p>浅层学习基本特征<br>深层学习宏观特征<br><img src="/img/15943015979608.jpg" alt="-w667"></p><h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><p>Main idea: 和两张图片都很像<br>$J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G)$</p><p><img src="/img/15943018797577.jpg" alt="-w682"></p><h3 id="content-cost-function"><a href="#content-cost-function" class="headerlink" title="content cost function"></a>content cost function</h3><p><img src="/img/15943020713953.jpg" alt="-w698"></p><h3 id="Style-cost-function"><a href="#Style-cost-function" class="headerlink" title="Style cost function"></a>Style cost function</h3><p><img src="/img/15943023722812.jpg" alt="-w603"><br>$\lambda$ 是 l 层的权重<br><img src="/img/15943032075944.jpg" alt="-w468"></p><h2 id="卷积1D和3D形式"><a href="#卷积1D和3D形式" class="headerlink" title="卷积1D和3D形式"></a>卷积1D和3D形式</h2><h3 id="1D形式"><a href="#1D形式" class="headerlink" title="1D形式"></a>1D形式</h3><p><img src="/img/15943038817548.jpg" alt="-w707"></p><h3 id="3D形式"><a href="#3D形式" class="headerlink" title="3D形式"></a>3D形式</h3><p><img src="/img/15943041391653.jpg" alt="-w695"></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> CNN </tag>
            
            <tag> 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记4：无模型预测 model-free prediction</title>
      <link href="/posts/rl-4/"/>
      <url>/posts/rl-4/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>这一章，解决的是用prediction的方法，来评估策略$\pi$的问题。</p><p>对于Env来说，不是参数已知的MDP<br>比如元组中a、s、P的关系不确定 or 未知</p><p>Prediction -&gt; Control<br>Evaluation -&gt; Optimization</p><h1 id="蒙特卡洛法-Monte-Carlo-learning"><a href="#蒙特卡洛法-Monte-Carlo-learning" class="headerlink" title="蒙特卡洛法 Monte-Carlo learning"></a>蒙特卡洛法 Monte-Carlo learning</h1><ul><li>定义：在不清楚MDP状态转移及即时奖励的情况下，直接从经历完整的Episode来学习状态价值，通常情况下某状态的价值等于在多个Episode中以该状态算得到的所有收获的平均。</li></ul><p>适用于MDP参数<strong>未知</strong>，回合制更新，遍历了所有状态s</p><p>MC是基于大数定律的：<br>当采样足够多时，就可以代表真值</p><script type="math/tex; mode=display">N(s)->\infty \Rightarrow V(s) -> V_\pi(s)</script><ul><li>说明：<ul><li>均值累计计算可以用$v = sum/N$</li><li>也可以用累进更新 Incremental Mean<script type="math/tex; mode=display">\begin{aligned}\mu_{k} &=\frac{1}{k} \sum_{j=1}^{k} x_{j} \\&=\frac{1}{k}\left(x_{k}+\sum_{j=1}^{k-1} x_{j}\right) \\&=\frac{1}{k}\left(x_{k}+(k-1) \mu_{k-1}\right) \\&=\mu_{k-1}+\frac{1}{k}\left(x_{k}-\mu_{k-1}\right)\end{aligned}</script></li></ul></li></ul><p>类似于PID的P 增益随着N增大，在逐步缩小<br>为了简化计算，改写方程，用$\alpha$代替$\frac{1}{N(s_t)}$<br>这样可以用固定步长来替代变步长$\frac{1}{k}$</p><script type="math/tex; mode=display">V(S_t) \leftarrow V(S_t)+\alpha(G_t - V(S_t))</script><p>可以看做是，$v_{new} = v_{old} + \alpha(v_{target} - v_{old})$，括号里面是误差<br>可以看到这里的$\alpha$和机器学习里面用的学习率是一个符号</p><h1 id="差分法Temporal-Difference-learning"><a href="#差分法Temporal-Difference-learning" class="headerlink" title="差分法Temporal-Difference learning"></a>差分法Temporal-Difference learning</h1><p>MC 在 episode遍历完之后，回合更新，效率低<br>TD 实现边走边更新</p><p>引入时间t的概念</p><ul><li>直接从episodes 的经验学习</li><li>model-free：<strong>不知道</strong>MDP的Transition转移和Reward回报</li><li>Bootstrapping自举学习，从部分例子学习</li></ul><p>Goal：学习$v_{\pi}$ 的值，under policy $\pi$</p><h2 id="时序分析法-TD-0）："><a href="#时序分析法-TD-0）：" class="headerlink" title="时序分析法 TD(0）："></a>时序分析法 TD(0）：</h2><script type="math/tex; mode=display">V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right)</script><ul><li>TD target 是 下一个时刻的$R_{t+1}+\gamma V\left(S_{t+1}\right)$</li><li>TD误差：$\left(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right)$</li></ul><p>Bootstrapping：根据episode表现来更新V值，自举（依靠自己努力获得）</p><h2 id="与MC方法区别"><a href="#与MC方法区别" class="headerlink" title="与MC方法区别"></a>与MC方法区别</h2><div class="table-container"><table><thead><tr><th>项目</th><th>MC</th><th>TD</th></tr></thead><tbody><tr><td>不完整片段学习能力</td><td>无</td><td>有</td></tr><tr><td>在线学习(every step)能力</td><td>update until the end</td><td>有</td></tr><tr><td>loop环境学习能力</td><td>无，必须terminating</td><td>有</td></tr><tr><td>收敛性好</td><td>是</td><td></td></tr><tr><td>初值敏感</td><td>否</td><td>是</td></tr><tr><td>偏差bias</td><td>zero</td><td>some</td></tr><tr><td>方差variance</td><td>high</td><td>low</td></tr></tbody></table></div><h2 id="估计方法背后的理论"><a href="#估计方法背后的理论" class="headerlink" title="估计方法背后的理论"></a>估计方法背后的理论</h2><ul><li>MC方法：最小化均方根MSE<script type="math/tex; mode=display">\sum_{k=1}^{K} \sum_{t=1}^{T_{k}}\left(G_{t}^{k}-V\left(s_{t}^{k}\right)\right)^{2}</script></li><li>TD（0）方法：<strong>最大似然估计</strong> max likelihood Markov model<br>Solution to the MDP $\langle\mathcal{S}, \mathcal{A}, \hat{\mathcal{P}}, \hat{\mathcal{R}}, \gamma\rangle$ that best fits the data<script type="math/tex; mode=display">\hat{\mathcal{P}}_{s, s^{\prime}}^{a} =\frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{t=1}^{T_{k}} 1\left(s_{t}^{k}, a_{t}^{k}, s_{t+1}^{k}=s, a, s^{\prime}\right)</script><script type="math/tex; mode=display">\hat{\mathcal{R}}_{s}^{a} =\frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{t=1}^{T_{k}} 1\left(s_{t}^{k}, a_{t}^{k}=s, a\right) r_{t}^{k}</script></li></ul><p><img src="/img/15972187186144.jpg" alt="MC-TD收敛速度对比"></p><h2 id="总结：DP、MC、TD"><a href="#总结：DP、MC、TD" class="headerlink" title="总结：DP、MC、TD"></a>总结：DP、MC、TD</h2><ul><li>Bootstrapping自举：利用自己估计值update</li><li>Sampling采样 ：更新样本期望</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">项目</th><th style="text-align:center">动态规划DP</th><th style="text-align:center">蒙特卡洛MC</th><th style="text-align:center">差分TD</th></tr></thead><tbody><tr><td style="text-align:center">自举Bootstrapping</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">采样Sampling</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr></tbody></table></div><ul><li>TD用了Markov特性，因此在MP过程高效</li><li>MC相反，统计规律，非MP过程同样有效</li></ul><p>MC: 采样，一次完整经历，用实际收获更新状态预估价值</p><p><img src="/img/15972190302462.png" alt="MC-深度"></p><p>TD：采样，经历可不完整，用喜爱状态的预估状态价值预估收获再更新预估价值<br><img src="/img/15972190329229.png" alt="TD-窄而浅"></p><p>DP：没有采样，根据完整模型，依靠预估数据更新状态价值<br><img src="/img/15972190846702.png" alt="DP-宽度"></p><p><img src="/img/15961728958158.jpg" alt="全尺度搜索、动态规划、MC、TD 对比"></p><h1 id="TD-λ-法"><a href="#TD-λ-法" class="headerlink" title="TD(λ)法"></a>TD(λ)法</h1><p>视野（深度）影响TD算法的稳定性，但是视野去多深，不知道<br>因此，综合不同深度的视野，加权求和，即TD$(\lambda)$</p><p>扩展TD(0)，视野扩展到N个step，N=全过程时，变为MC<br><img src="/img/15961732715990.jpg" alt=""></p><p>TD（N）推导<br><img src="/img/15961732821137.jpg" alt="TD（N）"><br><img src="/img/15972192509726.jpg" alt="不同深度TD效果对比"></p><p>对于某个问题来说，没有那个N值是最优的<br>因此，用几何加权的方法来对视野做平均</p><h2 id="Forward-前向视角认知-TD-lambda"><a href="#Forward-前向视角认知-TD-lambda" class="headerlink" title="Forward 前向视角认知 $TD(\lambda)$"></a>Forward 前向视角认知 $TD(\lambda)$</h2><ul><li><p>例子：<br>老鼠在连续接受了3次响铃和1次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？<br><img src="/img/15972196164084.png" alt=""></p></li><li><p>两个启发：</p><ul><li>出现频率高的状态</li><li>出现频率低的状态</li></ul></li></ul><p><img src="/img/15962140900575.jpg" alt="-w561"></p><p>$\lambda$：对视野的平均<br>for iteration： t -&gt; t+1<br>update value function</p><p><img src="/img/15961782979683.jpg" alt="-w621"></p><p>引入权重概念，前面的重要，指数衰减<br><img src="/img/15961783140794.jpg" alt="-w533"></p><h2 id="Backward-反向认知TD-λ-：提供了单步更新的机制"><a href="#Backward-反向认知TD-λ-：提供了单步更新的机制" class="headerlink" title="Backward 反向认知TD(λ)：提供了单步更新的机制"></a>Backward 反向认知TD(λ)：提供了<strong>单步</strong>更新的机制</h2><p>Credit assignment：<br>引入 Eligibility Traces：状态s的权重，是一个时间序列<br>当s重复出现，E值升高，不出现，指数下降<br>$E_{0}(s)=0$<br>$E_{t}(s)=\gamma \lambda E_{t-1}(s)+\mathbf{1}\left(S_{t}=s\right)$</p><p>Backward步骤：</p><ul><li>对每个状态s 创建 迹值</li><li>对每个状态s 更新 V(s)</li><li>与 TD-error($\delta_t$) 和 Eligibility trace $E_t(s)$ 成比例</li></ul><script type="math/tex; mode=display">\begin{aligned}\delta_{t} &=R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\\V(s) & \leftarrow V(s)+\alpha \delta_{t} E_{t}(s)\end{aligned}</script><p><img src="/img/15962140598528.jpg" alt="-w426"></p><script type="math/tex; mode=display">\sum_{t=1}^{T} \alpha \delta_{t} E_{t}(s)=\sum_{t=1}^{T} \alpha\left(G_{t}^{\lambda}-V\left(S_{t}\right)\right) 1\left(S_{t}=s\right)</script><p><img src="/img/15964228971321.jpg" alt="-w605"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/img/15964233494644.jpg" alt="-w627"></p><ul><li>Offline update：TD(0) = TD($\lambda$) = TD(1)</li><li>Online update： TD($\lambda$)前后向视图不一致，引入Exact online TD($\lambda$)可以解决这个问题</li><li>TD(0) 向后看一步</li><li>TD($\lambda$) 视野距离按$\lambda$指数衰减，叠加</li><li>TD(1) 视野不按指数衰减</li><li>能在RL中被应用，看中了TD的自举特性</li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> 无模型预测 model-free prediction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记3：动态规划 planning by dynamic programming（DP）</title>
      <link href="/posts/rl-3/"/>
      <url>/posts/rl-3/</url>
      
        <content type="html"><![CDATA[<p>规划，适用于MDP模型参数<strong>已知</strong><br>学习，适用于Env未知或部分未知</p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>动态规划分为两步，Prediction、Control</p><ul><li>（Prediction）Value:是对策略$\pi$的评价<script type="math/tex; mode=display"><s,P^\pi,R^\pi,\gamma>, \pi \rightarrow V_\pi</script></li><li>（Control）Policy $\pi$:是对Value的选择<script type="math/tex; mode=display"><s,P^\pi,R^\pi,\gamma>, V \rightarrow \pi</script></li></ul><h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><ul><li>prediction：迭代法<br>对所有状态s，应用贝尔曼公式<script type="math/tex; mode=display">v_{k+1}(s)=\sum_{a \in A} \pi(a \mid s)\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right)</script></li></ul><p>右边的$v_k$不确定，但是迭代下去，因为$R^a_s$确定，所以收敛到真值$v_\pi$</p><ul><li>Control：greedly，每次都选基于上次预测最好的那个a</li></ul><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>问题：每走一步，r = -1，走到出口可以停止<br>在随机策略下，迭代k，最使v收敛<br>得到$v^{\pi}(s)$<br><img src="/img/15959276754125.jpg" alt="-w395"><br>然后最简单的策略，greedy，往v值高的地方走。<br><img src="/img/15959300701562.jpg" alt="-w562"></p><h1 id="Policy-iteration：-O-mn-2"><a href="#Policy-iteration：-O-mn-2" class="headerlink" title="Policy iteration：$O(mn^2)$"></a>Policy iteration：$O(mn^2)$</h1><p>每一步，遍历动作A</p><p>Find optim policy：<br>以Greedy为例，迭代：$\pi’ = greedy(v_{\pi})$<br>策略更新为：</p><script type="math/tex; mode=display">\pi^{\prime}(s)=\underset{a \in \mathcal{A}}{\operatorname{argmax}} q_{\pi}(s, a)</script><p>值函数更新为：</p><script type="math/tex; mode=display">v_{\pi}(s)=\max _{a \in \mathcal{A}} q_{\pi}(s, a)</script><p><strong>主动</strong>改变策略，策略改变之后进行评估<br>根据q值，从集合A中选a，更新策略$\pi$，使新q大于之前一步</p><script type="math/tex; mode=display">q_{\pi}\left(s, \pi^{\prime}(s)\right)=\max _{a \in \mathcal{A}} q_{\pi}(s, a) \geq q_{\pi}(s, \pi(s))=v_{\pi}(s)</script><p>证明：</p><script type="math/tex; mode=display">\begin{aligned}v_{\pi}(s) & \leq q_{\pi}\left(s, \pi^{\prime}(s)\right)=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right] \\& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, \pi^{\prime}\left(S_{t+1}\right)\right) \mid S_{t}=s\right] \\& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} q_{\pi}\left(S_{t+2}, \pi^{\prime}\left(S_{t+2}\right)\right) \mid S_{t}=s\right] \\& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\ldots \mid S_{t}=s\right]=v_{\pi^{\prime}}(s)\end{aligned}</script><p>所以保证了每次迭代，价值函数 与 策略 增长</p><h1 id="Value-iteration：-O-m-2n-2"><a href="#Value-iteration：-O-m-2n-2" class="headerlink" title="Value iteration：$O(m^2n^2)$"></a>Value iteration：$O(m^2n^2)$</h1><p>每一步，遍历了状态S和动作A</p><p>最优策略：</p><ul><li>当前状态s下，所有动作a产生回报的最大值</li><li>采取动作a后，状态由s -&gt; s’的期望<br>公式：<script type="math/tex; mode=display">v_{*}(s)=\max _{a} \mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)</script></li></ul><p>根据值函数，选择策略</p><h1 id="值迭代和policy迭代的区别"><a href="#值迭代和policy迭代的区别" class="headerlink" title="值迭代和policy迭代的区别"></a>值迭代和policy迭代的区别</h1><ul><li>policy iteration每次迭代v(s)都会变大；而value iteration则不是。</li><li>价值迭代不需要策略参与，依据MDP 模型，直接迭代，需要P矩阵、r 等<strong>已知</strong><ul><li>policy iteration： policy-&gt;value-&gt;policy</li><li>value iteration：value-&gt;value</li></ul></li></ul><h1 id="Trick"><a href="#Trick" class="headerlink" title="Trick:"></a>Trick:</h1><p>三种值迭代方法:<br>常规的值迭代，要遍历过所有s之后，才进行一次迭代，因此存在old、new两个v(s)</p><ul><li>in-place DP：新值直接替换旧值，只存储一个v(s)，<ul><li>异步更新，提高效率</li><li>缺点：更新顺序影响收敛性</li></ul></li><li>Prioritised sweeping：state的影响力排序<ul><li>比较贝尔曼误差绝对值，大的更新，小的忽略</li></ul></li><li>Real-time DP：遍历过的才更新<ul><li>省去了agent 未遍历的状态s，对于稀疏任务效率提升极大</li></ul></li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> 动态规划 dynamic programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MBSE 基于模型的系统工程</title>
      <link href="/posts/mbse/"/>
      <url>/posts/mbse/</url>
      
        <content type="html"><![CDATA[<h1 id="MBSE"><a href="#MBSE" class="headerlink" title="MBSE"></a>MBSE</h1><p>根据国际系统工程协会（INCOSE）在 2007 年发布的《SE 愿景 2020》中的定义，MBSE 是建模方法在系统工程中的形式化应用，用以支持在系统全生命周期内开展需求、设计、分析、验证和确认相关的活动。从定义可以看到，MBSE 是基于文档的传统系统工程工作模式的演进，力求以多视角的系统模型做为桥梁，将跨学科/领域的模型关联起来，实现跨学科/领域的模型追溯，从而驱动大型复杂系统生存周期内各阶段的工程活动，最终实现以模型驱动的方法来采集、捕获和提炼数据、信息和知识。<br>《INCOSE 系统工程手册》、《NASA 系统工程手册》、《FAA 系统工程手册》以及《中国商用飞机有限责任公司系统工程手册》中对系统工程实践有完善的描述，如果需要深入了解系统工程相关概念和具体实践，请参阅这些手册。<br>MBSE 是采用模型驱动的方式对系统工程的实践，本文就从系统工程要做的几个典型任务入手，介绍 MBSE 都做什么，帮助大家理解MBSE的内涵，并进一步开展 MBSE 的实践。</p><h2 id="系统工程的主要活动包括："><a href="#系统工程的主要活动包括：" class="headerlink" title="系统工程的主要活动包括："></a>系统工程的主要活动包括：</h2><ul><li><p>任务/目标定义</p></li><li><p>需求工程</p></li><li><p>系统架构</p></li><li><p>系统集成</p></li><li><p>验证与确认</p></li><li><p>技术分析</p></li><li><p>范围管理</p></li></ul><p>技术领导力和技术管理</p><p>下面就来看看每一项活动的具体内容。为了清晰展示各项活动的关联关系，下图展示了各项活动在我们熟知 V 流程中的位置。</p><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15942030645422.jpg?x-oss-process=style/blog" alt=""></p><p>◆  ◆  ◆  ◆</p><h2 id="任务-目标定义"><a href="#任务-目标定义" class="headerlink" title="任务/目标定义"></a>任务/目标定义</h2><p>创造新系统或对现有系统进行修改，都是从任务/目标定义开始，也就是捕获涉众需求。任务或目标定义并不聚焦在特定的待开发系统上，而是在一个更大的背景中识别潜在的需求，这种需求将成为开发某个系统的理由。这种需求通常用系统使用者的语言进行描述，而不是技术语言，很多时候也会包括一定程度的定量描述。一般通过 CONOPS（Concept of Operations）文件来描述待开发系统的任务和目标。</p><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p>系统工程中的架构设计主要是指定义其组成和各部分的关系，通常以示意图（Diagrams）的形式体现.包括在一定环境下的系统高层概念描述、系统的组成部分、系统各部分的交互、系统与环境的交互等。同时，系统架构设计也要描述系统产生的过程和对各备选方案的评估过程，并基于系统需求来定义低层需求，从而将需求分配到各组成部分。<br>工程实践过程中，我们常常会将待设计的系统或产品按照物理形式进行分解，称为产品分解结构 PBS（Product Breakdown Structure），同时也会根据任务/目标定义的内容以及需求工程中获取的功能要求进行逻辑分解，形成功能分解结构 FBS（Function Breakdown Structure），简单直观的可将 PBS 与 FBS 之间的映射关系称为系统架构。</p><h2 id="需求工程"><a href="#需求工程" class="headerlink" title="需求工程"></a>需求工程</h2><p>即需求生成和管理，通过正式的技术语言对系统的功能、属性以及质量因素等进行阐述，通常叫做“需求管理”或“需求工程”，包括对需求的定义、分析、确认和管理。需求工程的输入是任务/目标定义的输出，通过分析、综合、验证的迭代过程将涉众需求通过技术语言进行形式化表示，形成高层需求和底层需求，是需求工程的主要任务。</p><h2 id="系统集成"><a href="#系统集成" class="headerlink" title="系统集成"></a>系统集成</h2><p>系统集成包含设计、购买和创造各类系统组件，对系统组件进行测试，而系统组件包括硬件、软件或过程。这些组件由于庞大和复杂，而通常也被当作系统看待。系统集成主要完成构建符合预期系统框架要求的系统组件，这部分任务通常由一系列的组装和测试操作组成。</p><h2 id="确认与验证"><a href="#确认与验证" class="headerlink" title="确认与验证"></a>确认与验证</h2><p>确认，是对开发完成的系统（或像需求和架构这样的开发成果）与系统的预期任务或目标进行比对，即要确定“做正确的事”，提供需求一致性和完整性的证明；验证，是通过检查，分析，展示，测试或其他的客观证据，来对系统（开发成果）和需求进行比对，以此来展示“正确的做事”。确认和验证是整个系统生命周期过程中需要持续进行的系统工程活动，分析、测试、评审是进行验证的常用方法，追溯、分析、建模、测试、相似性度量（或经验）和评审是进行确认的常用方法。</p><h2 id="技术分析"><a href="#技术分析" class="headerlink" title="技术分析"></a>技术分析</h2><p>这里的技术分析是指系统级技术分析，尤其是对系统性能满足需求的满足度进行评估；包括性能分析，时序分析，容量分析，质量分析，趋势，敏感度，失效模式和影响性分析等，主要涉及技术性能度量，以及其他类似的对系统配置和组件的多学科评价；除非是与需求工程或系统架构设计不可分割，可能还需要进行功能分析，预测分析，权衡分析等。<br>这些技术分析一般都是基于不同的系统层级，使用不同的定量建模技术构建不同逼真度的分析模型进行计算和仿真而开展的，是为技术理解和决策提供严格的数据和信息基础的必要手段。</p><h2 id="范围管理"><a href="#范围管理" class="headerlink" title="范围管理"></a>范围管理</h2><p>采购和供应链问题的技术定义和管理，主要关注与上下游的合同关系：向上是开发合同，对整个系统开发的范围进行定义，主要涉及系统需求，向下是对委托出去开发的系统组件的范围进行定义。</p><h2 id="技术领导力和技术管理"><a href="#技术领导力和技术管理" class="headerlink" title="技术领导力和技术管理"></a>技术领导力和技术管理</h2><p>技术领导力和技术管理活动主要包括项目策划，技术过程评估，技术控制，团队建设，交叉协同，提供通用语言和目标，风险管理和接口管理等；与项目管理不同的是，技术领导力和技术管理主要关注技术目标和技术指导。</p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 系统工程 </tag>
            
            <tag> MBSE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记2：马尔科夫决策过程Markov decision process(MDP)</title>
      <link href="/posts/rl-2/"/>
      <url>/posts/rl-2/</url>
      
        <content type="html"><![CDATA[<h1 id="马尔科夫过程（Markov-Process，MP）"><a href="#马尔科夫过程（Markov-Process，MP）" class="headerlink" title="马尔科夫过程（Markov Process，MP）"></a>马尔科夫过程（Markov Process，MP）</h1><p>我们说一个state若满足 ，则其具有马尔可夫性，即该state完全包含了历史中的所有信息。马尔科夫过程是无记忆的随机过程，即随机状态序列 具有马尔可夫属性。</p><p>一个马尔科夫过程可以由一个元组组成$\langle\mathcal{S}, \mathcal{P}\rangle$</p><p>$\mathcal{S}$为（有限）的状态（state）集；<br>$\mathcal{P}$为状态转移矩阵， $$<br>P_{s s^{\prime}}=\mathbb{P}\left(S_{t+1}=s^{\prime} \mid S_{t}=s\right)</p><script type="math/tex; mode=display">。所谓状态转移矩阵就是描述了一个状态到另一个状态发生的概率，所以**_矩阵每一行元素之和为1_**。## 马尔科夫奖励过程（Markov Reward Process，MRP）在MP上加入了 奖励Reward 和 折扣系数$\gamma$![-w460](/img/15961148196502.jpg)对于状态转移概率矩阵确定的情况，Value值可以显式计算得到![-w238](/img/15961148266893.jpg)对于MDP，并不适用，因为$\mathbb{P}$**非线性**### 解析解对于理想的的MDP，且参数已知，其解析解可以直接逆运算得到状态在策略下的value值，可以由一下的公式计算得到</script><p>\begin{array}{c}<br>v_{\pi}=\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} v_{\pi} \\<br>v_{\pi}=\left(I-\gamma \mathcal{P}^{\pi}\right)^{-1} \mathcal{R}^{\pi}<br>\end{array}</p><script type="math/tex; mode=display">## 马尔科夫决策过程（Markov Decision Process，MDP）MDP相对于MP加入了瞬时奖励 $R$(Immediate reward）、动作集合$A$和折扣因子 $\gamma$ （Discount factor），这里的瞬时奖励说的是从一个状态 s到下一个状态 s' 即可获得的rewards，虽然是“奖励”，但如果这个状态的变化对实现目标不利，就是一个负值，变成了“惩罚”，所以reward就是我们告诉agent什么是我们想要得到的，但不是我们如何去得到。MDP由元组 $\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma\rangle$ 定义。其中$\mathcal{S}$为（有限）的状态（state）集；$\mathcal{A}$为有限的动作集；$\mathcal{P}$为状态转移矩阵。所谓状态转移矩阵就是描述了一个状态到另一个状态发生的概率，所以矩阵每一行元素之和为1。[公式]$\mathcal{R}$为回报函数（reward function), [公式]$\gamma$为折扣因子，范围在[0,1]之间， 越大，说明agent看得越“远”。对于每一个$\pi$,$a\in A$</script><p>\begin{aligned}<br>\mathcal{P}_{s, s^{\prime}}^{\pi} &amp;=\sum_{a \in \mathcal{A}} \pi(a \mid s) \mathcal{P}_{s s^{\prime}}^{a} \\<br>\mathcal{R}_{s}^{\pi} &amp;=\sum_{a \in \mathcal{A}} \pi(a \mid s) \mathcal{R}_{s}^{a}<br>\end{aligned}</p><script type="math/tex; mode=display">### 收获 Return定义：收获$G_t$为在一个马尔科夫奖励链上从t时刻开始往后所有的奖励的有衰减的总和。也有翻译成“收益”或"回报"。G值，从t时刻起，包括了**未来**，计算了**折扣**的总奖励：</script><p>G_t = R_{t+1}+\gamma R_{t+2} + … = \sum^\infty_{k=0}\gamma^k R_{t+k+1}</p><script type="math/tex; mode=display">- 考虑了未来的不确定性- 给与较近的未来更高权重### 价值函数和动作值函数![-w397](/img/15959184331156.jpg)- 价值函数：在状态s，策略π下的值函数</script><p>v_{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]</p><script type="math/tex; mode=display">- 动作值函数（action-value function）：在状态s，执行动作a，遵循策略π，回报期望</script><p>q_{\pi}(s, a)=\mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=t, A_{t}=a\right]</p><script type="math/tex; mode=display"># 贝尔曼方程状态值函数可以分解为即刻回报+未来回报x折扣值分解 -> 迭代实现</script><p>v_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right]</p><script type="math/tex; mode=display">动作值函数类似：</script><p>q_{\pi}(s, a)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) \mid S_{t}=s, A_{t}=a\right]</p><script type="math/tex; mode=display">在每个状态，会存在多个备选动作；每个动作，也可能会导致不一样的状态，因此存在下图。![](/img/15958154132177.jpg)</script><p>v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s) q_{\pi}(s, a) \\ q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right) \\ </p><script type="math/tex; mode=display">于是$$v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s)\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right) \\ q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \sum_{a^{\prime} \in \mathcal{A}} \pi\left(a^{\prime} \mid s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)</script><p>描述了当前状态值函数和其后续状态值函数之间的关系，即状态值函数（动作值函数）等于瞬时回报的期望加上下一状态的（折扣）状态值函数（动作值函数）的期望。</p><h2 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h2><p>学习的目的是优化一个策略π使得值函数v or q最大</p><script type="math/tex; mode=display">v_{*}(s)=\max _{\pi} v_{\pi}(s)</script><script type="math/tex; mode=display"> q_{*}(s, a)=\max _{\pi} q_{\pi}(s, a)</script><p> 对于任意一个MDPs，存在一个$\pi_<em>$使得$ v_{\pi_{</em>}}(s)=v_{<em>}(s), \quad q_{\pi_{</em>}}(s, a)=q_{*}(s, a) $<br>可得，贝尔曼最优方程：</p><script type="math/tex; mode=display">v_{*}(s)=\max _{a} \mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)</script><script type="math/tex; mode=display">q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)</script><h3 id="求解最优方程方法"><a href="#求解最优方程方法" class="headerlink" title="求解最优方程方法"></a>求解最优方程方法</h3><ul><li>Value iteration</li><li>Policy iteration</li><li>Q-learning</li><li>Sarsa</li><li>等</li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 RL </tag>
            
            <tag> 马尔科夫决策过程 MDP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记1：基本概念</title>
      <link href="/posts/rl-1/"/>
      <url>/posts/rl-1/</url>
      
        <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>强化学习是一门多学科交叉的技术<br><img src="/img/%E6%88%AA%E5%B1%8F2020-07-27%20%E4%B8%8B%E5%8D%885.35.46-1.png" alt="截屏2020-07-27 下午5.35.46"><br><img src="/img/15973090963916.jpg" alt=""></p><h2 id="与传统控制的关系："><a href="#与传统控制的关系：" class="headerlink" title="与传统控制的关系："></a>与传统控制的关系：</h2><ul><li>相似性：</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">RL</th><th style="text-align:center">traditional control</th></tr></thead><tbody><tr><td style="text-align:center">agent</td><td style="text-align:center">controller</td></tr><tr><td style="text-align:center">env</td><td style="text-align:center">plant + enviroment</td></tr><tr><td style="text-align:center">reward</td><td style="text-align:center">feedback（error signals）</td></tr><tr><td style="text-align:center">value</td><td style="text-align:center">optimize function</td></tr></tbody></table></div><ul><li>不同点：<ul><li>传统的控制：将任务分解成多个任务的串并联，设计（子）控制器</li><li>机器学习：将控制器压缩成黑盒Black box<br><img src="/img/15953218263369.jpg" alt="-w1354"></li></ul></li></ul><p>强化学习不同于 监督、非监督学习（与静态数据交互），与环境产生交互，产生最优结果的动作序列。</p><h1 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h1><p>RL组成要素Agent、Env<br><img src="/img/15972068404024.png" alt=""></p><h2 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h2><p>组成要素：Policy、Value function、Model其中至少一个<br><img src="/img/15958481757170.jpg" alt="-w410"></p><h3 id="策略-Policy-：observation-to-action的映射"><a href="#策略-Policy-：observation-to-action的映射" class="headerlink" title="策略(Policy)：observation to action的映射"></a>策略(Policy)：observation to action的映射</h3><ul><li>固定策略：$a = \pi(s)$给定状态$s$，对应fixed的action $a$</li><li>随机策略：当Agent处于某一个state的时候，它做的Action是不确定的，例如你可以选择study也可以选择game，也就是说你在某一个状态是以一定的概率去选择某一个action。也就是说，策略的选择是一个条件概率$\pi(a|s)$，这里的$\pi$与数序中的$\pi$没有任何关系，他只是代表一个函数而已（其实就是$f(a|s)$）。<script type="math/tex; mode=display">\pi(a \mid s)=P\left(A_{t}=a \mid S_{t}=s\right)</script>函数代表：在状态$s$时采取动作$a$的概率分布。</li></ul><p><img src="/img/%E6%88%AA%E5%B1%8F2020-07-27%20%E4%B8%8B%E5%8D%887.07.57-1.png" alt="截屏2020-07-27 下午7.07.57"></p><h3 id="价值-value-function-：未来奖励的预测（期望）"><a href="#价值-value-function-：未来奖励的预测（期望）" class="headerlink" title="价值(value function)：未来奖励的预测（期望）"></a>价值(value function)：未来奖励的预测（期望）</h3><p>前面我们说到过奖励，当Agent在$t$时刻执行某个动作时，会得到一个$\mathcal{R_{t+1}}$。我们可以想一下蝴蝶效应，这个Action会影响$\mathcal{R_{t+1}}$，那么他会不会影响$\mathcal{R_{t+2}},\mathcal{R_{t+3}}…$呢？很可能会的，比如说在电游中，你所做的某个选择肯定会对接下来的游戏产生影响，这个影响可以深远，也可以没那么深渊（对，我说的就是隐形守护者，mmp），因此状态价值函数可以表示为：</p><script type="math/tex; mode=display">v_{\pi}(s)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right)</script><p>$v_{\pi}(s)$与策略函数$\pi$有关，可以理解为当Agent以策略$\pi$运行时，状态$s$的价值是多少。也就是在此状态下，我能够得到多少回报。<br><img src="/img/15958481422216.png" alt="-w425"></p><h2 id="模型Model："><a href="#模型Model：" class="headerlink" title="模型Model："></a>模型Model：</h2><p>对未知env的预测，包括状态s、奖励r</p><h3 id="预测状态：s-gt-s’的概率"><a href="#预测状态：s-gt-s’的概率" class="headerlink" title="预测状态：s->s’的概率"></a>预测状态：s-&gt;s’的概率</h3><script type="math/tex; mode=display">\mathcal{P}_{s s^{\prime}}^{a} =\mathbb{P}  \left(S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right)</script><h3 id="预测奖励：R的期望"><a href="#预测奖励：R的期望" class="headerlink" title="预测奖励：R的期望"></a>预测奖励：R的期望</h3><script type="math/tex; mode=display">\mathcal{R}_{s}^{a} =\mathbb{E} R_{t+1} \left( \mid S_{t}=s, A_{t}=a\right)</script><h2 id="环境Env"><a href="#环境Env" class="headerlink" title="环境Env"></a>环境Env</h2><h3 id="完全可观测环境"><a href="#完全可观测环境" class="headerlink" title="完全可观测环境"></a>完全可观测环境</h3><p>个体观测=个体状态=环境状态<br>标准的<strong>MDP </strong></p><h3 id="部分可观测环境"><a href="#部分可观测环境" class="headerlink" title="部分可观测环境"></a>部分可观测环境</h3><p>环境不完全可观测</p><h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><ul><li>Beliefs of environment state：用个体的经验，记录历史状态并创建概率分布函数<script type="math/tex; mode=display">S_{t}^{a}=\left(\mathbb{P}\left[S_{t}^{e}=s^{1}\right], \ldots, \mathbb{P}\left[S_{t}^{e}=s^{n}\right]\right)</script></li><li>Recurrent neural network：用循环神经网络，表示为，上个状态和当前观测的函数<script type="math/tex; mode=display">S_{t}^{a}=\sigma\left(S_{t-1}^{a} W_{s}+O_{t} W_{o}\right)</script></li></ul><h1 id="概念区分"><a href="#概念区分" class="headerlink" title="概念区分"></a>概念区分</h1><h2 id="学习和规划-Learning-amp-Planning"><a href="#学习和规划-Learning-amp-Planning" class="headerlink" title="学习和规划 Learning &amp; Planning"></a>学习和规划 Learning &amp; Planning</h2><ul><li>学习：环境初始时是<strong>未知</strong>的，个体不知道环境如何工作，个体通过与环境进行交互，逐渐改善其行为策略。</li><li>规划: 环境如何工作对于个体是<strong>已知</strong>或<strong>近似已知</strong>的，个体并不与环境发生实际的交互，而是利用其构建的模型进行计算，在此基础上改善其行为策略。<br>一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。</li></ul><h2 id="预测和控制-Prediction-amp-Control"><a href="#预测和控制-Prediction-amp-Control" class="headerlink" title="预测和控制 Prediction &amp; Control"></a>预测和控制 Prediction &amp; Control</h2><p>在强化学习里，我们经常需要先解决关于预测（prediction）的问题，而后在此基础上解决关于控制（Control）的问题。</p><ul><li>预测：给定一个策略，<strong>评价</strong>未来。可以看成是求解在给定策略下的价值函数（value function）的过程。How well will I(an agent) do if I(the agent) follow a specific policy?</li><li>控制：<strong>找到</strong>一个好的<strong>策略</strong>来最大化未来的奖励。</li></ul><h2 id="探索和利用-Exploration-amp-Exploitation"><a href="#探索和利用-Exploration-amp-Exploitation" class="headerlink" title="探索和利用 Exploration &amp; Exploitation"></a>探索和利用 Exploration &amp; Exploitation</h2><p>试错的学习，个体需要从其与环境的交互中<strong>发现并执行</strong>一个好的策略，同时又不至于在<strong>试错</strong>的过程中丢失太多的奖励。探索和利用是个体进行决策时需要平衡的两个方面</p><h3 id="探索率-epsilon"><a href="#探索率-epsilon" class="headerlink" title="探索率$\epsilon$"></a>探索率$\epsilon$</h3><p>怎么说的探索率呢？它主要是为了防止陷入局部最优。比如说目前在$s_1$状态下有两个$a_1,a_2$。我们通过计算出，发现执行$a_1$的动作比较好，但是为了防止陷入局部最优，我们会选择以$\epsilon$的概率来执行$a_2$，以$1-\epsilon$的概率来执行$a_1$。一般来说，$\epsilon$ 随着训练次数的增加而逐渐减小。</p><h2 id="其他概念"><a href="#其他概念" class="headerlink" title="其他概念"></a>其他概念</h2><h3 id="gamma-奖励衰减因子"><a href="#gamma-奖励衰减因子" class="headerlink" title="$\gamma$奖励衰减因子"></a>$\gamma$奖励衰减因子</h3><p>在上面的价值函数中，有一个变量$\gamma$ ，即奖励衰减因子，在[0，1]之间。如果为0，则是贪婪法，即价值只由当前的奖励决定，如果是1，则所有的后续状态奖励和当前奖励一视同仁。一般来说取0到1之间的数。</p><h3 id="环境的状态转化模型"><a href="#环境的状态转化模型" class="headerlink" title="环境的状态转化模型"></a>环境的状态转化模型</h3><p>由于在某个状态下，执行一定的action，能够达到新的一个状态$S_{t+1}$，但是$S_{t+1}$不一定是唯一的。环境的状态转化模型，可以理解为一个概率状态机，它是一个概率模型，即在状态$t$下采取动作$a$,转到下一个状态$s’$的概率，表示为$P^a_{ss’}$。</p><p>具体在RL_2，MP 中讲解</p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 强化学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-Coursera笔记</title>
      <link href="/posts/machine-learning/"/>
      <url>/posts/machine-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="AI-gt-机器学习分类图"><a href="#AI-gt-机器学习分类图" class="headerlink" title="AI->机器学习分类图"></a>AI-&gt;机器学习分类图</h1><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15936818055208.jpg?x-oss-process=style/blog" alt="-w669"></p><h1 id="矩阵补课"><a href="#矩阵补课" class="headerlink" title="矩阵补课"></a>矩阵补课</h1><h2 id="特征值分解EVD，奇异值分解SVD"><a href="#特征值分解EVD，奇异值分解SVD" class="headerlink" title="特征值分解EVD，奇异值分解SVD"></a>特征值分解EVD，奇异值分解SVD</h2><p>$A$是矩阵<br>$x_i$ 是单位特征向量<br>$\lambda_i$是特征值<br>$\Lambda$ 是矩阵特征值</p><h3 id="EVD特征值分解（The-eigenvalue-value-decomposition）"><a href="#EVD特征值分解（The-eigenvalue-value-decomposition）" class="headerlink" title="EVD特征值分解（The eigenvalue value decomposition）"></a>EVD特征值分解（The eigenvalue value decomposition）</h3><p>针对方阵，特征值<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15921997636239.jpg?x-oss-process=style/blog" alt="-w397"><br>$A = U\Lambda U^{-1} = U\Lambda U^T$<br>进行矩阵运算时，Ax，先对x分解$x =aU^T= a_1 x_1+…a_mx_m$<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922013862028.jpg?x-oss-process=style/blog" alt="-w366"></p><p>则$U\Lambda U^T x = U\Lambda U^T U a^T = U\Lambda a^T$<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922013786257.jpg?x-oss-process=style/blog" alt="-w698"></p><p>$U\Lambda a^T = U(\lambda a)^T$<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922013718111.jpg?x-oss-process=style/blog" alt="-w712"><br>效果如下，将向量在单位特征向量上，伸长为$\lambda$倍<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922016553419.jpg?x-oss-process=style/blog" alt="-w554"></p><h3 id="SVD奇异值分解（Singularly-Valuable-Decomposition）"><a href="#SVD奇异值分解（Singularly-Valuable-Decomposition）" class="headerlink" title="SVD奇异值分解（Singularly Valuable Decomposition）"></a>SVD奇异值分解（Singularly Valuable Decomposition）</h3><p>矩阵A，mxn维，将n维的向量映射到m维空间中,k&lt;=m<br>正交基，$(v_1,v_2…v_n)$<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922032144227.jpg?x-oss-process=style/blog" alt="-w624"><br>$A^T A = \lambda_j$<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922033896848.jpg?x-oss-process=style/blog" alt="-w730"><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922036689443.jpg?x-oss-process=style/blog" alt="-w722"><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922038887667.jpg?x-oss-process=style/blog" alt="-w725"></p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922052033790.jpg?x-oss-process=style/blog" alt="-w697"></p><p>存储领域，选取u，v正交基矩阵，计算奇异值矩阵，使奇异值矩阵尽量集中，即可取到</p><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p>E：经验<br>T：任务<br>P：概率</p><h3 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h3><ul><li>监督学习(supervisor learning)：分类（classification）、回归（regression）</li><li>无监督学习(unsupervisor learning)：</li><li>强化学习Reinforcement learning</li></ul><h2 id="2、Linear-regression线型回归"><a href="#2、Linear-regression线型回归" class="headerlink" title="2、Linear regression线型回归"></a>2、Linear regression线型回归</h2><h3 id="Cost-funciton-代价函数"><a href="#Cost-funciton-代价函数" class="headerlink" title="Cost funciton-代价函数"></a>Cost funciton-代价函数</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922084781269.jpg?x-oss-process=style/blog" alt="-w444"><br>矩阵表达<br>$J(\theta) = \frac{1}{2m}(X\theta-Y)^T(X\theta-Y)$</p><ul><li>梯度下降法（Gradient Descent）<br>$\theta_{i+1} =\theta_i - \alpha\nabla J(\theta) $<br>$\frac{\partial J(\theta)}{\partial\theta} = \frac{1}{m}X^T(X\theta-Y)$<br>推导过程<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923078000390.jpg?x-oss-process=style/blog" alt="-w787"><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923078144953.jpg?x-oss-process=style/blog" alt="-w607"></li></ul><ul><li>正规方程法（Normal Equation）<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922203884161.jpg?x-oss-process=style/blog" alt="-w601"><br>图中公式，theta的维是n，不是m<br>另一种理解方式<br>相当于求解$Y = X\theta$<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922219982331.jpg?x-oss-process=style/blog" alt="-w714"></li></ul><p>b相当于y，a相当于x组成的矩阵，<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922210683035.jpg?x-oss-process=style/blog" alt="-w189"><br>求导过程<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922218251765.jpg?x-oss-process=style/blog" alt="-w598"></p><h3 id="线性代数回顾"><a href="#线性代数回顾" class="headerlink" title="线性代数回顾"></a>线性代数回顾</h3><p>矩阵、向量使用规范<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922243037001.jpg?x-oss-process=style/blog" alt="-w860"></p><h3 id="加速梯度下降方法，让-x-i-尺度一致"><a href="#加速梯度下降方法，让-x-i-尺度一致" class="headerlink" title="加速梯度下降方法，让$x_i$尺度一致"></a>加速梯度下降方法，让$x_i$尺度一致</h3><ul><li>Feature Scaling<br>将输入值归一化，缩放到[-1,1]之间，梯度下降法更快收敛</li><li>Mean Normalization<br>$x_i = \frac{x_i - \mu_i}{s_i}$,其中$\mu_i$是input的平均值，$s_i$是取值的范围，或者标准偏差</li></ul><h3 id="回归问题方法选择"><a href="#回归问题方法选择" class="headerlink" title="回归问题方法选择"></a>回归问题方法选择</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922433025100.jpg?x-oss-process=style/blog" alt="-w869"><br>正规方程法行不通：</p><ul><li>$X^TX$不可逆</li></ul><ol><li>元素中有redundant features,linearly dependent</li><li>过多的features，导致input维度n&gt;m</li></ol><h3 id="回归问题的矩阵表达"><a href="#回归问题的矩阵表达" class="headerlink" title="回归问题的矩阵表达"></a>回归问题的矩阵表达</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15922713533207.jpg?x-oss-process=style/blog" alt="-w650"></p><h2 id="3、Logistic-Regression逻辑回归"><a href="#3、Logistic-Regression逻辑回归" class="headerlink" title="3、Logistic Regression逻辑回归"></a>3、Logistic Regression逻辑回归</h2><p>分类classification</p><h3 id="函数表达式"><a href="#函数表达式" class="headerlink" title="函数表达式"></a>函数表达式</h3><p>$z = \theta^T x$<br>$h(z) = sigmoid(z)$<br>处理regression函数：连续变离散-&gt;Hypothesis </p><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>h(z)代表着一个边界，将值分为&gt;0和&lt;0<br>由于sigmoid函数的特性，程序最终会优化到z取值远离零点</p><h3 id="Cost-function-的选择"><a href="#Cost-function-的选择" class="headerlink" title="Cost function 的选择"></a>Cost function 的选择</h3><p>不能选择最小二乘法，因为目标是一个非凸函数<br>凸函数才能最好利用梯度下降法<br>所以对于，y-0，1的分类问题，改写cost function为<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923134620258.jpg?x-oss-process=style/blog" alt="-w375"><br>进一步改写为一个式子<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923135710044.jpg?x-oss-process=style/blog" alt="-w696"><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923571866503.jpg?x-oss-process=style/blog" alt="-w902"></p><p>推导过程中，利用了sigmoid的求导法则<br>$\sigma’(x) = \sigma(x)(1-\sigma(x))$</p><p>特殊设计过的sigmoid函数 和 cost function<br>使得，满足$\theta$参数更新可以矢量化<br>$\theta_{i+1} = \theta_i - \alpha  \nabla J(\theta)=\theta_i - \alpha X^T(g(X\theta) - Y)$<br>$g(X\theta)$是sigmoid函数对$\X\theta$矩阵每个元素进行操作<br>特征缩放，</p><h2 id="其他参数优化方法"><a href="#其他参数优化方法" class="headerlink" title="其他参数优化方法"></a>其他参数优化方法</h2><ul><li>Conjugate gradient 共轭下降法</li><li>BFGS</li><li>L_BFGS<br>优点：</li><li>不需要选择学习速率$\alpha$</li><li>收敛速度快<br>缺点：复杂<br>可以直接调用</li></ul><pre><code>1.设置优化参数 optimset = 初始参数，方法，强制结束迭代次数2.设置初始条件，Initialpara = 3.[Jval, theta'] = Cost_function (X,Y)4.调用优化函数[Jval，theta'] = (@Cost_function,Initialpara,optimset)，</code></pre><h2 id="多类别分类"><a href="#多类别分类" class="headerlink" title="多类别分类"></a>多类别分类</h2><p>构建i个分类器，利用i个h(z)，处理<br>分别给出属于某个分类的几率值</p><p>X 特征矩阵</p><h2 id="3-2回归遇到的问题，解决方案，正则化"><a href="#3-2回归遇到的问题，解决方案，正则化" class="headerlink" title="3.2回归遇到的问题，解决方案，正则化"></a>3.2回归遇到的问题，解决方案，正则化</h2><ul><li><p>过拟合<br>拟合特征数&gt;&gt;样本量，</p></li><li><p>欠拟合<br>特征数不够&lt;&lt;样本量，不能正确预测，回归</p><h3 id="办法"><a href="#办法" class="headerlink" title="办法"></a>办法</h3><p>1、 减少无关特征</p></li><li>手动减少无关特征</li><li>模型选择算法，自动选择相关变量<br>2、 regularization 正则化<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923657698696.jpg?x-oss-process=style/blog" alt="-w452"><br>正则化参数，使特征拟合参数减小权重</li></ul><h4 id="线性回归正则化"><a href="#线性回归正则化" class="headerlink" title="线性回归正则化"></a>线性回归正则化</h4><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923660929388.jpg?x-oss-process=style/blog" alt="-w784"><br><strong>对于逻辑回归正则化，式子一样</strong></p><h2 id="4、神经网络——Nonlinear-Hypotheses"><a href="#4、神经网络——Nonlinear-Hypotheses" class="headerlink" title="4、神经网络——Nonlinear Hypotheses"></a>4、神经网络——Nonlinear Hypotheses</h2><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923834646377.jpg?x-oss-process=style/blog" alt="-w591"><br>输入层、隐藏层、输出层<br>g 激活函数$\in[0,1]$：<br>h 输出函数</p><ul><li>阶跃</li><li>逻辑函数，sigmoid，无限可微</li><li>斜坡函数</li><li>高斯函数<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923839870398.jpg?x-oss-process=style/blog" alt="-w709"><h3 id="multiclass-classification"><a href="#multiclass-classification" class="headerlink" title="multiclass classification"></a>multiclass classification</h3>输出层y不是一个数字，[1;0;0;0] [0;1;0;0]instead<h3 id="Forward-propagation"><a href="#Forward-propagation" class="headerlink" title="Forward propagation"></a>Forward propagation</h3>$a^{(j+1)} = g(\Theta ^ {(j})a^{(j)})$</li></ul><h3 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h3><p>Cost function<br><strong>符号约定</strong><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923869999391.jpg?x-oss-process=style/blog" alt="-w788"><br><a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb">传播计算推导</a></p><h3 id="BP神经网络——算法步骤"><a href="#BP神经网络——算法步骤" class="headerlink" title="BP神经网络——算法步骤"></a>BP神经网络——算法步骤</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15923952253210.jpg?x-oss-process=style/blog" alt="-w822"><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15924600046523.jpg?x-oss-process=style/blog" alt="-w536"></p><p><strong>调用函数的时候 unroll矩阵-&gt;Vector</strong><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15924602839093.jpg?x-oss-process=style/blog" alt="-w702"></p><p>gradient check<br>引入 $\epsilon$，数值计算，缺点太慢，只用于编程时的校验</p><p>$\Theta$初始化<br>随机初始化，零值代入会有问题，权重难更新<br>我们将初始化权值 $\Theta_{ij}^{(l)}$ 的范围限定在 $[-\Phi ,\Phi ]$ 。</p><h2 id="6、Advice-for-applying-machine-learning"><a href="#6、Advice-for-applying-machine-learning" class="headerlink" title="6、Advice for applying machine learning"></a>6、Advice for applying machine learning</h2><h3 id="评价拟合函数hypothesis"><a href="#评价拟合函数hypothesis" class="headerlink" title="评价拟合函数hypothesis"></a>评价拟合函数hypothesis</h3><ol><li>分类数据集（training set、test set）</li><li>用训练集的theta 计算测试集的误差（分类问题，误差定义为0/1，最终统计结果表现为错误率）<h3 id="模型选择——（Train-Validation-Test-sets）"><a href="#模型选择——（Train-Validation-Test-sets）" class="headerlink" title="模型选择——（Train/ Validation/ Test sets）"></a>模型选择——（Train/ Validation/ Test sets）</h3></li></ol><ul><li>训练多个模型，在测试集中找到表现最优</li><li>偏差和方差（Bias/ Variance）<br>关于 模型种类<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925327225783.jpg?x-oss-process=style/blog" alt="-w335"><br>关于 正则化参数<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925342959750.jpg?x-oss-process=style/blog" alt="-w808"></li></ul><h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925410355870.jpg?x-oss-process=style/blog" alt="-w400"></p><p>High bias<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925413495753.jpg?x-oss-process=style/blog" alt="-w732"></p><p>High Variance<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925414621124.jpg?x-oss-process=style/blog" alt="-w696"></p><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925415469599.jpg?x-oss-process=style/blog" alt="-w537"></p><h2 id="6-2-设计神经网络"><a href="#6-2-设计神经网络" class="headerlink" title="6.2 设计神经网络"></a>6.2 设计神经网络</h2><ol><li><strong>快速部署</strong>、设计简单网络</li><li>plot 学习曲线，发现问题</li><li>误差分析（验证集）：数值被错误分类的特征，度量误差</li></ol><h3 id="误差度量-for-skewed-classes-偏斜类"><a href="#误差度量-for-skewed-classes-偏斜类" class="headerlink" title="误差度量 for skewed classes 偏斜类"></a>误差度量 for skewed classes 偏斜类</h3><p>precision/recall<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925459953399.jpg?x-oss-process=style/blog" alt="-w789"><br>针对最后一级h(x)，<br>防止错判，阈值提高，设定逻辑判断阈值0.9  instead of 0.5<br>防止漏过1，阈值放低</p><h4 id="综合评定标准"><a href="#综合评定标准" class="headerlink" title="综合评定标准"></a>综合评定标准</h4><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925496319235.jpg?x-oss-process=style/blog" alt="-w779"></p><h2 id="7、支持向量机SVM（support-vector-machine）"><a href="#7、支持向量机SVM（support-vector-machine）" class="headerlink" title="7、支持向量机SVM（support vector machine）"></a>7、支持向量机SVM（support vector machine）</h2><h3 id="7-1-SVM-大间距分类器（Large-Margin-Classification）"><a href="#7-1-SVM-大间距分类器（Large-Margin-Classification）" class="headerlink" title="7.1 SVM 大间距分类器（Large Margin Classification）"></a>7.1 SVM 大间距分类器（Large Margin Classification）</h3><p><strong>重写了cost function 和 h（z）</strong></p><p>支持向量机的代价函数为：</p><script type="math/tex; mode=display">min_{\theta} C[\sum_{i=1}^{m}{y^{(i)}}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^{n}{\theta_j^2}</script><p><img src="/img/15970703768252.jpg" alt="-w578"></p><p>有别于逻辑回归假设函数输出的是概率，支持向量机它是直接预测 y 的值是0还是<br><strong>假设函数</strong></p><script type="math/tex; mode=display">h_{\theta}(x)=\left\{\begin{matrix}1,\;\;if\; \theta^{T}x\geqslant 0\\ 0,\;\;otherwise\end{matrix}\right.</script><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925640229783.jpg?x-oss-process=style/blog" alt="-w889"><br>最小化$\theta$的模，相当于最大化样本在$\theta$上的投影长度，图中直观表现为，绿色边界在$\theta$方向上距离样本距离最远。<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925718971038.jpg?x-oss-process=style/blog" alt="-w549"><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15925719456841.jpg?x-oss-process=style/blog" alt="-w587"></p><h2 id="7-2-kernels核函数"><a href="#7-2-kernels核函数" class="headerlink" title="7.2 kernels核函数"></a>7.2 kernels核函数</h2><p>核函数满足$κ(xi·xj)=φ(xi)T·φ(xj)$<br>低维线性不可分（欠拟合）-&gt;映射到高维<br>避免维度灾难，引入核函数（Kernels），用样本去构造特征<br>适用于n&lt;&lt;m, 增加特征数量变为m</p><h3 id="高斯核函数"><a href="#高斯核函数" class="headerlink" title="高斯核函数"></a>高斯核函数</h3><p>参数：$l(i),\sigma$<br>$f = exp^{(-\frac{||x-l^{(i)}||^2}{2\sigma^2})}$<br>取值[0,1]</p><h3 id="注意的点"><a href="#注意的点" class="headerlink" title="注意的点"></a>注意的点</h3><p>核函数用于逻辑回归，运算很慢<br>核函数优化算法仅适用于SVM<br>使用前，一定归一化处理</p><h2 id="分类模型的选择"><a href="#分类模型的选择" class="headerlink" title="分类模型的选择"></a>分类模型的选择<a href=""></a></h2><p>7.3 分类模型的选择<br>目前，我们学到的分类模型有：<br>（1）逻辑回归；<br>（2）神经网络；<br>（3）SVM<br>怎么选择在这三者中做出选择呢？我们考虑特征维度 n 及样本规模 m ：</p><ol><li>如果 n 相对于 m 非常大，例如 n=10000 ，而 $m\in(10,1000)$ ：此时选用逻辑回归或者无核的 SVM。</li><li>如果 n 较小，m 适中，如  $n\in(1,1000)$ ，而  $m\in(10,10000)$ ：此时选用核函数为高斯核函数的 SVM。</li><li>如果 n 较小，m 较大，如  $n\in(1,1000)$ ，而 m&gt;50000 ：此时，需要创建更多的特征（比如通过多项式扩展），再使用逻辑回归或者无核的 SVM。 神经网络对于上述情形都有不错的适应性，但是计算性能上较慢。</li></ol><h2 id="8、无监督学习（Unsupervised-learning）"><a href="#8、无监督学习（Unsupervised-learning）" class="headerlink" title="8、无监督学习（Unsupervised learning）"></a>8、无监督学习（Unsupervised learning）</h2><h3 id="8-1-分类K-means-algorithm-Clustering"><a href="#8-1-分类K-means-algorithm-Clustering" class="headerlink" title="8.1 分类K-means algorithm(Clustering)"></a>8.1 分类K-means algorithm(Clustering)</h3><ol><li>cluster 分类，计算到$\mu_k$距离将下表k分配给$c_i$的</li><li>移动cluster central到，分类的平均点<h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4>Cost function<br>找到$c_i$ 和 $\mu_k$，使函数：<script type="math/tex; mode=display">J(c^{(1)},c^{(2)},\cdots ,c^{(m)};\mu_1,\mu_2,\cdots ,\mu_k)=\frac{1}{m}\sum_{i=1}^m\left \| x^{(i)}-\mu_c(i) \right \|^2</script></li></ol><p>$c_i\in[1,K]$</p><h4 id="mu-k-随机初始化，避免局部最优"><a href="#mu-k-随机初始化，避免局部最优" class="headerlink" title="$\mu_k$随机初始化，避免局部最优"></a>$\mu_k$随机初始化，避免局部最优</h4><ol><li>k&lt;m</li><li>随机指定$\mu_k = x(i)$</li><li>多次运算，找最优结果<br>小k时，多次初始化进行运算<h4 id="选择cluster-数量"><a href="#选择cluster-数量" class="headerlink" title="选择cluster 数量"></a>选择cluster 数量</h4>plot cluster 数量 为横坐标，找突变点</li></ol><h3 id="8-2-Dimensionality-reduction"><a href="#8-2-Dimensionality-reduction" class="headerlink" title="8.2 Dimensionality reduction"></a>8.2 Dimensionality reduction</h3><h4 id="数据压缩-Data-Compression"><a href="#数据压缩-Data-Compression" class="headerlink" title="数据压缩 Data Compression"></a>数据压缩 Data Compression</h4><p>减少冗余特征变量</p><h4 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h4><h4 id="PCA主成分分析法（Principal-Component-Analysis）"><a href="#PCA主成分分析法（Principal-Component-Analysis）" class="headerlink" title="PCA主成分分析法（Principal Component Analysis）"></a>PCA主成分分析法（Principal Component Analysis）</h4><h3 id="PCA算法流程"><a href="#PCA算法流程" class="headerlink" title="PCA算法流程"></a>PCA算法流程</h3><ol><li><p>特征压缩<br>假定我们需要将特征维度从 n 维降到 k 维。则 PCA 的执行流程如下：<br>特征标准化，平衡各个特征尺度：</p><script type="math/tex; mode=display">x^{(i)}_j=\frac{x^{(i)}_j-\mu_j}{s_j}</script><p>$\mu_j$ 为特征 j 的均值，sj 为特征 j 的标准差。<br>计算协方差矩阵 $\Sigma $ ：</p><script type="math/tex; mode=display">\Sigma =\frac{1}{m}\sum_{i=1}{m}(x^{(i)})(x^{(i)})^T=\frac{1}{m} \cdot  X^TX</script><p>通过奇异值分解（SVD），求取 $\Sigma $ 的特征向量（eigenvectors）：</p><script type="math/tex; mode=display">(U,S,V^T)=SVD(\Sigma )</script><p>从 U 中取出前 k 个左奇异向量，构成一个约减矩阵 Ureduce :</p><script type="math/tex; mode=display">U_{reduce}=(\mu^{(1)},\mu^{(2)},\cdots,\mu^{(k)})</script><p>计算新的特征向量： $z^{(i)}$</p><script type="math/tex; mode=display">z^{(i)}=U^{T}_{reduce} \cdot  x^{(i)}</script></li><li><p>特征还原<br>因为 PCA 仅保留了特征的主成分，所以 PCA 是一种有损的压缩方式，假定我们获得新特征向量为：</p><script type="math/tex; mode=display">z=U^T_{reduce}x</script><p>那么，还原后的特征 $x_{approx}$ 为：</p><script type="math/tex; mode=display">x_{approx}=U_{reduce}z</script></li><li>信息保留评价<br>降维多少才合适？<br>从 PCA 的执行流程中，我们知道，需要为 PCA 指定目的维度 k 。如果降维不多，则性能提升不大；如果目标维度太小，则又丢失了许多信息。通常，使用如下的流程的来评估 k 值选取优异：<br>求各样本的投影均方误差:<script type="math/tex; mode=display">\min \frac{1}{m}\sum_{j=1}^{m}\left \| x^{(i)}-x^{(i)}_{approx} \right \|^2</script>求数据的总方差variance：<script type="math/tex; mode=display">\frac{1}{m}\sum_{j=1}^{m}\left \| x^{(i)} \right \|^2</script>评估下式是否成立:<script type="math/tex; mode=display">\frac{\min \frac{1}{m}\sum_{j=1}^{m}\left \| x^{(i)}-x^{(i)}_{approx} \right \|^2}{\frac{1}{m}\sum_{j=1}^{m}\left \| x^{(i)} \right \|^2} \leqslant \epsilon</script>其中， $\epsilon $ 的取值可以为 0.01,0.05,0.10,⋯，假设  $\epsilon = 0.01 $ ，我们就说“特征间 99% 的差异性得到保留”。<br><strong>看$\Sigma$矩阵的二范数占比就知道</strong><br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15928009747208.jpg?x-oss-process=style/blog" alt="-w710"></li></ol><h3 id="PCA-point"><a href="#PCA-point" class="headerlink" title="PCA-point"></a>PCA-point</h3><p>协方差矩阵$\Sigma$看主成分<br>取前k个u构成向量<br>$U_{reduce}\in \mathbb{R}^{n\times k}$<br>PCA 不能解决过拟合，要用正则化的方式</p><h2 id="9、异常检测"><a href="#9、异常检测" class="headerlink" title="9、异常检测"></a>9、异常检测</h2><h3 id="9-1高斯分布-Gaussian-normal-distribution"><a href="#9-1高斯分布-Gaussian-normal-distribution" class="headerlink" title="9.1高斯分布(Gaussian normal distribution)"></a>9.1高斯分布(Gaussian normal distribution)</h3><p>$x\sim N(\mu,\sigma^2)$<br>其分布概率为：</p><script type="math/tex; mode=display">p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})</script><p>其中 $\mu$ 为期望值（均值）， $\sigma^2$ 为方差。<br>在概率论中，对有限个样本进行参数估计</p><script type="math/tex; mode=display">\mu_j = \frac{1}{m} \sum_{i=1}^{m}x_j^{(i)}\;\;\;,\;\;\; \delta^2_j = \frac{1}{m} \sum_{i=1}^{m}(x_j^{(i)}-\mu_j)^2</script><p>这里对参数 $\mu$ 和参数 $\delta^2$ 的估计就是二者的极大似然估计。<br>假定每一个特征 $x_{1}$ 到 $x_{n}$ 均服从正态分布，则其模型的概率为：</p><script type="math/tex; mode=display">\begin{align*}p(x)&amp;=p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma_2^2) \cdots p(x_n;\mu_n,\sigma_n^2)\\&amp;=\prod_{j=1}^{n}p(x_j;\mu_j,\sigma_j^2)\\&amp;=\prod_{j=1}^{n} \frac{1}{\sqrt{2\pi}\sigma_{j}}exp(-\frac{(x_{j}-\mu_{j})^2}{2\sigma_{j}^2})\end{align*}</script><p>当 $p(x)&lt;\varepsilon$时，$x$ 为异常样本。</p><h3 id="算法评价"><a href="#算法评价" class="headerlink" title="算法评价"></a>算法评价</h3><p>由于异常样本是非常少的，所以整个数据集是非常偏斜的，我们不能单纯的用预测准确率来评估算法优劣，所以用我们之前的查准率（Precision）和召回率（Recall）计算出 F 值进行衡量异常检测算法了。<br>真阳性、假阳性、真阴性、假阴性<br>查准率（Precision）与 召回率（Recall）<br>F1 Score<br>我们还有一个参数 $\varepsilon$ ，这个 $\varepsilon$ 是我们用来决定什么时候把一个样本当做是异常样本的阈值。我们应该试用多个不同的 $\varepsilon$ 值，选取一个使得 F 值最大的那个 $\varepsilon$ 。</p><h3 id="异常检测与逻辑回归的区别"><a href="#异常检测与逻辑回归的区别" class="headerlink" title="异常检测与逻辑回归的区别"></a>异常检测与逻辑回归的区别</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15928075440417.jpg?x-oss-process=style/blog" alt="-w790"></p><p>异常检测数据特点是：</p><ol><li>数据偏斜，y=1数据量极少</li><li>异常数据特征不聚类（不稳定），难以预测</li></ol><h3 id="多元高斯函数"><a href="#多元高斯函数" class="headerlink" title="多元高斯函数"></a>多元高斯函数</h3><p>其概率模型为： <script type="math/tex">p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><br>（其中 $|\Sigma|$ 是 $\Sigma$ 的行列式，$\mu$ 表示样本均值，$\Sigma$ 表示样本协方差矩阵。）。<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15930031190056.jpg?x-oss-process=style/blog" alt="-w740"><br>其中$\Sigma$参数估计：</p><script type="math/tex; mode=display">\mu=\frac{1}{m}\sum_{i=1}^{m}{x^{(i)}}$$$$\Sigma=\frac{1}{m}\sum_{i=1}^{m}{(x^{(i)}-\mu)(x^{(i)}-\mu)^T}</script><h3 id="算法流程-多元高斯分布异常检测"><a href="#算法流程-多元高斯分布异常检测" class="headerlink" title="算法流程-多元高斯分布异常检测"></a>算法流程-多元高斯分布异常检测</h3><p>采用了多元高斯分布的异常检测算法流程如下：<br>选择一些足够反映异常样本的特征 $x_j$ 。<br>对各个样本进行参数估计： <script type="math/tex">\mu=\frac{1}{m}\sum_{i=1}^{m}{x^{(i)}}$$$$\Sigma=\frac{1}{m}\sum_{i=1}^{m}{(x^{(i)}-\mu)(x^{(i)}-\mu)^T}</script><br>当新的样本 x 到来时，计算 $p(x)$ ：</p><script type="math/tex; mode=display">p(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><p>如果 $p(x)&lt;\varepsilon $ ，则认为样本 x 是异常样本。</p><h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><p>一般高斯模型：<br>需要手动创建一些特征来描述某些特征的相关性<br>多元高斯模型：<br>利用协方差矩阵$\Sigma$获得了各个特征相关性</p><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><p>一般高斯模型：<br>计算复杂度低，适用于高维特征<br>多元高斯模型：<br>计算复杂</p><h3 id="效果¶"><a href="#效果¶" class="headerlink" title="效果¶"></a>效果¶</h3><p>一般高斯模型：<br>在样本数目 m 较小时也工作良好<br>多元高斯模型：<br>需要 $\Sigma$ 可逆，亦即需要 $m&gt;n$ ，(通常会考虑 $ m \geqslant 10*n $，确保有足够多的数据去拟合这些变量，更好的去评估协方差矩阵 $\Sigma$ )且各个特征不能线性相关，如不能存在 $x_2=3x_1$ 或者 $x_3=x_1+2x_2$</p><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>基于多元高斯分布模型的异常检测应用十分有限。</p><h3 id="9、2-推荐器-Recommender-system"><a href="#9、2-推荐器-Recommender-system" class="headerlink" title="9、2 推荐器 Recommender system"></a>9、2 推荐器 Recommender system</h3><h4 id="Content-based-recommendations"><a href="#Content-based-recommendations" class="headerlink" title="Content based recommendations"></a>Content based recommendations</h4><p>$y(i,j) = \theta_{j}^T x_i$评分等于电影特征成分*用户喜好<br>前提：电影特征$x_i$已知，求解用户喜好$\theta_j$</p><p>为了对用户 j 打分状况作出最精确的预测，我们需要：</p><script type="math/tex; mode=display">\min_{(\theta^{(j)})}=\frac{1}{2}\sum_{i:r(i,j)=1}^{}{((\theta^{(j)})^T(x^{(i)})-y^{(i,j)})^2}+\frac{\lambda}{2}\sum_{k=1}^{n}{(\theta_k^{(j)})^2}</script><p>计算出所有的 $\theta$ 为：</p><script type="math/tex; mode=display">J(\theta^{(1)},\cdots,\theta^{(n_u)})=\min_{(\theta^{(1)},\cdots,\theta^{(n_u)})}=\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}^{}{((\theta^{(j)})^T(x^{(i)})-y^{(i,j)})^2}+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}{(\theta_k^{(j)})^2}</script><p>与前面所学线性回归内容的思路一致，为了计算出 $J(\theta^{(1)},\cdots,\theta^{(n_u)})$，使用梯度下降法来更新参数：<br>更新偏置（插值）：</p><script type="math/tex; mode=display">\theta^{(j)}_0=\theta^{(j)}_0-\alpha \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x^{(i)}_0</script><p>更新权重：</p><script type="math/tex; mode=display">\theta^{(j)}_k=\theta^{(j)}_k-\alpha \left( \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x^{(i)}_k+\lambda \theta^{(j)}_k \right),\;\;\; k \neq 0</script><h4 id="协同过滤Collaborative-filtering"><a href="#协同过滤Collaborative-filtering" class="headerlink" title="协同过滤Collaborative filtering"></a>协同过滤Collaborative filtering</h4><p>电影特征成分$x_i$和用户喜好$\theta_j$均未知</p><h4 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h4><ol><li>目标优化<br>当用户给出他们喜欢的类型，即 $\theta^{(1)},\cdots,\theta^{(n_u)}$ ，我们可以由下列式子得出 $x^{(i)}$ ：<script type="math/tex; mode=display">\min_{(x^{(i)})}=\frac{1}{2}\sum_{j:r(i,j)=1}^{}{((\theta^{(j)})^T(x^{(i)})-y^{(i,j)})^2}+\frac{\lambda}{2}\sum_{k=1}^{n}{(x_k^{(i)})^2}</script>可出所有的 x 则为：<script type="math/tex; mode=display">\min_{(x^{(1)},\cdots,x^{(n_m)})}=\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}^{}{((\theta^{(j)})^T(x^{(i)})-y^{(i,j)})^2}+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}{(x_k^{(i)})^2}</script>只要我们得到 $\theta$ 或者 x ，都能互相推导出来。<br>协同过滤算法基本思想就是当我们得到其中一个数据的时候，我们推导出另一个，然后根据推导出来的再推导回去进行优化，优化后再继续推导继续优化，如此循环协同推导。</li><li>协同过滤的目标优化<br>推测用户喜好：给定$x^{(1)},\cdots,x^{(n_m)}$ ，估计$\theta^{(1)},\cdots,\theta^{(n_\mu)}$ ： <script type="math/tex">\min_{(\theta^{(1)},\cdots,\theta^{(n_\mu)})}=\frac{1}{2}\sum_{j=1}^{n_\mu}\sum_{i:r(i,j)=1}^{}{((\theta^{(j)})^T(x^{(i)})-y^{(i,j)})^2}+\frac{\lambda}{2}\sum_{j=1}^{n_\mu}\sum_{k=1}^{n}{(\theta_k^{(j)})^2}</script><br>推测商品内容：给定$\theta^{(1)},\cdots,\theta^{(n_\mu)}$ ，估计$x^{(1)},\cdots,x^{(n_m)}$ ： <script type="math/tex">\min_{(x^{(1)},\cdots,x^{(n_m)})}=\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}^{}{((\theta^{(j)})^T(x^{(i)})-y^{(i,j)})^2}+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}{(x_k^{(i)})^2}</script><br>协同过滤：同时优化$x^{(1)},\cdots,x^{(n_m)}$ ，估计$\theta^{(1)},\cdots,\theta^{(n_\mu)}$： <script type="math/tex">\min \; J(x^{(1)},\cdots,x^{(n_m)};\theta^{(1)},\cdots,\theta^{(n_\mu)})</script><br>即：<script type="math/tex; mode=display">\min_{(x^{(1)},\cdots,x^{(n_m)};\theta^{(1)},\cdots,\theta^{(n_\mu)})}=\frac{1}{2}\sum_{(i,j):r(i,j)=1}^{}{((\theta^{(j)})^T(x^{(i)})-y^{(i,j)})^2}+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}{(x_k^{(i)})^2}+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}{(\theta_k^{(j)})^2}</script>因为正则化的原因在这里面不再有之前的 $x_0=1$,$\theta_0=0$ 。</li><li>协同过滤算法的步骤为：<br>随机初始化$x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_\mu)} $为一些较小值，与神经网络的参数初始化类似，为避免系统陷入僵死状态，不使用 0 值初始化。<br>通过梯度下降的算法计算出$J(x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_\mu)})$,参数更新式为： <script type="math/tex">x^{(i)}_k=x^{(i)}_k-\alpha \left( \sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta^{(j)}_k+\lambda x^{(i)}_k \right)$$$$\theta^{(j)}_k=\theta^{(j)}_k-\alpha \left( \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x^{(i)}_k+\lambda \theta^{(j)}_k \right)</script><br>如果用户的偏好向量为$\theta$，而商品的特征向量为 x ，则可以预测用户评价为 $\theta^Tx$ 。<br>因为协同过滤算法 $\theta$ 和 x 相互影响，因此，二者都没必要使用偏置 $\theta_0$ 和 $x_0$，即，$x \in \mathbb{R}^n$、 $\theta \in \mathbb{R}^n$ 。<h4 id="低秩分解（Low-Rank-Matrix-Factorization）"><a href="#低秩分解（Low-Rank-Matrix-Factorization）" class="headerlink" title="低秩分解（Low Rank Matrix Factorization）"></a>低秩分解（Low Rank Matrix Factorization）</h4>$Y = X\Theta^T$<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15933132737330.jpg?x-oss-process=style/blog" alt="-w712"></li></ol><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ol><li>$\lambda$正则化，使$\theta$趋向0</li><li>平均值正则化(mean normalization)<br>将平均值作为0点，让Y偏置<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15933144020810.jpg?x-oss-process=style/blog" alt="-w704"></li></ol><h2 id="10、大数据集——提升运算速度"><a href="#10、大数据集——提升运算速度" class="headerlink" title="10、大数据集——提升运算速度"></a>10、大数据集——提升运算速度</h2><h3 id="Stochastic-Gradient-Descent（随机梯度下降法）"><a href="#Stochastic-Gradient-Descent（随机梯度下降法）" class="headerlink" title="Stochastic Gradient Descent（随机梯度下降法）"></a>Stochastic Gradient Descent（随机梯度下降法）</h3><p>不用全部数据集进行运算</p><ol><li>预处理，随机排序</li><li>内循环，单个特征修改拟合参数<br>每次使用一个样本<h4 id="SGD收敛性"><a href="#SGD收敛性" class="headerlink" title="SGD收敛性"></a>SGD收敛性</h4>每隔1000个样本画cost函数</li></ol><ul><li>小$\alpha$让曲线慢，准</li><li>大样本采样间距-&gt;曲线更光滑</li><li>随着迭代次数，减小α</li></ul><h3 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini-Batch Gradient Descent"></a>Mini-Batch Gradient Descent</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15933256124873.jpg?x-oss-process=style/blog" alt="-w672"><br>矢量化-&gt;并行计算，提高效率</p><h3 id="在线学习Online-learning"><a href="#在线学习Online-learning" class="headerlink" title="在线学习Online learning"></a>在线学习Online learning</h3><p>数据集连续，减少存储成本</p><h3 id="Map-reduce-and-data-parallelism"><a href="#Map-reduce-and-data-parallelism" class="headerlink" title="Map reduce and data parallelism"></a>Map reduce and data parallelism</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15933397359609.jpg?x-oss-process=style/blog" alt="-w728"><br>代数计算库自动implement</p><h2 id="12、Photo-OCR-pipeline"><a href="#12、Photo-OCR-pipeline" class="headerlink" title="12、Photo OCR pipeline"></a>12、Photo OCR pipeline</h2><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15933522420345.jpg?x-oss-process=style/blog" alt="-w684"></p><ol><li>文本检测</li><li>特征分割</li><li>特征识别</li><li>修正C1eaning-&gt;cleaning<h3 id="Sliding-window-滑窗分类器"><a href="#Sliding-window-滑窗分类器" class="headerlink" title="Sliding window 滑窗分类器"></a>Sliding window 滑窗分类器</h3><h4 id="文本检测"><a href="#文本检测" class="headerlink" title="文本检测"></a>文本检测</h4>步长step size<br>不同大小，按照比例缩放</li></ol><p>检测到特征，相邻互联</p><h4 id="特征分割"><a href="#特征分割" class="headerlink" title="特征分割"></a>特征分割</h4><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15933520905301.jpg?x-oss-process=style/blog" alt="-w656"></p><h3 id="获取数据，人造数据"><a href="#获取数据，人造数据" class="headerlink" title="获取数据，人造数据"></a>获取数据，人造数据</h3><ol><li>加背景噪音</li><li>字体处理</li><li>人工扭曲</li></ol><p>加入高斯噪声没用</p><h3 id="大量数据获取建议"><a href="#大量数据获取建议" class="headerlink" title="大量数据获取建议"></a>大量数据获取建议</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15933538670382.jpg?x-oss-process=style/blog" alt="-w667"></p><h3 id="Ceiling-analysis上限分析"><a href="#Ceiling-analysis上限分析" class="headerlink" title="Ceiling analysis上限分析"></a>Ceiling analysis上限分析</h3><p>找到提升最大的Module<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15933549643205.jpg?x-oss-process=style/blog" alt="-w725"></p><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p><a href="https://blog.csdn.net/xyk_hust/article/details/86649104">连接</a></p><h3 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h3><ul><li><p>向量机</p></li><li><p>核函数<br>作用：<br>减小计算量，解决多维输入问题<br>无需知道非线性变换函数的形式和参数</p></li></ul><p><a href="https://blog.csdn.net/weixin_42137700/article/details/81479322">核函数种类</a></p><ul><li>贝叶斯滤波器：概率滤波器</li></ul><h2 id="处理微分的手段"><a href="#处理微分的手段" class="headerlink" title="处理微分的手段"></a>处理微分的手段</h2><ol><li>微分+一阶惯性环节，$tf = s/(T_s s +1)$</li><li>TD微分跟踪器<img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15892146432096.jpg?x-oss-process=style/blog" alt="-w635"></li><li>状态观测器</li><li>卡尔曼滤波器</li></ol><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 ML </tag>
            
            <tag> Coursera </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN 序列模型 sequence model</title>
      <link href="/posts/sequence-model/"/>
      <url>/posts/sequence-model/</url>
      
        <content type="html"><![CDATA[<h1 id="Sequence-model"><a href="#Sequence-model" class="headerlink" title="Sequence model"></a>Sequence model</h1><p>概述：处理样本数不规则的模型</p><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943710523088.jpg?x-oss-process=style/blog" alt="-w769"></p><h2 id="recurrent-neural-network递归神经网络"><a href="#recurrent-neural-network递归神经网络" class="headerlink" title="recurrent neural network递归神经网络"></a>recurrent neural network递归神经网络</h2><p>参数共享,前-&gt;后<br>样本逐个扫描<br>a激活用一套参数<br>y激活用一套参数<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943708867352.jpg?x-oss-process=style/blog" alt="-w777"></p><h3 id="参数流"><a href="#参数流" class="headerlink" title="参数流"></a>参数流</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943568179775.jpg?x-oss-process=style/blog" alt="-w731"></p><h3 id="x、y个数不一致的RNN"><a href="#x、y个数不一致的RNN" class="headerlink" title="x、y个数不一致的RNN"></a>x、y个数不一致的RNN</h3><p>序列样本分类问题<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943713488434.jpg?x-oss-process=style/blog" alt="-w775"><br>音乐生成、机器翻译<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943715537565.jpg?x-oss-process=style/blog" alt="-w774"></p><h3 id="RNN类型总结"><a href="#RNN类型总结" class="headerlink" title="RNN类型总结"></a>RNN类型总结</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943716830777.jpg?x-oss-process=style/blog" alt="-w765"></p><h2 id="language-model-with-RNN"><a href="#language-model-with-RNN" class="headerlink" title="language model with RNN"></a>language model with RNN</h2><p>输出P(sentence),并按照y(i)展开为字符串<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943725690344.jpg?x-oss-process=style/blog" alt="-w774"></p><h3 id="从训练模型采样"><a href="#从训练模型采样" class="headerlink" title="从训练模型采样"></a>从训练模型采样</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943750855765.jpg?x-oss-process=style/blog" alt="-w776"></p><p>在训练过程中，结局梯度爆炸<br>gradient clipping：梯度过大时，重新缩放梯度向量</p><h2 id="GRU-gated-recurrent-unit"><a href="#GRU-gated-recurrent-unit" class="headerlink" title="GRU gated recurrent unit"></a>GRU gated recurrent unit</h2><p>解决了梯度爆炸问题<br>新建c^{<t>} = a^{<t>} </t></t></p><p>c的估计值<br>$\tilde C^{<t>} = tanh(w_c[c^{<t-1>},x^{<t>}]+b_c)$</t></t-1></t></p><p>Gata，门限值，0 or 1，选择是否记忆<br>$\Gamma_u = \sigma(w_u[c^{<t-1>},x^{<t>}]+b_u)$$</t></t-1></p><p>c的实际值更新函数<br>$c^{<t>} = \Gamma_u * \tilde c ^{<t>} + (1-\Gamma_u) c^{<t-1>}$</t-1></t></t></p><ul><li>GRU单元<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943765663495.jpg?x-oss-process=style/blog" alt="-w365"></li></ul><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15943771234112.jpg?x-oss-process=style/blog" alt="-w545"></p><h2 id="LSTM-（Long-Short-Term-Memory）"><a href="#LSTM-（Long-Short-Term-Memory）" class="headerlink" title="LSTM （Long Short Term Memory）"></a>LSTM （Long Short Term Memory）</h2><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15944389576811.jpg?x-oss-process=style/blog" alt="-w712"></p><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15944388078864.jpg?x-oss-process=style/blog" alt="-w788"></p><h2 id="Bidirectional双向-RNN-BRNN"><a href="#Bidirectional双向-RNN-BRNN" class="headerlink" title="Bidirectional双向 RNN  BRNN"></a>Bidirectional双向 RNN  BRNN</h2><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15944392852666.jpg?x-oss-process=style/blog" alt="-w782"></p><h2 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h2><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15944397105651.jpg?x-oss-process=style/blog" alt="-w790"></p><h2 id="word-representation"><a href="#word-representation" class="headerlink" title="word representation"></a>word representation</h2><p>只用 one-hot，无法表征单词之间的关系<br>点积为0<br>构建词向量 word vec<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946185176380.jpg?x-oss-process=style/blog" alt="-w647"></p><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946183444304.jpg?x-oss-process=style/blog" alt="-w668"></p><p>man - women<br>king - queen</p><p>词向量库 E 泛化negligible不错<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946186629440.jpg?x-oss-process=style/blog" alt="-w669"></p><p>相似度函数<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946189996342.jpg?x-oss-process=style/blog" alt="-w672"></p><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946193246812.jpg?x-oss-process=style/blog" alt="-w670"><br>应对大词典的softmax运算慢问题，构建二叉树数据结构，常用的放上面，不用每次计算概率<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946209191942.jpg?x-oss-process=style/blog" alt="-w656"></p><p>平衡P(t|c),避免the of 等 词频繁运算出现</p><h3 id="负采样法Negative-sampling"><a href="#负采样法Negative-sampling" class="headerlink" title="负采样法Negative sampling"></a>负采样法Negative sampling</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946217618512.jpg?x-oss-process=style/blog" alt="-w654"></p><h3 id="Glove-global-vectors-for-word-representation"><a href="#Glove-global-vectors-for-word-representation" class="headerlink" title="Glove global vectors for word representation"></a>Glove global vectors for word representation</h3><h2 id="情感分类sentiment-classification"><a href="#情感分类sentiment-classification" class="headerlink" title="情感分类sentiment classification"></a>情感分类sentiment classification</h2><p>问题描述：<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946227091045.jpg?x-oss-process=style/blog" alt="-w658"></p><h2 id="平均数-词向量分类"><a href="#平均数-词向量分类" class="headerlink" title="平均数 词向量分类"></a>平均数 词向量分类</h2><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946228651974.jpg?x-oss-process=style/blog" alt="-w660"></p><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946229353440.jpg?x-oss-process=style/blog" alt="-w660"></p><p>词编码向量的偏差消除<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946235846181.jpg?x-oss-process=style/blog" alt="-w659"></p><h2 id="变输入输出架构"><a href="#变输入输出架构" class="headerlink" title="变输入输出架构"></a>变输入输出架构</h2><p>主要应用在语言识别和机器翻译</p><p>架构：编码器 + 解码器各用了一个<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946322380539.jpg?x-oss-process=style/blog" alt="-w493"></p><h3 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h3><p>对于翻译算法来说，一次得到整个句子的最优概率对应翻译，搜索量太大，而贪心算法，每次只选一个，随机误差太大，效果差，因此引入Beam search 算法<br>每次考虑2步，第一步选B个，第二部全选n个，从B x n个中寻优</p><h3 id="概率估计值数值稳定性"><a href="#概率估计值数值稳定性" class="headerlink" title="概率估计值数值稳定性"></a>概率估计值数值稳定性</h3><ul><li>概率$\in [0,1]$，连乘，数值稳定性差</li><li>转化为log函数求和，越加越小</li><li>平均值，比求和好</li><li>用$\frac{1}{T_y^\alpha}$<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946358060953.jpg?x-oss-process=style/blog" alt="-w703"></li></ul><h3 id="Error-analysis"><a href="#Error-analysis" class="headerlink" title="Error analysis"></a>Error analysis</h3><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946364822178.jpg?x-oss-process=style/blog" alt="-w702"></p><h2 id="注意力集中-Attention-model-intution"><a href="#注意力集中-Attention-model-intution" class="headerlink" title="注意力集中 Attention model intution"></a>注意力集中 Attention model intution</h2><ul><li>长序列模型的问题<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946392688436.jpg?x-oss-process=style/blog" alt="-w703"><br>without 注意力模型，$y^{<t>}$ 取决于 $a^{<t>}$<br>带有注意力的系统，将权重，分散给其他的几个激活值$a^{<t>}$<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946399130903.jpg?x-oss-process=style/blog" alt="-w696"></t></t></t></li></ul><h3 id="注意力权重计算"><a href="#注意力权重计算" class="headerlink" title="注意力权重计算"></a>注意力权重计算</h3><p>用softmax保证和为1<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946403850376.jpg?x-oss-process=style/blog" alt="-w703"></p><h2 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h2><p>声音预处理，频谱<br><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946409369067.jpg?x-oss-process=style/blog" alt="-w700"></p><p><img src="http://hao-blog.oss-cn-hangzhou.aliyuncs.com/2020/08/05/15946409126527.jpg?x-oss-process=style/blog" alt="-w700"></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 序列模型 sequence model </tag>
            
            <tag> 循环神经网络 RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科学写作</title>
      <link href="/posts/paper-writting/"/>
      <url>/posts/paper-writting/</url>
      
        <content type="html"><![CDATA[<h1 id="Paper协作工具"><a href="#Paper协作工具" class="headerlink" title="Paper协作工具"></a>Paper协作工具</h1><h2 id="找词汇"><a href="#找词汇" class="headerlink" title="找词汇"></a>找词汇</h2><ul><li>Linggle</li><li>Netspeak</li></ul><h2 id="杂志影响力"><a href="#杂志影响力" class="headerlink" title="杂志影响力"></a>杂志影响力</h2><ul><li>Scimago Journal</li><li>Semantic Scholar</li><li>Google scholar</li></ul><p><a href="http://corpus.byu.edu/coca/">Corpus of Contemporary American English</a></p><h1 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h1><ol><li>理论创新</li><li>结果创新</li><li>方法创新</li></ol><h2 id="写作规范—符号"><a href="#写作规范—符号" class="headerlink" title="写作规范—符号"></a>写作规范—符号</h2><h2 id="Latex"><a href="#Latex" class="headerlink" title="Latex"></a>Latex</h2><ul><li>智能引用clerveref，引用格式带标题，可改引用格式，图、表、式:label 要在caption 之后，才可以正确引用</li><li>bib文件参考文献</li><li>\section*{acknowledgement}致谢无章节编号 </li></ul><h2 id="markdown"><a href="#markdown" class="headerlink" title="markdown"></a>markdown</h2><p><code>[TOC]是目录</code></p><h1 id="SCI论文写作要点"><a href="#SCI论文写作要点" class="headerlink" title="SCI论文写作要点"></a>SCI论文写作要点</h1><h2 id="1-事实汇总"><a href="#1-事实汇总" class="headerlink" title="1. 事实汇总"></a>1. 事实汇总</h2><h3 id="Paper-Structure"><a href="#Paper-Structure" class="headerlink" title="Paper Structure"></a>Paper Structure</h3><p><img src="/img/15966331937167.jpg" alt=""></p><h3 id="Review-Process"><a href="#Review-Process" class="headerlink" title="Review Process"></a>Review Process</h3><p><img src="/img/15966332078087.jpg" alt=""></p><p><img src="/img/15966332147634.jpg" alt=""></p><h3 id="文章第一印象"><a href="#文章第一印象" class="headerlink" title="文章第一印象"></a>文章第一印象</h3><ul><li>图</li><li>表</li><li>引用：数量20个、时间新</li></ul><h2 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h2><p><img src="/img/15966332219322.jpg" alt=""></p><p><img src="/img/15966332275940.jpg" alt=""></p><p><img src="/img/15966332337510.jpg" alt=""></p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><p>理论：推导过程<br>实证：调查方法，数据处理<br>工程应用：理论-&gt;实践</p><p>提供<strong>足够的、准确的，技术细节</strong><br>包括：架设、数学推导、验证、实验设备<br><img src="/img/15966332460219.jpg" alt=""></p><p>注意：<br>符号一致性，符号、单位、描述<br><img src="/img/15966332525954.jpg" alt=""></p><p>编号更新</p><h2 id="4-Result-and-Discussion（结果和讨论）"><a href="#4-Result-and-Discussion（结果和讨论）" class="headerlink" title="4. Result and Discussion（结果和讨论）"></a>4. Result and Discussion（结果和讨论）</h2><p><img src="/img/15966332610267.jpg" alt=""></p><h2 id="5-图表-Figure-and-Table-——第一印象"><a href="#5-图表-Figure-and-Table-——第一印象" class="headerlink" title="5. 图表(Figure and Table)——第一印象"></a>5. 图表(Figure and Table)——第一印象</h2><ol><li>Author’s guide </li><li>图表必须在论文中解释</li><li>引用规范，编号(Fig or Figure)</li><li>一致性</li><li>Caption清晰、准确、完整，不怕标题过长</li><li>区分数据</li><li>副标题（a）</li><li>杜绝一图多投</li><li><strong>保留原始数据</strong></li></ol><h2 id="6-结论conclusion、摘要abstract、题目title"><a href="#6-结论conclusion、摘要abstract、题目title" class="headerlink" title="6. 结论conclusion、摘要abstract、题目title"></a>6. 结论conclusion、摘要abstract、题目title</h2><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>总结 Result and discussion，重写，not copy</p><h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><p><img src="/img/15966332705655.jpg" alt=""></p><h3 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h3><p><img src="/img/15966332770401.jpg" alt=""></p><h2 id="7-回复审稿人"><a href="#7-回复审稿人" class="headerlink" title="7. 回复审稿人"></a>7. 回复审稿人</h2><h3 id="调整心态："><a href="#调整心态：" class="headerlink" title="调整心态："></a>调整心态：</h3><p>两个审稿人给大修：录用比例超过50%</p><p>论文没有3天可见</p><p>逐一回答每一条意见，每个人可能5页纸</p><h3 id="清晰标明所有文章中的修改"><a href="#清晰标明所有文章中的修改" class="headerlink" title="清晰标明所有文章中的修改"></a>清晰标明所有文章中的修改</h3><p>Each comment will be directly addressed regarding the modified manuscript with changes highlighted in yellow</p><h3 id="礼貌客气"><a href="#礼貌客气" class="headerlink" title="礼貌客气"></a>礼貌客气</h3><p>peer review 没有报酬，感谢和评审时间($$$)<br>The co-authors and I would like to thank you for the time and effort spent in reviewing the manuscript.</p><p>同意：The authors would like to thank the reviewer for the suggestion, ……<br>The authors would like to thank the reviewer’s comment on this problem,……</p><p>不同意：委婉表达，不要怼回去<br>The reviewer’s statement is correct in that …… However, the authors wish to ……<br>corresponding work in the near future and will publish it at a later time.<br>The reviewer’s comment is very useful and profound. However, the current effort</p><h3 id="问题分类"><a href="#问题分类" class="headerlink" title="问题分类"></a>问题分类</h3><h4 id="最多的问题："><a href="#最多的问题：" class="headerlink" title="最多的问题："></a>最多的问题：</h4><p>文字、图标样式修改<br>研究背景：abstract introduction conclusion<br>添加引用文献的要求：<br>忽略了重要工作，慎重对待，讨论并引用<br>添加审稿人的研究工作</p><h4 id="最难的问题："><a href="#最难的问题：" class="headerlink" title="最难的问题："></a>最难的问题：</h4><p>创新性：别人做过，方法新，没有支撑</p><p>实验细节问题：加补实验</p><ol><li>设计其他实验的实验结果解释审稿人的问题，</li><li>文献说明实验困难</li><li>有其他证据证明文中观点 </li></ol><h4 id="回复Reviewers-时间"><a href="#回复Reviewers-时间" class="headerlink" title="回复Reviewers 时间"></a>回复Reviewers 时间</h4><p>1week-1month</p><h4 id="必问问题"><a href="#必问问题" class="headerlink" title="必问问题"></a>必问问题</h4><p>文章背景相关<br>整个领域的贡献<br>与重要文献的关系</p><p>浏览文章图表<br>novelty创新性<br>重视<strong>Introduction</strong><br>实验细节<br>每一句话都有实验数据或者文献支撑</p><h4 id="Rejection的冲动"><a href="#Rejection的冲动" class="headerlink" title="Rejection的冲动"></a>Rejection的冲动</h4><p>数学工程错误<br>明显剽窃<br>错字错词多，图表潦草<br>一稿多投，明显雷同</p><p>不要夸大其词<br>图片信息量太小，灌水</p><h4 id="文章好感度爆棚"><a href="#文章好感度爆棚" class="headerlink" title="文章好感度爆棚"></a>文章好感度爆棚</h4><p>描述问题清楚，逻辑清楚<br>格式标准，图表清晰<br>模拟实验均具备<br>尽力按照审稿人修改</p><h4 id="怼回去"><a href="#怼回去" class="headerlink" title="怼回去"></a>怼回去</h4><p>基础：审稿人吹毛求疵拒稿、编辑很欣赏</p><ul><li>回答好意见，补充漏洞</li><li>只批评没建议，结合其他审稿人正面的意见来反驳</li><li>审稿人的漏洞</li><li>礼貌不谦卑，反驳不吵架</li></ul><h1 id="审稿时间进度"><a href="#审稿时间进度" class="headerlink" title="审稿时间进度"></a>审稿时间进度</h1><p><img src="https://i.loli.net/2020/06/23/GeDM7k1pxFJlSHN.jpg" alt="-w670"></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科学写作 </tag>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac必备软件推荐，让你效率起飞🚀</title>
      <link href="/posts/mac-app/"/>
      <url>/posts/mac-app/</url>
      
        <content type="html"><![CDATA[<h1 id="系统工具效率"><a href="#系统工具效率" class="headerlink" title="系统工具效率"></a>系统工具效率</h1><ul><li>Alfred<br>置顶，比spotlight高效，支持各种插件，没有上限</li><li>Appcleaner<br>mac卸载软件就靠它，一键拖入，彻底清理干净。想想CleanMyMac、Dr.Cleaner还收费，真是笑死人</li><li>istate menus<br>监控你的电脑，cpu、内存、网络、硬盘，你能想到的都有了</li><li>handshaker<br>老罗确实改变了世界，锤子出品必属精品，mac和手机连接的神器，有线和无线均可，Android和iPhone都可以用，itunes是个啥？？？</li><li>Duet<br>Mac下<code>mac os 10.15</code>有随航(Sidecar)，谁用这个，Windows可以用这个把ipad变为第二个屏幕，超好用，支持无线连接</li><li>Parallel Desktop<br>Mac下的虚拟机，比<code>vitural box</code> <code>VMware</code>好用太多</li><li>外接屏幕开启Retina(Hidpi)<br><a href="https://github.com/xzhih/one-key-hidpi/blob/master/README-zh.md">一键命令行</a></li></ul><h1 id="应用效率工具"><a href="#应用效率工具" class="headerlink" title="应用效率工具"></a>应用效率工具</h1><ul><li>坚果云<br>最好用的网盘，已经入了4年个人专业版会员，用来多终端同步工作资料，不要太方便</li><li>休息一下<br>20min提醒你休息一下眼睛，坐办公室挺需要的</li><li><a href="https://www.yingdev.com/projects/wgestures">WGestures2</a><br>mac os 自带触发角，用起来爽飞了，这是一款加强版的应用，装饰键 + 手势，自定义</li><li>Anydesk<br>即开即用的轻量化远程协助工具，teamviewer还要注册账号</li><li>Etcher<br>U盘刻录工具，大佬都在用</li><li>Blackmagic Disk Speed Test<br>磁盘测速，mac下都用这个，baseline一样，参数对比才有意义</li><li>CleanMyMac<br>mac下的鲁大师，虽然mac不用怎么管理，也没中过病毒，硬盘小的话，可以用来清垃圾什么的</li><li>DaisyDisk<br>清理电脑磁盘必备，可视化做的很不错</li><li>Folx<br>挺好用的下载管理工具</li><li>Transmission<br>种子下载下载</li><li>μ torrent<br>种子下载工具</li><li>kaka<br>压缩包工具</li><li>Mosaic<br>比Magnet定制程度高，实现windows下<code>win + →</code>等功能</li><li>Wechat 小助手<br>远程控制mac、免认证登录、消息防撤回、AI自动回复等功能，还有主题<br><a href="https://github.com/MustangYM/WeChatExtension-ForMac">wechat-plugin</a></li><li>Wechat 小助手自动更新<br>微信版本更新后，会删除小助手，这个命令行工具，可以帮你开机自检，微信更新后，自动安装助手<br><a href="https://github.com/lmk123/oh-my-wechat">oh my wechat</a></li><li>Notion<br>神器，添加各种服务，项目管理、笔记、备忘，All-in-one workspace</li></ul><h1 id="音视频工具"><a href="#音视频工具" class="headerlink" title="音视频工具"></a>音视频工具</h1><ul><li>Downie<br>它支持1000+网站视频下载，包括YouTube、B站等，而且可以设置4K分辨率下载，几乎无所不能，绝对的神器</li><li>Permute<br>通用格式转换软件，包括视频、音频，图片乃至pdf，不仅效率高，而且完美配合downie、iTunes，可以连接Downie使用</li><li>IINA<br>最好用的视频播放器，轻量、简洁、支持云播放、字幕在线搜索</li><li>Adobe Zii<br>Adobe全家桶都可以通过它来白嫖，再说就说多了</li><li>Picgo<br>支持多家图床(github、阿里、七牛、腾讯、smms、imgur等)的管理工具</li><li>Pixelmator Pro<br>mac下的Photoshop，300Mb，比ps轻量，功能足够且易用</li><li>VideoGif<br>功能如其名，快速视频转gif工具，支持简单的尺寸和时间轴编辑</li><li>PicGif<br>功能如其名，快速图片转gif工具，win下美图秀秀就挺好😂</li><li>imageOptim<br>快速压缩图片用，支持Terminal Command<code>imageoptim image_url</code></li><li>Inpaint<br>快速图片除背景</li><li>remove.bg<br>快速图片除背景</li><li>iShot<br>截长图工具，还有很多其他截图功能</li><li>sip<br>快速获取屏幕上颜色，输出可以是各种格式</li></ul><h1 id="阅读与写作"><a href="#阅读与写作" class="headerlink" title="阅读与写作"></a>阅读与写作</h1><ul><li>SCI hub<br>必须置顶，配合<code>google scholar</code>下文献效率提升100倍是有的</li><li>texpad<br>mac下<code>latex</code>写作工具，支持<code>live edit</code>模式，好用，谁用谁知道系列</li><li>Mweb<br>mac下<code>markdown</code>编辑器，个人感觉最好用，自带图床插件，发布<code>blog</code>效率不要太高</li><li>skim<br>最清凉的pdf预览工具，可以配合<code>atom</code> <code>vs code</code>实现latex的pdf预览，开大体积pdf超快</li><li>PDF Expert<br>Mac下的<code>adobe acrobat</code>，</li><li>caj viewer<br>知网文献打开必备，其实还有一个网址可以下载pdf版的知网文献，私信获取</li><li>Mendeley<br><code>Zotero</code>、<code>Endnote</code>也挺好，各有所长吧，习惯了一个就行<br>配合坚果云使用更佳，毕竟免费的云同步不挂代理，有点慢</li><li>Mathpix snipping tool<br>latex 公式 OCR 最强软件，准确率一流，识别完自动复制，多种样式任你挑选</li><li>copy translator<br>整合了<code>google</code> <code>baidu</code> <code>bing</code> 翻译的英文文献阅读工具</li><li>Reeder<br>RSS阅读管理工具，订阅阅读，不用看广告了</li><li>PDF Squeezer<br>pdf压缩工具，有一个网站也挺好用的</li><li>small pdf<br>就是上面说的那个网站，功能齐全，免费有次数限制</li><li><a href="https://oversea.cnki.net/index/">知网海外版</a><br>可以下载pdf，不用再用<code>caj viewer</code>看了</li></ul><h1 id="网络和代理"><a href="#网络和代理" class="headerlink" title="网络和代理"></a>网络和代理</h1><ul><li>Little Snitch Configuration<br>监控、管理电脑应用网络权限</li><li>shadowsocks R<br>经典的网络自由工具，</li><li>clash x<br>新一代网络自由工具，支持订阅管理，支持<code>ssr</code>、<code>V2-ray</code>协议，UI友好，我也在用</li></ul><h1 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h1><ul><li>draw.io<br>开源流程图，mac下的visio替代品，轻量化，支持google drive、onedrive等，office插件支持</li><li>OmniGraffle<br>流程图高级编辑工具</li></ul><h1 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h1><ul><li>VS Code<br>轻量多语言IDE，定制性高，插件丰富，现在可以云开发了，NB</li><li>Pycharm<br>python高级IDE，比<code>VS Code</code>有点重，大项目会用这个吧</li><li>Kite<br>高级代码自动补全插件，支持<code>js</code>、<code>python</code>语言和<code>VS code</code>和 <code>pycharm</code>等IDE</li><li>Dash<br>代码文档查阅工具</li><li>CodeRunner<br>轻量化的IDE，所有语言都支持，NB</li><li>Sourcetrail<br><code>git</code>的 UI软件，个人感觉比官方的<code>github desktop</code>功能强大</li><li>pynsource<br><code>python</code>源码阅读工具，显示<code>uml</code>图，快速理解项目结构</li></ul><h1 id="游戏"><a href="#游戏" class="headerlink" title="游戏"></a>游戏</h1><ul><li>OpenEmu<br>小霸王玩过吧，任天堂系列的主机这里都支持</li><li>Dolphin<br>模拟器，同上</li><li>RetroArch<br>模拟器，同上，对手柄支持最好，fake PS 3手柄也可以</li><li>Controller lite<br>快速测试你的手柄可用否</li></ul><h1 id="浏览器插件"><a href="#浏览器插件" class="headerlink" title="浏览器插件"></a>浏览器插件</h1><ul><li>Polyglot<br>safari翻译插件</li><li>AdBlock<br>safari广告屏蔽插件</li></ul><h1 id="英语"><a href="#英语" class="headerlink" title="英语"></a>英语</h1><ul><li>Grammarly<br>查重就别用<code>1改</code>什么的了，差距还是有一点的</li><li>欧路词典<br>词库专业，UI漂亮</li></ul><h1 id="有趣的工具"><a href="#有趣的工具" class="headerlink" title="有趣的工具"></a>有趣的工具</h1><ul><li><a href="goodness for free">十分钟邮箱- BccTo.ME</a></li><li>Fliqlo<br>颜值屏保，颜值一级</li><li><a href="https://wallhaven.cc/search?q=space&amp;categories=110&amp;purity=100&amp;atleast=2560x1440&amp;sorting=relevance&amp;order=desc&amp;page=7">壁纸网站wallhaven.cc</a></li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> trick </tag>
            
            <tag> App </tag>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>控制理论笔记</title>
      <link href="/posts/advanced-control/"/>
      <url>/posts/advanced-control/</url>
      
        <content type="html"><![CDATA[<h1 id="经典控制理论"><a href="#经典控制理论" class="headerlink" title="经典控制理论"></a>经典控制理论</h1><h2 id="动态系统建模"><a href="#动态系统建模" class="headerlink" title="动态系统建模"></a>动态系统建模</h2><p>通过配置系统输入u(t)，使u(s)G(s)的极点使系统满足一定特性</p><h3 id="一阶系统特性"><a href="#一阶系统特性" class="headerlink" title="一阶系统特性"></a>一阶系统特性</h3><p>$G(s) = \frac{a}{s+a}$<br>$\frac{1}{a}$是时间常数$\tau$，对应上升为0.63<br>$4\tau$对应阶跃响应0.98</p><h3 id="二阶系统特性"><a href="#二阶系统特性" class="headerlink" title="二阶系统特性"></a>二阶系统特性</h3><p>$m\ddot x+B\dot x+kx=F$<br>$\ddot x+2\omega_n\xi \dot x+\omega_n^2x=\frac{F}{m}$</p><p>阻尼比固有频率:$\omega_n\sqrt{1-\xi^2}$</p><p>单位化：$u(t)=\frac{F}{\omega_n^2}$<br>$H(s) = \frac{\omega_n^2}{s^2+2\xi\omega_ns+\omega_n^2}$</p><p><img src="/img/15744291457149.jpg" alt=""></p><p>零极点图：<br>极点全部在左，系统稳定<br>虚轴长度代表<strong>振荡周期</strong><br>实轴长度代表<strong>衰减速度</strong><br>$\cos \theta$代表<strong>阻尼比</strong></p><h2 id="SISO-system稳定性判据"><a href="#SISO-system稳定性判据" class="headerlink" title="SISO system稳定性判据"></a>SISO system稳定性判据</h2><p>特征多项式系数判断传递函数稳定性</p><ol><li>Hurwitz霍尔维兹判据：构建霍尔维兹行列式，全部为正</li></ol><p>$D1 = a_1$</p><p>$D2 = \begin{pmatrix}<br>a_1&amp;a_3\\<br>a_0&amp;a_2<br>\end{pmatrix}$</p><p>$D3 = \begin{pmatrix}<br>a_{1}&amp; a_{3}&amp; a_{5}\\<br>a_{0}&amp; a_{2}&amp; a_{4}\\<br>0&amp; a_{1}&amp; a_{3}<br>\end{pmatrix}$</p><ol><li>Lienard-Chipard林纳德-齐帕特判据：系数都大于零，奇数或偶数阶次行列式</li><li>Routh劳斯判据：<br>求$e_{ss}$时顺序，1判断稳定性、2求E(s)，3应用终值定理$e_{ss} = \lim \limits_{s\rightarrow0}sE(s)$</li><li>频率稳定判据：<br>H. Nyquist奈奎斯特判据，开环频率特性，判断闭环稳定性<br>$F(s) = 1 +G(s)H(s)$的p，极点，是开环传函极点<br>z零点，闭环传递函数的极点封闭曲线内$R=P-Z$</li></ol><h2 id="频率特性"><a href="#频率特性" class="headerlink" title="频率特性"></a>频率特性</h2><ul><li>只适用于线性定常模型，否则不能拉式变换</li><li>稳定条件下使用</li><li>bode图单位用dB：20log(Mo/Mi)，表征了能量</li></ul><ol><li>幅值相应:magnitude response<br>$\frac{M_o}{M_i} = \left | G(j\omega)\right |$</li><li>幅角响应:Phase response<br>$\phi_o-\phi_i = \angle G(j\omega)$</li><li><p>带阻尼比的共振频率:<br>$\omega = \omega_n \sqrt{1-2\zeta^2}\$<br>此时的极值：$\frac{1}{2\zeta\sqrt{1-\zeta^2}}$</p></li><li><p>幅值裕度h：相位为-π时，幅值距0dB的差值<br>相位裕度$\gamma$:幅值为1（0dB）时，相位距-π的差<br>根据幅相图，(0,0)出发为开环，(-1,0)出发为闭环</p></li><li><p>不同频段信息</p></li></ol><ul><li>低频段$G(j\omega)$反映了系统的稳态精度<br>0dB/sec-&gt;稳态精度</li><li>中频段：穿越0dB$\omega_c$<br>反映了系统的平稳性和快速性<br>-20dB/sec开环积分，闭环一阶，快速性<br>-40dB/sec开环双积分，闭环二阶，零阻尼，频率段不宜过宽，穿越频率取-20斜率</li><li>高频段反映了系统对高频干扰抑制能力</li></ul><h2 id="系统矫正"><a href="#系统矫正" class="headerlink" title="系统矫正"></a>系统矫正</h2><h3 id="串联矫正"><a href="#串联矫正" class="headerlink" title="串联矫正"></a>串联矫正</h3><ol><li><p>超前矫正<br>$G_c(s)=\frac{1+aTs}{1+Ts},a&gt;1$</p></li><li><p>滞后矫正<br>$G_c(s)=\frac{1+bTs}{1+Ts},b&lt;1$</p></li><li><p>滞后超前矫正<br>两个合起来</p></li><li><p>PID矫正器</p></li><li><p>复合矫正<br>前置矫正：指令-&gt;Gc(s)-&gt;误差，一般补偿分母s，开环前向增益1<br>干扰前置补偿：干扰测量-&gt;Gc(s)-&gt;误差，误差-&gt;干扰端传函$Gs^{-1}$</p></li></ol><h2 id="根轨迹"><a href="#根轨迹" class="headerlink" title="根轨迹"></a>根轨迹</h2><p>（开环-&gt;闭环稳定性）:分析G(s)的N、P，看闭环系统稳定性<br>开环传递函数中开环增益K从0-无穷时，闭环特征根的移动轨迹<br>单位负反馈闭环传递函数<br>$\phi(s) = \frac{C(s)}{R(s)}=\frac{G(s)}{1+G(s)}$<br>G(s)是一个 </p><p><img src="/img/%E6%88%AA%E5%B1%8F2020-04-12%20%E4%B8%8B%E5%8D%883.14.24-1.png" alt="截屏2020-04-12 下午3.14.24"></p><h3 id="非线性系统"><a href="#非线性系统" class="headerlink" title="非线性系统"></a>非线性系统</h3><p>叠加原理不适用<br>常规分类：<br>死区<br>饱和<br>间隙-滞环</p><p>系统收敛：消耗系统能量<br>系统发散：从外界获取能量</p><h1 id="相关词汇"><a href="#相关词汇" class="headerlink" title="相关词汇"></a>相关词汇</h1><p>$X_{ss}(t)$:ss-steady state<br>$T_s$Delay time<br>$T_r$Rise time<br>$M_p$Max Overshoot<br>$T_{ss}$Setting time调节时间<br>BIBO:输入稳定，输出稳定bounded input-bounded output<br>Real:实轴<br>Im：虚轴<br>Proportional：比例<br>Integral：积分<br>Differential：微分<br>bounded input-bounded output：稳定性<br>$\forall$for all ：任意<br>$\exists$ at least one ：存在<br>$\left | \cdot  \right |$norm：范数</p><h1 id="工程数学基础"><a href="#工程数学基础" class="headerlink" title="工程数学基础"></a>工程数学基础</h1><h2 id="1-特征值-特征向量-过渡矩阵-rightarrow-矩阵对角化"><a href="#1-特征值-特征向量-过渡矩阵-rightarrow-矩阵对角化" class="headerlink" title="1. 特征值,特征向量,过渡矩阵$\rightarrow$矩阵对角化"></a>1. 特征值,特征向量,过渡矩阵$\rightarrow$矩阵对角化</h2><p>特征值$\lambda$有$\lambda v=Av$<br>$\ | \lambda I-A\ | = 0$<br>特征值<br>解法：将$\lambda$代回$( \lambda I - A)<em> v = 0$<br>$\lambda_1 、\lambda_2$对应特征向量$v_1 、v_2$<br><em>*过渡矩阵</em></em>：特征向量组成的矩阵<br>$P =<br>\begin {pmatrix} v_1&amp;v_2<br>\end {pmatrix}$<br>$AP=A[v_1  v_2] = [Av_1 Av_2]=[\lambda_1v_1 \lambda_2 v_2]=<br>\begin{bmatrix}<br>\lambda_1v_{11} &amp; \lambda_2v_{21}\\<br>\lambda_1v_{12} &amp; \lambda_2v_{22}<br>\end{bmatrix}<br>=P\Lambda<br>$<br>所以有，单位向量矩阵P将A特征值对角化矩阵<br>$P^-1AP = \Lambda$</p><h2 id="2-线性化-Linearization"><a href="#2-线性化-Linearization" class="headerlink" title="2. 线性化 Linearization"></a>2. 线性化 Linearization</h2><p>非线性：$1/x,\sqrt{x},x^n等$</p><ul><li>用泰勒级数展开<br>在平衡点(Fixed point)$x_0$附近线性化</li></ul><ol><li>令导数项为0，求得平衡点x的值$x=x_0$</li><li>把$x_\sigma = x_0 + x_d$代入$f(x_\sigma)=f(x_0)+f’(x_0)(x_\sigma-x_0)$</li><li>把$x = x_\sigma$代入微分方程<br>将$\sigma$的x用x_0和x_d替换，然后<br>得到了关于x_d的线性化微分方程<br>$\dot x = A x + b u$求A的雅可比矩阵<br>行是函数，列为对变量的偏导；<br>求平衡点，代入偏导雅可比矩阵；<br>展开得到线性化后的微分方程</li></ol><h2 id="3-卷积与LTI冲激响应（LTI：linear-time-invariant-system）"><a href="#3-卷积与LTI冲激响应（LTI：linear-time-invariant-system）" class="headerlink" title="3. 卷积与LTI冲激响应（LTI：linear time invariant system）"></a>3. 卷积与LTI冲激响应（LTI：linear time invariant system）</h2><p>卷积：$x(t) = f(t)*h(t)=\int_0^t f(\tau)h(t-\tau)d\tau$<br>$f(t)$=输入<br>$h(t)$=单位冲激响应<br>$L_{卷积}$=L乘积</p><h2 id="4-欧拉公式Euler’s-Formula"><a href="#4-欧拉公式Euler’s-Formula" class="headerlink" title="4. 欧拉公式Euler’s Formula"></a>4. 欧拉公式Euler’s Formula</h2><p>$e^{i\theta}=\cos(\theta)+i\sin(\theta)$</p><h2 id="5-复数Complex-Number"><a href="#5-复数Complex-Number" class="headerlink" title="5. 复数Complex Number"></a>5. 复数Complex Number</h2><p> $\sin(x) = C\rightarrow x = \pi/2+2k\pi + \ln(C\pm\sqrt{C^2-1})i$<br>$Z = a + b i $<br>$Re(Z) =a $<br>$Im(Z)=b $<br>$\left | Z \right | = \sqrt{a^2+b^2}$<br>$Z = \left | Z \right | \cdot (\cos\theta+i\sin\theta)= \left | Z \right | \cdot e^{i\theta}$<br>$Z_1 \cdot Z_2 = \left | Z_1 \right | \left | Z_2 \right | e^{\theta_1+\theta_2}$<br>$Z+\bar Z = 2a$<br>$Z- \bar Z = 2bi$</p><h2 id="6-阈值选取"><a href="#6-阈值选取" class="headerlink" title="6. 阈值选取"></a>6. 阈值选取</h2><p>Normal Distribution正态分布、高斯分布<br>$X = (\mu,\sigma^2)$<br>漏检False Dismissal<br>误警False Alarm</p><h1 id="Advanced控制理论"><a href="#Advanced控制理论" class="headerlink" title="Advanced控制理论"></a>Advanced控制理论</h1><p>状态空间：State-Space，包含输入、输出、状态，写成一阶微分方程的形式<br>$\dot x = A x + B u$<br>$y = Cx+Du$</p><h2 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h2><h3 id="两种类型"><a href="#两种类型" class="headerlink" title="两种类型"></a>两种类型</h3><ol><li><p>Lyapunov稳定性：有界<br>$\forall t_0, \forall \epsilon &gt;0, \exists \delta (t_0, \epsilon):\left | x(t_0)\right |&lt;\delta(t_0,\epsilon)\Rightarrow \forall t \geqslant t_0,  \left | x(t) \right | &lt; \epsilon$<img src="/img/15753853291223.jpg" alt=""><br>$a \, of\,  \lambda_i \leqslant 0$实部<br>判断方法：</p></li><li><p>渐进稳定性：<br>$\exists \delta(t_0)&gt;0: \left |x(t_0)\right |&lt;\delta(t_0) \Rightarrow<br>\lim \limits_{t \rightarrow \infty }<br>\left | x(t)\right | = 0<br>$<br>$a \, of\,  \lambda_i &lt; 0$实部</p></li></ol><h3 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h3><ol><li>直接方法：解微分方程(Direct method)<br>求解λ的值，判断正负</li><li>第二方法：(2nd method)<br>$(i)V(0) = 0$<br>$(ii) V(x) \geqslant  0 , in\,  D-{0}$ PSD:postive semi definit<br>$(iii)\dot V(x) \leqslant 0 , in\,  D-{0}$NSD:negative semi definit<br>$\Rightarrow x = 0$</li></ol><h3 id="3-不稳定"><a href="#3-不稳定" class="headerlink" title="3. 不稳定"></a>3. 不稳定</h3><p>存在至少一个特征值实部大于零</p><h2 id="相图分析-phase-portrait"><a href="#相图分析-phase-portrait" class="headerlink" title="相图分析-phase-portrait"></a>相图分析-phase-portrait</h2><p>plot(x,$\dot x$)，通过x初值，分析点在轨迹上的移动，判断稳不稳定<br>matlab绘制实例</p><pre><code>% 画解微分方程组的相图clear;cla;clc;[x,y]=meshgrid(linspace(-5,5));streamslice(x,y,0 * x + 2 * y,-3 * x + 0 * y );xlabel('x');ylabel('y');</code></pre><p><img src="/img/15747442437226.jpg" alt="w400"></p><p>特征值和相图的关系<br><img src="/img/15753593791413.jpg" alt=""></p><h2 id="齐次状态方程解-dot-x-A-x"><a href="#齐次状态方程解-dot-x-A-x" class="headerlink" title="齐次状态方程解$\dot x = A x$"></a>齐次状态方程解$\dot x = A x$</h2><p>$\dot x = a x\rightarrow x(t) = e^{at}x(0)$<br>同理，多元线性方程<br>$\dot x = a x\rightarrow x(t) = e^{At}x(0)$<br>其中，<strong>状态转移矩阵$\Phi(t)$解法</strong></p><ul><li>数值法：<br>$\Phi(t) = e^{At}=I+At+\frac{1}{2!}A^2t^2+…+\frac{1}{k!}A^kt^k$</li><li>解析法：<br>$\Phi(t) = L^{-1}[sI-A]^{-1}$</li></ul><p>性质：<br>$\Phi(0) = I$<br>$x(t) = \Phi(t-t_0)x(t_0)$<br>$\Phi ^{-1}(t) = \Phi(-t)$</p><h2 id="非齐次状态方程-dot-x-A-x-B-u"><a href="#非齐次状态方程-dot-x-A-x-B-u" class="headerlink" title="非齐次状态方程$\dot x = A x + B u$"></a>非齐次状态方程$\dot x = A x + B u$</h2><p>$x(t) = \Phi (t)x(0)+ \int_0^t\Phi(t-\tau)Bu(\tau)d\tau$<br>初始状态x(0)响应+输入项u(t)响应</p><h2 id="线性系统可控性与可观测性"><a href="#线性系统可控性与可观测性" class="headerlink" title="线性系统可控性与可观测性"></a>线性系统可控性与可观测性</h2><p>可控性：$\forall x(0),x(t_f), \exists t_f &lt; +\infty , u[0,t_f], st. x(0)\rightarrow x(t_f)$<br>充要条件：</p><ol><li><p>$S = [b\, Ab\, A^2…\, A^{n-1}b]$<br>理论可行，但是实际物理不一定<br>以离散系统为例证明：</p><script type="math/tex; mode=display">x_ 0 = 0\\x_1 = Ax_0 + Bu_0 = Bu_0\\x_2 = Ax_1 + Bu_1 = ABu_0 + B u_1\\x_3 = Ax_2 + Bu_2 = A^2Bu_0 + AB u_1 + B u_2\\</script><p>Matlab 求解，Co矩阵 “ctrb(A,B)”</p></li><li><p>$rank[S] = n, det \,  S \neq 0$</p></li></ol><p>可观性：$\forall t \in [t_0,t_f],已知y(t),u(t),可求x(t_0)$<br>$rank<br>\begin{bmatrix}<br>C\\<br>CA\\<br>CA^2\\<br>…\\<br>CA^{n-1}<br>\end{bmatrix}<br>=n<br>$</p><h3 id="引理"><a href="#引理" class="headerlink" title="引理"></a>引理</h3><p>$f(\lambda) = \sum_{i=0}^{n}a_i\lambda ^i$<br>$f(A) = 0 \rightarrow A^n = \sum_{i=0}^{n-1}a_iA^i$</p><p>求解$\left | \lambda I - A\right |$的特征多项式<br>将$\lambda = A $代入，得到递推公式，解算$A^n$</p><h2 id="状态反馈与状态观测器"><a href="#状态反馈与状态观测器" class="headerlink" title="状态反馈与状态观测器"></a>状态反馈与状态观测器</h2><p>取$u=v-kx$，其中，v为参考输入，系统闭环矩阵由A变为A-Bk</p><ol><li>不改变可控性，有可能改变可观性</li><li>闭环特征值</li></ol><h2 id="状态观测器"><a href="#状态观测器" class="headerlink" title="状态观测器"></a>状态观测器</h2><h3 id="平凡观测器"><a href="#平凡观测器" class="headerlink" title="平凡观测器"></a>平凡观测器</h3><p>对于系统</p><script type="math/tex; mode=display">\tag{1}\dot x = Ax+Bu\\y = Cx + Du</script><p>观测器形式（模拟器）：$\dot {\hat x }=A\hat x +Bu$<br>定义$e=x-\hat x$<br>，有$\dot e = \dot x - \dot {\hat x }=A(x-\hat x)=Ae$<br>结论：没有消除误差的能力，估计误差模型收敛性依赖于系统矩阵$A$，若$\det(A)=0$，则观测器误差不能收敛。</p><h3 id="完全Luenberger观测器"><a href="#完全Luenberger观测器" class="headerlink" title="完全Luenberger观测器"></a>完全Luenberger观测器</h3><p>观测器分为，模拟器，修正器部分，通过输出<code>y</code>的信息来修正观测器的收敛性。</p><script type="math/tex; mode=display">\tag{1}\dot x = Ax+Bu\\y = Cx + Du</script><script type="math/tex; mode=display">\tag{2}\dot {\hat x} = A\hat x + Bu + L (y - \hat y)</script><script type="math/tex; mode=display">\tag{3}\dot {\hat y} = C \dot x+ Du</script><p>将$(3)$代入$(1)$</p><script type="math/tex; mode=display">\tag{4}\dot {\hat x} = (A-LC)\hat x + (B-LD)u + Ly</script><p>定义$e = x - \hat x$，求解$\dot e$，联立$(1)$和$(4)$</p><script type="math/tex; mode=display">\tag{5}\dot e = Ax+Bu-(A-LC)\hat x -(B-LD)u-Ly</script><p>将$(1)$代入$(5)$</p><script type="math/tex; mode=display">\tag{6}\dot e = (A-LC)e</script><h4 id="测量噪声的影响"><a href="#测量噪声的影响" class="headerlink" title="测量噪声的影响"></a>测量噪声的影响</h4><p>考虑测量噪声</p><script type="math/tex; mode=display">y(t) = Cx(t) +  r(t)</script><p>系统的观测误差动态为</p><script type="math/tex; mode=display">\dot e (t) = (A-LC)e(t)-Lr(t)</script><p>结论：观测器特征值太大，增益矩阵$L$过大，对测量误差r(t)有放大作用。</p><h3 id="分离原理"><a href="#分离原理" class="headerlink" title="分离原理"></a>分离原理</h3><p>分别对系统</p><ul><li>设计观测器<ul><li><script type="math/tex; mode=display">\dot {\hat x} = A\hat x + Bu + L (y - \hat y)\\\hat y = C\hat x</script></li></ul></li><li>设计控制率<ul><li>$u = -K\hat x + Sw$</li></ul></li><li>对分别设计的观测器和控制器构建扩展状态方程<script type="math/tex; mode=display">\underbrace{\left[\begin{array}{c}\dot{\mathbf{x}} \\ \dot{\mathbf{e}}\end{array}\right]}_{\dot{\mathbf{x}}_{e}}=\underbrace{\left[\begin{array}{cc}\mathbf{A}-\mathbf{B K} & \mathbf{B K} \\ \mathbf{0} & \mathbf{A}-\mathbf{L} \mathbf{C}\end{array}\right]}_{\mathbf{A}_{e}} \underbrace{\left[\begin{array}{c}\mathbf{x} \\ \mathbf{e}\end{array}\right]}_{\mathbf{x}_{e}}+\underbrace{\left[\begin{array}{c}\mathbf{B S} \\ \mathbf{0}\end{array}\right]}_{\mathbf{B}_{e}} \mathbf{w}</script></li></ul><p>合成的系统可以描述为$\dot X_e = A_e X + B_e u$<br>评估扩展动态系统的矩阵$A_e$等于观测器和控制率矩阵的矩阵特征值相乘<br>有$\det(\lambda L - A_e) = \det(\lambda L - A+BK) \det(\lambda L-A+LC)$<br>表明：带有控制率和观测器的系统，可以先独立设计，在最后合成</p><p><img src="/img/15755438743134.jpg" alt=""></p><h1 id="Kalman滤波器原理以及在matalb中的实现"><a href="#Kalman滤波器原理以及在matalb中的实现" class="headerlink" title="Kalman滤波器原理以及在matalb中的实现"></a>Kalman滤波器原理以及在matalb中的实现</h1><p>状态转移矩阵：<br>这里要改一下，改成估计量<br>$x_t^- = F_t x_{t-1} + B_t u_t$</p><p>状态转移矩阵:$P_t^-=FP_{t-1}F^T+Q$</p><p>协方差矩阵:<br>$<br>\begin{bmatrix}<br>\sigma_{11}&amp;\sigma_{12}\\<br>\sigma_{12}&amp;\sigma_{22}\\<br>\end{bmatrix}<br>$</p><p><img src="/img/15748205077671.jpg" alt="w400"></p><p>卡尔曼方程≠状态观测器<br><img src="/img/15755459115506.jpg" alt="m180"></p><p><img src="/img/15755450314782.jpg" alt=""><br>以小车为例，讲卡尔曼滤波最优状态估计<br><img src="/img/15755448894756.jpg" alt=""><br>在上图中，P是观测值$\hat x$的方差<br>R是观测器中，来自预估值的比例</p><p>概率函数相乘，多传感器信息融合</p><h2 id="非线性控制理论"><a href="#非线性控制理论" class="headerlink" title="非线性控制理论"></a>非线性控制理论</h2><h3 id="ARC"><a href="#ARC" class="headerlink" title="ARC"></a>ARC</h3><p>Barbalat’s 引理 lemma</p><ol><li>$V\geq0$</li><li>$\dot{V} \leq -g(t)$, where $g(t)\geq 0$</li><li>$\dot{g}(t)\in L_{\infty}$, if $\dot{g}(t)$ is bounded the $g(t)$ is uniformly continous.<br>Then, $\lim_{t-&gt;\infty} g(t)=0$<br>Consquently, $\lim_{t-&gt;\infty} e = 0 (k\neq0)$</li></ol><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 控制理论笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级控制理论 </tag>
            
            <tag> 经典控制理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac设置</title>
      <link href="/posts/settings-mac/"/>
      <url>/posts/settings-mac/</url>
      
        <content type="html"><![CDATA[<h1 id="系统相关"><a href="#系统相关" class="headerlink" title="系统相关"></a>系统相关</h1><h2 id="系统安装工具"><a href="#系统安装工具" class="headerlink" title="系统安装工具"></a>系统安装工具</h2><ul><li><p>系统安装、刻录工具<br><a href="https://rufus.akeo.ie/downloads/">U启动盘制作工具rufus</a><br>WTG辅助工具wtg-assistant</p></li><li><p>bootcamp蓝牙鼠标连不上<br>重置SMC，关机，control➕option➕shift➕电源for10s，再开机</p></li></ul><h2 id="Mac两个bin目录"><a href="#Mac两个bin目录" class="headerlink" title="Mac两个bin目录"></a>Mac两个bin目录</h2><p>相同点</p><p>/usr/bin和/usr/local/bin都是用来存储终端命令二进制文件或者命令的软链接<br>这两个bin目录都是已经包含在环境变量里的目录，程序放在里面或者链接到里面命令就可以在终端里直接执行。</p><p>不同点</p><p>Mac的/usr/bin目录是不允许增删文件的，/usr/local/bin增删文件来实现在终端里直接运行，只需要有管理员权限。</p><h2 id="Terminal——终端"><a href="#Terminal——终端" class="headerlink" title="Terminal——终端"></a>Terminal——终端</h2><ul><li>系统允许任何来源安装包<br>sudo spctl —master-disable</li><li>使其变为可执行脚本<br><code>chmod u+x filename</code> </li><li>更改skim背景色<br><code>defaults write -app skim SKPageBackgroundColor -array 0.78 0.93 0.8 1</code></li><li>命令行压缩加密压缩包<br>压缩文件<br><code>zip -e test.zip test.txt</code><br>压缩文件夹<br><code>zip -er test.zip test</code></li><li>读写ntfs 文件灰色，无法打开，拷贝<br><code>xattr -d com.apple.FinderInfo filepath</code></li><li><p>跳过验证dmg<br><code>xattr -d com.apple.quarantine '/Applications/Xcode.app'</code></p></li><li><p>brew<br><a href="https://www.jianshu.com/p/67db55780450">brew安装</a><br><a href="https://www.jianshu.com/p/6ea6e19c060d">mac brew install 慢解决办法</a></p></li><li><p>mac10.15软件安装提示已损坏解决办法<br>终端执行<br><code>sudo spctl --master-disablesudo xattr -r -d com.apple.quarantine /Applications/Sketch.app/</code></p></li><li><p>编辑环境变量文件文件<br>```cd ~/<br>vim .bash_profile</p><pre><code>粘贴入下列代码</code></pre><h1 id="proxy-list"><a href="#proxy-list" class="headerlink" title="proxy list"></a>proxy list</h1><p>alias proxy=’export all_proxy=socks5://127.0.0.1:1086’<br>alias unproxy=’unset all_proxy’</p><pre><code>control + c 键入wq保存退出 保存隐藏文件到环境变量```cd ~/source .bash_profile</code></pre></li></ul><ul><li><p>系统环境变量<br>修改<code>~/.bash_profile</code></p></li><li><p>添加软链接，对终端等命令皆有效<br>把后者链接到前者，<a href="https://www.omegaxyz.com/2020/02/05/mac-pip-config/">详细参考</a><br><code>ln -s /Library/Frameworks/Python.framework/Versions/3.x/bin/pip /usr/local/bin/pip3</code></p></li></ul><h2 id="咳血上网"><a href="#咳血上网" class="headerlink" title="咳血上网"></a>咳血上网</h2><ul><li><a href="https://zcssr.me/auth/login">HJF-ZCSSR订阅购买网站</a></li><li>订阅地址：<ul><li>ssr订阅：<code>https://n55.pw/link/PgKooWdckjZl5hf⑧</code></li><li>clash订阅：<code>https://n55.pw/link/PgKooWdckjZl5hf⑧?clash=①</code></li></ul></li><li>节点测速链接<ul><li><a href="https://fast.com/zh/cn/#">Netflix</a></li><li><a href="https://www.speedtest.net/">speedtest</a></li></ul></li></ul><h2 id="iTerm2"><a href="#iTerm2" class="headerlink" title="iTerm2"></a>iTerm2</h2><p>配置免密登录ssh</p><ul><li><ol><li>创建脚本文件如下</li></ol></li></ul><pre><code class="lang-bash">#!/usr/bin/expectset ip [lindex $argv 0]set user [lindex $argv 1]set password [lindex $argv 2]set timeout 10spawn ssh $user@$ipexpect &amp;#123;        "*yes/no" &amp;#123;send "yes\r";exp_continue&amp;#125;        "*password:" &amp;#123;send "$password\r"&amp;#125;&amp;#125;interact</code></pre><p>上述三个argv是变量传参的接口</p><ul><li><ol><li>配置iterm2的profile</li></ol></li></ul><p>在配置iterm2（iTerm2 -&gt; Preferences -&gt; Profiles）</p><p><img src="/img/16023214284111.png" alt=""></p><ul><li><ol><li>在profiles选项卡选择对应配置文件登录</li></ol></li></ul><p><img src="/img/16023214488409.png" alt=""></p><h2 id="hosts修改"><a href="#hosts修改" class="headerlink" title="hosts修改"></a>hosts修改</h2><p>shift+command+G<br>前往<br><code>/private/etc/</code></p><h2 id="Mac系统插件推荐"><a href="#Mac系统插件推荐" class="headerlink" title="Mac系统插件推荐"></a>Mac系统插件推荐</h2><ul><li><a href="https://github.com/lmk123/oh-my-wechat">Oh my way!微信插件</a></li><li><a href="https://www.zhihu.com/question/24850356">浏览器弹出mackeeper广告清理</a></li><li><a href="https://www.macx.cn/thread-2204536-1-1.html">油猴</a></li></ul><h2 id="Mac-OS-X-11中的-usr-bin-的“Operation-not-permitted”"><a href="#Mac-OS-X-11中的-usr-bin-的“Operation-not-permitted”" class="headerlink" title="Mac OS X 11中的/usr/bin 的“Operation not permitted”"></a>Mac OS X 11中的/usr/bin 的“Operation not permitted”</h2><p><a href="https://www.jianshu.com/p/22b89f19afd6">https://www.jianshu.com/p/22b89f19afd6</a></p><h2 id="设置mac-PATH"><a href="#设置mac-PATH" class="headerlink" title="设置mac PATH"></a>设置mac PATH</h2><p>在mac系统下打开终端，输入：<br><code>touch .bash_profileopen -e .bash_profile</code><br>这样会弹出一个“.bash_profile”文件.<br>打开文件后应该是空白的，在文件中添加：<br>export PATH=${PATH}:????</p><p>其中????代表你电脑中adb文件的路径，本人的配置文件如下：<br><code>export PATH=${PATH}:~/Library/Android/sdk/platform-tools</code><br>添加完后，保存并关闭文件，至此，adb配置完毕。</p><h2 id="前往网上邻居"><a href="#前往网上邻居" class="headerlink" title="前往网上邻居"></a>前往网上邻居</h2><p>finder目录<code>command+K</code><br><strong>One cloud</strong>:<br><code>smb://192.168.31.200</code></p><h2 id="mac除了magicmouse外，蓝牙鼠标发飘，延迟的问题"><a href="#mac除了magicmouse外，蓝牙鼠标发飘，延迟的问题" class="headerlink" title="mac除了magicmouse外，蓝牙鼠标发飘，延迟的问题"></a>mac除了magicmouse外，蓝牙鼠标发飘，延迟的问题</h2><ol><li>打开“系统偏好设定”，然后点击下面的“网络”图标，打开“网络”设定界面。在这里，一般会看到三个：wi-fi、蓝牙、Thunderbolt 网桥三个连接。</li><li>在“网络”窗口的左下角，会看到三个图标：“+”、“-”，最后一个是齿轮。点击这个齿轮，选择“设定服务顺序”</li><li>用鼠标点按住蓝牙PAN，往上拖到第一的位置，保存、应用。</li></ol><h2 id="Terminal-commands"><a href="#Terminal-commands" class="headerlink" title="Terminal commands"></a>Terminal commands</h2><p>图片本地压缩工具：<code>imageoptim aaa.jpg</code><br>开源软件安装：<code>brew install example</code><br>python库安装<code>pip3 install example</code><br>最后更新配置的环境变量<code>source .bash_profile</code></p><p>同理：http代理设置如下：</p><ul><li>让终端走http代理——应用层，不改变系统层<pre><code>export http_proxy="http://localhost:1087"export https_proxy="http://localhost:1087"</code></pre></li></ul><p><a href="https://mirrors.tuna.tsinghua.edu.cn/help/homebrew/">brew 源替换清华</a><br>git 太慢的解决办法</p><ol><li>在<a href="https://www.ipaddress.com/查询网址对应ip">https://www.ipaddress.com/查询网址对应ip</a></li><li>手动更改hosts</li></ol><pre><code>199.232.68.133 raw.githubusercontent.com199.232.69.194 github.global.ssl.fastly.net140.82.114.4 github.com</code></pre><ol><li><p>刷新dns缓存<br><code>sudo dscacheutil -flushcache</code></p></li><li><p>git 代理设置<br>手动操作</p></li></ol><ul><li>设置</li></ul><pre><code>git config --global http.proxy socks5://127.0.0.1:1086;git config --global https.proxy socks5://127.0.0.1:1086</code></pre><ul><li>卸载</li></ul><pre><code>git config --global --unset http.proxygit config --global --unset https.proxy</code></pre><ul><li>写入命令行vim添加：</li></ul><pre><code>alias git_proxy='git config --global http.proxy socks5://127.0.0.1:1086; git config --global https.proxy socks5://127.0.0.1:1086'alias ungit_proxy='git config --global --unset http.proxy; git config --global --unset https.proxy'</code></pre><p>查看是否添加上：<br><code>cat ~/.gitconfig</code></p><ul><li>brew源更改（采用阿里云）</li></ul><pre><code>cd "$(brew --repo)"# 查看远程仓库git remote -v# 删除远程git remote rm origin # 添加阿里源 ：git remote add origin https://mirrors.aliyun.com/homebrew/brew.git# 切换成阿里源: git remote set-url origin https://mirrors.aliyun.com/homebrew/brew.git</code></pre><ul><li>brew-core源更改</li></ul><pre><code>cd "$(brew --repo)/Library/Taps/homebrew/homebrew-core"# 查看远程仓库 git remote -v # 删除远程： git remote rm origin # 添加阿里源 ：git remote add origin https://mirrors.aliyun.com/homebrew/homebrew-core.git# 切换成阿里源： git remote set-url origin https://mirrors.aliyun.com/homebrew/homebrew-core.git</code></pre><ul><li>bottle源更改</li></ul><pre><code>echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.aliyun.com/homebrew/homebrew-bottles' &gt;&gt; ~/.bash_profilesource ~/.bash_profile</code></pre><ul><li>cask源更改（中科大源）</li></ul><pre><code>cd "$(brew --repo)"/Library/Taps/homebrew/homebrew-caskgit remote rm origin git remote add origin https://mirrors.ustc.edu.cn/homebrew-cask.gitgit remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git</code></pre><ul><li>环境变量设置，快捷命令设置<br>自己在 ~/.bash_profile 中配置环境变量, 可是每次重启终端后配置的不生效.需要重新执行<br><code>$source ~/.bash_profile</code><br>发现zsh加载的是 ~/.zshrc文件，而 ‘.zshrc’ 文件中并没有定义任务环境变量。<br>解决办法</li></ul><p>在~/.zshrc文件最后，增加一行：<br><code>source ~/.bash_profile</code></p><h2 id="UEFI多系统引导工具"><a href="#UEFI多系统引导工具" class="headerlink" title="UEFI多系统引导工具"></a>UEFI多系统引导工具</h2><ul><li>clover</li><li>rEFInd：</li><li>grub4dos</li></ul><h2 id="Finder"><a href="#Finder" class="headerlink" title="Finder"></a>Finder</h2><ul><li>显示隐藏文件：shift + Command + . </li></ul><h2 id="硬盘"><a href="#硬盘" class="headerlink" title="硬盘"></a>硬盘</h2><h3 id="ntfs原生挂载"><a href="#ntfs原生挂载" class="headerlink" title="ntfs原生挂载"></a>ntfs原生挂载</h3><p>terminal：<br><code>sudo nano /etc/fstab</code><br><code>LABEL=Elements\040SE none ntfs rw,auto,nobrowse</code><br>\040代表空格<br>ctrl + x 保存，y确定</p><h3 id="外接硬盘无法挂载"><a href="#外接硬盘无法挂载" class="headerlink" title="外接硬盘无法挂载"></a>外接硬盘无法挂载</h3><p>需要先进行检查修复</p><pre><code>diskutil listdiskutil repairVolume /dev/disk4diskutil mount /dev/disk4</code></pre><h1 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h1><h2 id="Latex"><a href="#Latex" class="headerlink" title="Latex"></a>Latex</h2><ol><li>文献不引用编译会报错，修正方法，clean过程文件，重新编译</li><li><a href="http://www.latexstudio.net/archives/12260.html">mac vs code 改编译器</a></li><li><a href="http://www.latexstudio.net/archives/12321.html">mac vs_code skim_pdf_viewer设置</a></li><li>vscode-skim正向同步：命令行，sync from cursor，快捷键option+command+J</li><li>vscode-skim反向同步：shift+command+click</li><li>vscode-编译：command + shift + B</li><li>xelatex编译慢的解决办法，重建字体缓存<code>fc -cache -fv</code></li></ol><h3 id="Mac中输入latex公式，用mathtype编译"><a href="#Mac中输入latex公式，用mathtype编译" class="headerlink" title="Mac中输入latex公式，用mathtype编译"></a>Mac中输入latex公式，用mathtype编译</h3><p>mac word 中键入latex公式<br>输入格式如下$f(x) = a^2+2$<br>按下option(alt)+\，mathtype会编译，并且可以修改</p><h2 id="其他插件-多数可见"><a href="#其他插件-多数可见" class="headerlink" title="其他插件-多数可见"></a>其他插件-多数可见</h2><p>pdf: PDF Expert、skim<br>远程控制: AnyDesk<br>文献管理: Mendeley Desktop<br>编程学习: Code Runner<br>翻墙: <a href="https://github.com/XX-net/XX-Net">xx.net</a><br>Markdown：MWeb，<a href="https://zh.mweb.im/how_to_use_library_in_ios.html">使用说明</a><br>safari 插件：polyglot，双击选中翻译</p><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><ul><li><a href="http://matplotlib.org/users/installing.html" title="matplotlib">matplotlib官网</a></li><li>VSCODE，编译器选Python3，设置路径</li><li>mac上使用virtualenv搭建多个<a href="https://www.jianshu.com/p/6ff149813e6c">python环境</a></li><li>Python3.x.x后安装pip出现command not found 错误<a href="http://blog.csdn.net/llh_1178/article/details/77948568">http://blog.csdn.net/llh_1178/article/details/77948568</a></li><li>Matplotlib论文格式python plot<code>pip install git+https://github.com/garrettj403/SciencePlots.git</code></li><li>python3 matplot修改中文字体<br>以下内容只针对mac os x ，亲测有效</li></ul><hr><pre><code>1.从mac字体目录/System/Library/Fonts添加SimHei字体（simhei.ttf文件）到/usr/local/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/xxx.ttf2.rm -rf ~/.matplotlib/*.cache注意rm -rf命令，确认路径没错在用设置matplotlib使用的字体资源我是直接重命名的方式，避免出错。3.代码中添加mpl.rcParams['font.sans-serif'] = u'SimHei'[更改字体](https://blog.csdn.net/gmr2453929471/article/details/78655834)</code></pre><hr><ul><li>编辑器<code>jupyter notebook</code> 升级为<code>jupyter lab</code><br>支持<code>python kite</code>代码补全</li></ul><h2 id="黑苹果安装"><a href="#黑苹果安装" class="headerlink" title="黑苹果安装"></a>黑苹果安装</h2><ul><li>聪聪黑苹果一键安装工具</li><li>clover停更了，现在用<a href="https://dortania.github.io/OpenCore-Install-Guide/">opencore</a></li></ul><h1 id="Something-interesting"><a href="#Something-interesting" class="headerlink" title="Something interesting"></a>Something interesting</h1><h2 id="US-fake信息"><a href="#US-fake信息" class="headerlink" title="US-fake信息"></a>US-fake信息</h2><p><a href="https://www.fakenamegenerator.com/index.php#">https://www.fakenamegenerator.com/index.php#</a></p><h2 id="mac手柄ps3-controller"><a href="#mac手柄ps3-controller" class="headerlink" title="mac手柄ps3 controller"></a>mac手柄ps3 controller</h2><ul><li><p>windows<br><a href="https://www.youtube.com/watch?v=c0qXZHNj_P4">https://www.youtube.com/watch?v=c0qXZHNj_P4</a></p></li><li><p>dualshock 3 panhai controller<br><a href="https://gist.github.com/OlesenkoViktor/32c700e025bf4567db8feb1ed467f8ee">https://gist.github.com/OlesenkoViktor/32c700e025bf4567db8feb1ed467f8ee</a></p></li></ul><p>最后发现，先启动retroarch之后，手柄就可以在openemu等平台使用</p><h2 id="mac——安卓刷机"><a href="#mac——安卓刷机" class="headerlink" title="mac——安卓刷机"></a>mac——安卓刷机</h2><p>android adb 黑域<br><code>adb devicesadb -d shell sh /data/data/me.piebridge.brevent/brevent.sh</code></p><p>fastboot模式：<code>adb  reboot bootloader</code></p><p><a href="https://www.jianshu.com/p/5ba1cf5869bc">mac_adb_安卓刷机</a></p><p><a href="https://highonandroid.com/android-smartphones/how-to-root-android-p9-0-pixelpixel-xlpixel-2pixel-2xl/">pixel_root教程</a></p><p><a href="https://twrp.me">TWRP下载地址</a>：第三方recovery<br>root 管理器：magisk</p><p><a href="https://zhuanlan.zhihu.com/p/90965580">安卓10root</a></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 设置备忘 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> App </tag>
            
            <tag> 系统 </tag>
            
            <tag> Mac </tag>
            
            <tag> 黑苹果 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex设置</title>
      <link href="/posts/settings-latex/"/>
      <url>/posts/settings-latex/</url>
      
        <content type="html"><![CDATA[<h1 id="Texpad实时编译注意"><a href="#Texpad实时编译注意" class="headerlink" title="Texpad实时编译注意"></a>Texpad实时编译注意</h1><p>Texpad live 支持实时编译<br>缺点：不支持高级packages，如cref</p><p>经过验证，下列包不支持实时编译</p><pre><code>\crefname&amp;#123;figure&amp;#125;&amp;#123;Fig.&amp;#125;&amp;#123;Figs.&amp;#125;\crefname&amp;#123;table&amp;#125;&amp;#123;Table.&amp;#125;&amp;#123;Tables.&amp;#125;\crefname&amp;#123;appendix&amp;#125;&amp;#123;&amp;#125;&amp;#123;&amp;#125;\crefname&amp;#123;equation&amp;#125;&amp;#123;&amp;#125;&amp;#123;&amp;#125;</code></pre><p>所以最后文档定型之后，调整格式的时候再添加</p><h2 id="强制使用自定义图例标签，ref"><a href="#强制使用自定义图例标签，ref" class="headerlink" title="强制使用自定义图例标签，ref"></a>强制使用自定义图例标签，ref</h2><p>\renewcommand{\Figurename}{Fig.}</p><h2 id="投稿latex模板自定义修改"><a href="#投稿latex模板自定义修改" class="headerlink" title="投稿latex模板自定义修改"></a>投稿latex模板自定义修改</h2><p>一般要求单列，双倍间距（可以在cls文件里面调）</p><h2 id="实时预览中文"><a href="#实时预览中文" class="headerlink" title="实时预览中文"></a>实时预览中文</h2><p>使用包CJK</p><pre><code class="lang-latex">\documentclass&amp;#123;article&amp;#125;\usepackage&amp;#123;CJKutf8&amp;#125;\begin&amp;#123;document&amp;#125;\begin&amp;#123;CJK&amp;#125;&amp;#123;UTF8&amp;#125;&amp;#123;gbsn&amp;#125;这是一个CJK例子,使用了UTF-8编码和gbsn字体。\end&amp;#123;CJK&amp;#125;\end&amp;#123;document&amp;#125;</code></pre><p>另外两种方法</p><ul><li><code>ctex</code></li></ul><pre><code class="lang-latex">\documentclass[UTF8]&amp;#123;ctexart&amp;#125;\begin&amp;#123;document&amp;#125;这是一个CTEX的utf-8编码例子，&amp;#123;\kaishu 这里是楷体显示&amp;#125;，&amp;#123;\songti 这里是宋体显示&amp;#125;，&amp;#123;\heiti 这里是黑体显示&amp;#125;，&amp;#123;\fangsong 这里是仿宋显示&amp;#125;。\end&amp;#123;document&amp;#125;</code></pre><ul><li><code>xeCJK</code>只支持<code>xelatex</code>包，不能实时预览</li></ul><pre><code class="lang-latex">\documentclass&amp;#123;article&amp;#125;\usepackage&amp;#123;xeCJK&amp;#125;\setCJKmainfont&amp;#123;SimSun&amp;#125;\begin&amp;#123;document&amp;#125;中文 \LaTeX 示例。\end&amp;#123;document&amp;#125;</code></pre><h2 id="长公式跨双栏显示"><a href="#长公式跨双栏显示" class="headerlink" title="长公式跨双栏显示"></a>长公式跨双栏显示</h2><ul><li>使用<code>cuted</code>包的<code>strip</code>环境</li><li>更改strip默认行距，<code>\stripsep -3pt plus 3pt minus 2pt</code></li></ul><pre><code class="lang-latex">\begin&amp;#123;strip&amp;#125;\begin&amp;#123;equation&amp;#125;\label&amp;#123;haha&amp;#125;a&amp;=b+c=b+c=b+c=b+c=b+c=b+c\\&amp;=b+c=b+c=b+c=b+c=b+c=b+c\end&amp;#123;equation&amp;#125;\end&amp;#123;strip&amp;#125;</code></pre><h1 id="Word公式转换"><a href="#Word公式转换" class="headerlink" title="Word公式转换"></a>Word公式转换</h1><p>mathtype支持latex公式和mathml公式的转换<br>Win10有BUG，默认报错找不到<code>.xml</code>文件<br>需要用管理员打开word，才能mathtype不报错</p><h1 id="latex公式编辑器"><a href="#latex公式编辑器" class="headerlink" title="latex公式编辑器"></a>latex公式编辑器</h1><p><a href="https://www.latexlive.com/##">latex公式编辑器</a></p><h1 id="报错解决"><a href="#报错解决" class="headerlink" title="报错解决"></a>报错解决</h1><ul><li><code>cref</code>引用不现实<code>algorithm</code>等<code>label</code>，将<code>cref</code>放在<code>algorithm2e</code>后面</li></ul><h1 id="zotero使用mendeley数据库"><a href="#zotero使用mendeley数据库" class="headerlink" title="zotero使用mendeley数据库"></a>zotero使用mendeley数据库</h1><p><a href="https://zhuanlan.zhihu.com/p/31453719">https://zhuanlan.zhihu.com/p/31453719</a><br>思想，使用软链接的方式，配合zotfile插件，将原先在zotero根目录storage子目录的文件，链接到mendeley目录，平铺展开，并关闭mendeley的重命名功能更，避免对pdf文件重复修改</p><ul><li>zotero插件<ul><li>zotfile 管理pdf</li><li>jasminum 识别中文文献</li><li>zotero-better-bibtex 管理bibtex</li></ul></li></ul><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 设置备忘 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Snippets </tag>
            
            <tag> Latex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matlab设置</title>
      <link href="/posts/settings-matlab/"/>
      <url>/posts/settings-matlab/</url>
      
        <content type="html"><![CDATA[<h2 id="代码片段snippets"><a href="#代码片段snippets" class="headerlink" title="代码片段snippets"></a>代码片段snippets</h2><ul><li>保存pdf调整a4纸至合适大小</li></ul><pre><code>h = figure;plot(1:10);set(h,'Units','Inches');pos = get(h,'Position');set(h,'PaperPositionMode','Auto','PaperUnits','Inches','PaperSize',[pos(3), pos(4)])print(h,'filename','-dpdf','-r0')</code></pre><ul><li>缺省参数默认值设定</li></ul><pre><code>function f(arg1, arg2, arg3)if ~exist('arg2', 'var')    arg2 = arg2Default;end</code></pre><ul><li><a href="https://blog.csdn.net/ckzhb/article/details/81105384">plot-legend多行设置</a></li></ul><pre><code>lgd1 = legend('第一条');set(lgd1,'FontSize',12,'Location', 'SouthOutside','box','off','Fontname','times new roman');  %注：将legend放在图外面时，Legend 1不能通过鼠标移动，只能通过代码调整位置 % ============= Legend 2 :ax2 = axes('position',get(gca,'position'),'visible','off');lgd2 = legend(ax2, [p2 p3], '第二条', '第三条');set(lgd2,'FontSize',12,'Location', 'SouthOutside','Orientation','horizontal','box','off','Fontname','times new roman');</code></pre><p>Another way:<br><code>legend({'cos(x)','cos(2x)','cos(3x)','cos(4x)'},'Location','northwest','NumColumns',2)</code></p><ul><li>mac打开时闪退<br><code>/Applications/Polyspace/R2019b.app/bin/matlab -nosplash</code></li><li>matlab-mex命令 10.15.4问题<br><code>https://www.mathworks.com/matlabcentral/answers/512901-mex-xcodebuild-error-sdk-macosx10-15-4-cannot-be-located/?s_tid=mlc_lp_leaf</code></li></ul><h2 id="绘制等高线"><a href="#绘制等高线" class="headerlink" title="绘制等高线"></a>绘制等高线</h2><pre><code>contour(u, v, z, [-0.5, -0.5], 'LineWidth', 2)</code></pre><p>区间是zlim</p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 设置备忘 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matlab </tag>
            
            <tag> snippets </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-snippets</title>
      <link href="/posts/settings-python/"/>
      <url>/posts/settings-python/</url>
      
        <content type="html"><![CDATA[<h2 id="python读取txt数据plot"><a href="#python读取txt数据plot" class="headerlink" title="python读取txt数据plot"></a>python读取txt数据plot</h2><ul><li>从txt获取数据</li></ul><pre><code class="lang-python">def get_txt_result(filename, sp=-1, ep=-1, debug=False):    fileReadObj = open(filename)    # 再读入后续的文件    fileLines = fileReadObj.readlines()  # 一次性读入所有的行    if sp &gt;= 0 and ep &gt; sp:        fileLines = fileLines[sp:ep]    mylist = fileLines[0].split("\t")  # 用""将字符分开    # print(mylist)    linecount = len(fileLines)    colcount = len(mylist)    data = np.zeros((linecount, colcount))    j = 0    for line in fileLines:        line = line.strip()  # 去掉字符串头尾的空格        mylist = line.split("\t")  # 用""将字符分开        i = 0        for x in mylist:            data[j][i] = float(x)            i = i + 1        j = j + 1    N = j    dT = 0.0001    time = np.linspace(start=0, stop=N * dT, num=N)    return time, data</code></pre><h2 id="python保存data到excel"><a href="#python保存data到excel" class="headerlink" title="python保存data到excel"></a>python保存data到excel</h2><pre><code class="lang-python">import numpy as npimport pandas as pd# prepare for datadata = np.arange(1,101).reshape((10,10))data_df = pd.DataFrame(data)# change the index and column namedata_df.columns = ['A','B','C','D','E','F','G','H','I','J']data_df.index = ['a','b','c','d','e','f','g','h','i','j']# create and writer pd.DataFrame to excelwriter = pd.ExcelWriter('Save_Excel.xlsx')data_df.to_excel(writer,'page_1',float_format='%.5f') # float_format 控制精度writer.save()</code></pre><h2 id="python-Science-风格画图"><a href="#python-Science-风格画图" class="headerlink" title="python Science 风格画图"></a>python Science 风格画图</h2><ul><li><p>安装<code>pip install SciencePlots</code></p></li><li><p>使用</p></li></ul><pre><code class="lang-python">import matplotlib.pyplot as pltimport numpy as npplt.style.use('science')x = np.linspace(00,10,50)y = np.sin(x)y2 = np.cos(x)plt.plot(x,y)plt.plot(x,y2)plt.show()</code></pre><h2 id="matplotlib-help文档"><a href="#matplotlib-help文档" class="headerlink" title="matplotlib help文档"></a>matplotlib help文档</h2><p><a href="https://matplotlib.org/gallery.html">地址</a></p><p><img src="/img/contact.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 设置备忘 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> snippets </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
